{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuaalpuerto/ML-guide/blob/main/Learning_TextGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36a9c615-17a0-455c-8f9c-f0d25fb8824b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:43:10.594204491Z",
          "start_time": "2024-06-11T15:43:10.589328053Z"
        },
        "id": "36a9c615-17a0-455c-8f9c-f0d25fb8824b",
        "outputId": "ce386217-5095-4353-c22f-8786f4fdac91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for textgrad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU textgrad --progress-bar off\n",
        "!pip install -qU openai --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load fireworks API key\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/MyDrive/env/env.json') as jsonfile:\n",
        "    env = json.load(jsonfile)\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = env['fireworks.ai']['apiKey']\n",
        "# os.environ['OPENAI_BASE_URL'] = \"https://api.fireworks.ai/inference/v1\"\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = env['openai']['apiKey']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_loXjG44xfJ",
        "outputId": "03e2c9f3-6058-46c2-ae49-fe40c150c240"
      },
      "id": "k_loXjG44xfJ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "from textgrad.engine import get_engine\n",
        "from textgrad import Variable\n",
        "from textgrad.optimizer import TextualGradientDescent\n",
        "from textgrad.loss import TextLoss\n",
        "from textgrad.engine.openai import ChatOpenAI\n",
        "\n",
        "engine = ChatOpenAI(model_string='gpt-3.5-turbo')\n",
        "eval_engine = ChatOpenAI(model_string='gpt-4o')"
      ],
      "metadata": {
        "id": "imcaXNen5ats"
      },
      "id": "imcaXNen5ats",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c65fb4456d84c8fc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:43:17.669096228Z",
          "start_time": "2024-06-11T15:43:17.665325560Z"
        },
        "id": "c65fb4456d84c8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aed2e1ab-1ba3-4583-caec-e634986a81ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! I'm here and ready to assist you. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x = Variable(\"A sntence with a typo\", role_description=\"The input sentence\", requires_grad=True)\n",
        "engine.generate(\"Hello how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b627edc07c0d3737",
      "metadata": {
        "collapsed": false,
        "id": "b627edc07c0d3737"
      },
      "source": [
        "## Introduction: Loss\n",
        "\n",
        "Again, Loss in TextGrad is the metaphorical equivalent of loss in PyTorch. We use Losses in different form in TextGrad but for now we will focus on a simple TextLoss. TextLoss is going to evaluate the loss wrt a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "252e0a0152b81f14",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:32.894722136Z",
          "start_time": "2024-06-11T15:44:32.890708561Z"
        },
        "id": "252e0a0152b81f14"
      },
      "outputs": [],
      "source": [
        "system_prompt = Variable(\"Evaluate the correctness of this sentence\", role_description=\"The system prompt\")\n",
        "loss = TextLoss(system_prompt, engine=engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff137c99e0659dcc",
      "metadata": {
        "collapsed": false,
        "id": "ff137c99e0659dcc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6f05ec2bf907b3ba",
      "metadata": {
        "collapsed": false,
        "id": "6f05ec2bf907b3ba"
      },
      "source": [
        "## Introduction: Optimizer\n",
        "\n",
        "Keeping on the analogy with PyTorch, the optimizer in TextGrad is the object that will update the parameters of the model. In this case, the parameters are the variables that have `requires_grad` set to `True`.\n",
        "\n",
        "**NOTE** This is a text optimizer! It will do all operations with text!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f93f80b9e3ad36",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:33.741130951Z",
          "start_time": "2024-06-11T15:44:33.734977769Z"
        },
        "id": "78f93f80b9e3ad36"
      },
      "outputs": [],
      "source": [
        "optimizer = TextualGradientDescent(parameters=[x], engine=engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26883eb74ce0d01",
      "metadata": {
        "collapsed": false,
        "id": "d26883eb74ce0d01"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "We can now put all the pieces together. We have a variable, an engine, a loss, and an optimizer. We can now run a single optimization step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9817e0ae0179376d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:41.730132530Z",
          "start_time": "2024-06-11T15:44:34.997777872Z"
        },
        "id": "9817e0ae0179376d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0c7126-5b84-4d23-bf4f-7157f75c93ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ],
      "source": [
        "l = loss(x)\n",
        "l.backward(engine)\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e3fab0efdd579e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:41.738985151Z",
          "start_time": "2024-06-11T15:44:41.731989729Z"
        },
        "id": "77e3fab0efdd579e",
        "outputId": "6abafe38-9030-4c5b-f12c-6f295826553c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A sentence with a typo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "x.value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution optimization\n",
        "\n",
        "From this [notebook](https://colab.research.google.com/github/zou-group/TextGrad/blob/main/examples/notebooks/Tutorial-Solution-Optimization.ipynb) the result from our model is not good compared gpt4o."
      ],
      "metadata": {
        "id": "2v3f5XKg-7p4"
      },
      "id": "2v3f5XKg-7p4"
    },
    {
      "cell_type": "code",
      "source": [
        "import textgrad as tg\n",
        "\n",
        "initial_solution = \"\"\"To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula:\n",
        "x = (-b ± √(b^2 - 4ac)) / 2a\n",
        "a = 3, b = -7, c = 2\n",
        "x = (7 ± √((-7)^2 + 4(3)(2))) / 6\n",
        "x = (7 ± √73) / 6\n",
        "The solutions are:\n",
        "x1 = (7 + √73)\n",
        "x2 = (7 - √73)\"\"\"\n",
        "\n",
        "solution = tg.Variable(initial_solution,\n",
        "                       requires_grad=True,\n",
        "                       role_description=\"solution to the math question\")\n",
        "\n",
        "loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math question.\n",
        "Do not attempt to solve it yourself, do not give a solution, only identify errors. Be super concise.\"\"\",\n",
        "                                 requires_grad=False,\n",
        "                                 role_description=\"system prompt\")\n",
        "\n",
        "loss_fn = tg.TextLoss(loss_system_prompt, engine=eval_engine)\n",
        "# We have optimizer here that adjust the `solution`\n",
        "optimizer = tg.TGD(parameters=[solution], engine=eval_engine)"
      ],
      "metadata": {
        "id": "vkmGWyGv_FWi"
      },
      "id": "vkmGWyGv_FWi",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We compute the loss or in this contet create feedback base on our loss_system_prompt\n",
        "loss = loss_fn(solution)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAXPaDlJ_gyh",
        "outputId": "d7f7a545-2212-43d7-c820-14b5b17a38b8"
      },
      "id": "VAXPaDlJ_gyh",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=The solutions are missing the denominator 6.\n",
              "\n",
              "Correct solutions:\n",
              "x1 = (7 + √73) / 6\n",
              "x2 = (7 - √73) / 6, role=response from the language model, grads=)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we will create compute the natural language gradients (feedback) and store it in solution Variable (variable will be the parameters)\n",
        "loss.backward(engine=eval_engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "074069iT_9H6",
        "outputId": "ab8f9013-9982-49b6-babd-e2c0c32180a0"
      },
      "id": "074069iT_9H6",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i8_UnBvcqb3",
        "outputId": "15359fd6-900b-42d4-8791-7b1d3268bc20"
      },
      "id": "7i8_UnBvcqb3",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The solutions are missing the denominator 6.\n",
            "\n",
            "Correct solutions:\n",
            "x1 = (7 + √73) / 6\n",
            "x2 = (7 - √73) / 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we base on natuarl laguage gradients we update the text/solution/prompt\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVxMtkgNcZTc",
        "outputId": "45cdb0b4-ec26-4542-ba8c-35b646de9292"
      },
      "id": "lVxMtkgNcZTc",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SwyMFaXccfO",
        "outputId": "e482a942-bfef-44cd-8e2f-f77cf9aaea7e"
      },
      "id": "9SwyMFaXccfO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula:\n",
            "x = (-b ± √(b^2 - 4ac)) / 2a\n",
            "a = 3, b = -7, c = 2\n",
            "x = (7 ± √((-7)^2 + 4(3)(2))) / 6\n",
            "x = (7 ± √73) / 6\n",
            "The solutions are:\n",
            "x1 = (7 + √73) / 6\n",
            "x2 = (7 - √73) / 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Another solution optimization example\n",
        "\n",
        "import textgrad as tg\n",
        "\n",
        "\n",
        "model = tg.BlackboxLLM(engine=engine)\n",
        "question_string = (\"If it takes 1 hour to dry 25 shirts under the sun, \"\n",
        "                   \"how long will it take to dry 30 shirts under the sun? \"\n",
        "                   \"Reason step by step\")\n",
        "\n",
        "question = tg.Variable(question_string,\n",
        "                       role_description=\"question to the LLM\",\n",
        "                       requires_grad=False)\n",
        "\n",
        "answer = model(question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gko3gsprwqqM",
        "outputId": "1eda9752-145a-4589-80d0-69a96585359c",
        "cellView": "form"
      },
      "id": "Gko3gsprwqqM",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=Sure, I'd be happy to help you reason through this problem!\n",
              "\n",
              "1. First, let's consider how long it takes to dry one shirt in the sun. From the information given, we know that 25 shirts take 1 hour to dry. So, if we divide the number of shirts by the time it takes to dry them, we can find out how long it takes to dry one shirt. That is, 25 shirts / 1 hour = 1 shirt per hour.\n",
              "\n",
              "2. Now, if we want to dry 30 shirts, and it takes 1 hour to dry 1 shirt, then to find out how long it will take to dry 30 shirts, we simply multiply the number of shirts by the time it takes to dry one shirt. That is, 30 shirts * 1 hour per shirt = 30 hours.\n",
              "\n",
              "However, this answer assumes that you can only dry one shirt at a time, which may not be the case. If you have space to lay out all 30 shirts at once, then the drying time would still be 1 hour, just like it was for 25 shirts. The drying time depends on the rate of drying (which seems to be about 25 shirts per hour in this case), not the total amount of time each shirt needs to dry.\n",
              "\n",
              "So, the answer could be either 30 hours (if you can only dry one shirt at a time) or 1 hour (if you can dry all 30 shirts at once). I would need more information about your drying setup to give a more precise answer., role=response from the language model, grads=)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer.set_role_description(\"concise and accurate answer to the question\")\n",
        "\n",
        "optimizer = tg.TGD(parameters=[answer], engine=eval_engine)\n",
        "evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
        "                           \"Evaluate any given answer to this question, \"\n",
        "                           \"be smart, logical, and very critical. \"\n",
        "                           \"Just provide concise feedback.\")\n",
        "\n",
        "\n",
        "# TextLoss is a natural-language specified loss function that describes\n",
        "# how we want to evaluate the reasoning.\n",
        "loss_fn = tg.TextLoss(evaluation_instruction, engine=eval_engine)"
      ],
      "metadata": {
        "id": "e3TE4ZH81q5o"
      },
      "id": "e3TE4ZH81q5o",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always clear previous gradient to not affect the subsequent gradient computation\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "wzBFevE210Zb"
      },
      "id": "wzBFevE210Zb",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(answer)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnUJF8uanYCj",
        "outputId": "4213e342-f269-4ec4-89ff-b2f5e27d1979"
      },
      "id": "nnUJF8uanYCj",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=The answer to this question depends on the assumption made about the drying process. If we assume that the drying process is parallelizable, meaning that all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. However, if we assume that the drying process is sequential, meaning that shirts can only be dried one at a time, then it would take 30 hours to dry 30 shirts.\n",
              "\n",
              "The given answer of 30 hours is based on the assumption that the drying process is sequential. However, this assumption may not be valid, as it is not explicitly stated in the question. Therefore, the answer of 30 hours should be evaluated with caution, and the assumption of a sequential drying process should be made explicit.\n",
              "\n",
              "In conclusion, the answer to this question depends on the assumption made about the drying process. If the drying process is parallelizable, then the answer is 1 hour. If the drying process is sequential, then the answer is 30 hours. The given answer of 30 hours is based on the assumption of a sequential drying process, and should be evaluated with caution., role=response from the language model, grads=)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward(engine=eval_engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMUPl89nnc0R",
        "outputId": "939079f2-2042-4f5d-f8dd-9a5e7c5979ad"
      },
      "id": "VMUPl89nnc0R",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0qxTtcUneSX",
        "outputId": "8f58b193-4108-4385-e406-9c1153951148"
      },
      "id": "c0qxTtcUneSX",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygnITwpunfRj",
        "outputId": "f2d04a94-c321-4512-a4ae-4168d6f91fa7"
      },
      "id": "ygnITwpunfRj",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=Assuming the drying process is sequential, meaning shirts can only be dried one at a time, it would take 30 hours to dry 30 shirts. However, if the drying process is parallelizable, meaning all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. The given answer of 30 hours is based on the assumption of a sequential drying process., role=concise and accurate answer to the question, grads=Here is a conversation:\n",
              "\n",
              "<CONVERSATION><LM_SYSTEM_PROMPT> Here's a question: If it takes 1 hour to dry 25 shirts under the sun, how long will it take to dry 30 shirts under the sun? Reason step by step. Evaluate any given answer to this question, be smart, logical, and very critical. Just provide concise feedback. </LM_SYSTEM_PROMPT>\n",
              "\n",
              "<LM_INPUT> Sure, I'd be happy to help you reason through this problem!\n",
              "\n",
              "1. First, let's consider how long it takes to dry one shirt in the sun. From the information given, we know that 25 shirts take 1 hour to dry. So, if we divide the number of shirts by the time it takes to dry them, we can find out how long it takes to dry one shirt. That is, 25 shirts / 1 hour = 1 shirt per hour.\n",
              "\n",
              "2. Now, if we want to dry 30 shirts, and it takes 1 hour to dry 1 shirt, then to find out how long it will take to dry 30 shirts, we simply multiply the number of shirts by the time it takes to dry one shirt. That is, 30 shirts * 1 hour per shirt = 30 hours.\n",
              "\n",
              "However, this answer assumes that you can only dry one shirt at a time, which may not be the case. If you have space to lay out all 30 shirts at once, then the drying time would still be 1 hour, just like it was for 25 shirts. The drying time depends on the rate of drying (which seems to be about 25 shirts per hour in this case), not the total amount of time each shirt needs to dry.\n",
              "\n",
              "So, the answer could be either 30 hours (if you can only dry one shirt at a time) or 1 hour (if you can dry all 30 shirts at once). I would need more information about your drying setup to give a more precise answer. </LM_INPUT>\n",
              "\n",
              "<LM_OUTPUT> The answer to this question depends on the assumption made about the drying process. If we assume that the drying process is parallelizable, meaning that all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. However, if we assume that the drying process is sequential, meaning that shirts can only be dried one at a time, then it would take 30 hours to dry 30 shirts.\n",
              "\n",
              "The given answer of 30 hours is based on the assumption that the drying process is sequential. However, this assumption may not be valid, as it is not explicitly stated in the question. Therefore, the answer of 30 hours should be evaluated with caution, and the assumption of a sequential drying process should be made explicit.\n",
              "\n",
              "In conclusion, the answer to this question depends on the assumption made about the drying process. If the drying process is parallelizable, then the answer is 1 hour. If the drying process is sequential, then the answer is 30 hours. The given answer of 30 hours is based on the assumption of a sequential drying process, and should be evaluated with caution. </LM_OUTPUT>\n",
              "\n",
              "</CONVERSATION>\n",
              "\n",
              "This conversation is potentially part of a larger system. The output is used as response from the language model\n",
              "\n",
              "Here is the feedback we got for concise and accurate answer to the question in the conversation:\n",
              "\n",
              "<FEEDBACK>The variable provides a detailed and well-reasoned response to the question, but it could be improved in a few ways to better meet the objective of being a concise and accurate answer.\n",
              "\n",
              "1. Simplify the explanation: The variable provides a detailed step-by-step explanation, which is helpful for understanding the reasoning process. However, it could be simplified to make it more concise. For example, the variable could state the assumption that the drying process is sequential and then directly provide the answer of 30 hours.\n",
              "2. Clarify the assumption: The variable mentions that the drying process could be parallelizable, but it does not explicitly state that the answer of 30 hours is based on the assumption of a sequential drying process. This could be made more explicit to avoid confusion.\n",
              "3. Provide a more precise answer: The variable ends by stating that more information is needed to provide a more precise answer. However, it could be improved by providing a more definitive answer based on the information given. For example, the variable could state that the answer is 30 hours, assuming a sequential drying process, and then provide an alternative answer if the drying process is parallelizable.\n",
              "\n",
              "Overall, the variable provides a good response to the question, but it could be improved by simplifying the explanation, clarifying the assumption, and providing a more precise answer.</FEEDBACK>\n",
              "\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How much is the loss\n",
        "print(f\"Loss: {loss.value}\")\n",
        "print('---' * 100)\n",
        "# Display the gradient that would improve the next iteration.\n",
        "print(f\"Gradient: {answer.gradients}\")\n",
        "print('---' * 100)\n",
        "# Display the actual answer\n",
        "print(f\"Answer: {answer}\")\n",
        "print('---' * 100)\n",
        "print(f\"Value: {answer.value}\")\n",
        "print('---' * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDnbca6Z6xGc",
        "outputId": "dae34fce-33c7-411d-958c-3ca59e6b1157"
      },
      "id": "XDnbca6Z6xGc",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: The answer to this question depends on the assumption made about the drying process. If we assume that the drying process is parallelizable, meaning that all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. However, if we assume that the drying process is sequential, meaning that shirts can only be dried one at a time, then it would take 30 hours to dry 30 shirts.\n",
            "\n",
            "The given answer of 30 hours is based on the assumption that the drying process is sequential. However, this assumption may not be valid, as it is not explicitly stated in the question. Therefore, the answer of 30 hours should be evaluated with caution, and the assumption of a sequential drying process should be made explicit.\n",
            "\n",
            "In conclusion, the answer to this question depends on the assumption made about the drying process. If the drying process is parallelizable, then the answer is 1 hour. If the drying process is sequential, then the answer is 30 hours. The given answer of 30 hours is based on the assumption of a sequential drying process, and should be evaluated with caution.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Gradient: {Variable(value=The variable provides a detailed and well-reasoned response to the question, but it could be improved in a few ways to better meet the objective of being a concise and accurate answer.\n",
            "\n",
            "1. Simplify the explanation: The variable provides a detailed step-by-step explanation, which is helpful for understanding the reasoning process. However, it could be simplified to make it more concise. For example, the variable could state the assumption that the drying process is sequential and then directly provide the answer of 30 hours.\n",
            "2. Clarify the assumption: The variable mentions that the drying process could be parallelizable, but it does not explicitly state that the answer of 30 hours is based on the assumption of a sequential drying process. This could be made more explicit to avoid confusion.\n",
            "3. Provide a more precise answer: The variable ends by stating that more information is needed to provide a more precise answer. However, it could be improved by providing a more definitive answer based on the information given. For example, the variable could state that the answer is 30 hours, assuming a sequential drying process, and then provide an alternative answer if the drying process is parallelizable.\n",
            "\n",
            "Overall, the variable provides a good response to the question, but it could be improved by simplifying the explanation, clarifying the assumption, and providing a more precise answer., role=feedback to concise and accurate answer to the question, grads=)}\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Answer: Assuming the drying process is sequential, meaning shirts can only be dried one at a time, it would take 30 hours to dry 30 shirts. However, if the drying process is parallelizable, meaning all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. The given answer of 30 hours is based on the assumption of a sequential drying process.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Value: Assuming the drying process is sequential, meaning shirts can only be dried one at a time, it would take 30 hours to dry 30 shirts. However, if the drying process is parallelizable, meaning all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. The given answer of 30 hours is based on the assumption of a sequential drying process.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize our computation graph.\n",
        "loss.generate_graph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RIi3NdJL7RhG",
        "outputId": "5c803952-2dfd-47b5-b719-fc649f809120"
      },
      "id": "RIi3NdJL7RhG",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"380pt\" height=\"510pt\"\n viewBox=\"0.00 0.00 380.00 510.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 506)\">\n<title>%3</title>\n<polygon fill=\"lightgrey\" stroke=\"transparent\" points=\"-4,4 -4,-506 376,-506 376,4 -4,4\"/>\n<!-- 139214972985952 -->\n<g id=\"node1\" class=\"node\">\n<title>139214972985952</title>\n<polygon fill=\"lavender\" stroke=\"black\" points=\"279.5,-278 92.5,-278 92.5,0 279.5,0 279.5,-278\"/>\n<text text-anchor=\"start\" x=\"109\" y=\"-265.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Role: </text>\n<text text-anchor=\"start\" x=\"133\" y=\"-265.6\" font-family=\"Arial\" font-size=\"8.00\"> Response from the language model</text>\n<text text-anchor=\"start\" x=\"99.5\" y=\"-257.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Value: </text>\n<text text-anchor=\"start\" x=\"127.5\" y=\"-257.6\" font-family=\"Arial\" font-size=\"8.00\"> The answer to this question depends on</text>\n<text text-anchor=\"start\" x=\"117.5\" y=\"-249.6\" font-family=\"Arial\" font-size=\"8.00\">the assumption made about the drying</text>\n<text text-anchor=\"start\" x=\"119.5\" y=\"-241.6\" font-family=\"Arial\" font-size=\"8.00\">process. If we assume that the drying</text>\n<text text-anchor=\"start\" x=\"118.5\" y=\"-233.6\" font-family=\"Arial\" font-size=\"8.00\">process is parallelizable, meaning that</text>\n<text text-anchor=\"start\" x=\"126\" y=\"-225.6\" font-family=\"Arial\" font-size=\"8.00\">all shirts can be dried at the same</text>\n<text text-anchor=\"start\" x=\"121\" y=\"-217.6\" font-family=\"Arial\" font-size=\"8.00\">time, then it would still take 1 hour to</text>\n<text text-anchor=\"start\" x=\"124.5\" y=\"-209.6\" font-family=\"Arial\" font-size=\"8.00\">dry 30 shirts, just like it took 1 hour</text>\n<text text-anchor=\"start\" x=\"116\" y=\"-201.6\" font-family=\"Arial\" font-size=\"8.00\">to dry 25 shirts. However, if we assume</text>\n<text text-anchor=\"start\" x=\"121\" y=\"-193.6\" font-family=\"Arial\" font-size=\"8.00\">that the drying process is sequential,</text>\n<text text-anchor=\"start\" x=\"120.5\" y=\"-185.6\" font-family=\"Arial\" font-size=\"8.00\">meaning that shirts can only be dried</text>\n<text text-anchor=\"start\" x=\"123.5\" y=\"-177.6\" font-family=\"Arial\" font-size=\"8.00\">one at a time, then it would take 30</text>\n<text text-anchor=\"start\" x=\"114\" y=\"-169.6\" font-family=\"Arial\" font-size=\"8.00\">hours to dry 30 shirts. The given answer</text>\n<text text-anchor=\"start\" x=\"116\" y=\"-161.6\" font-family=\"Arial\" font-size=\"8.00\">of 30 hours is based on the assumption</text>\n<text text-anchor=\"start\" x=\"121\" y=\"-153.6\" font-family=\"Arial\" font-size=\"8.00\">that the drying process is sequential.</text>\n<text text-anchor=\"start\" x=\"119.5\" y=\"-145.6\" font-family=\"Arial\" font-size=\"8.00\">However, this assumption may not be</text>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-137.6\" font-family=\"Arial\" font-size=\"8.00\">valid, as it is not explicitly stated in</text>\n<text text-anchor=\"start\" x=\"117\" y=\"-129.6\" font-family=\"Arial\" font-size=\"8.00\">the question. Therefore, the answer of</text>\n<text text-anchor=\"start\" x=\"125\" y=\"-121.6\" font-family=\"Arial\" font-size=\"8.00\">30 hours should be evaluated with</text>\n<text text-anchor=\"start\" x=\"128\" y=\"-113.6\" font-family=\"Arial\" font-size=\"8.00\">caution, and the assumption of a</text>\n<text text-anchor=\"start\" x=\"111\" y=\"-105.6\" font-family=\"Arial\" font-size=\"8.00\">sequential drying process should be made</text>\n<text text-anchor=\"start\" x=\"122.5\" y=\"-97.6\" font-family=\"Arial\" font-size=\"8.00\">explicit. In conclusion, the answer to</text>\n<text text-anchor=\"start\" x=\"113.5\" y=\"-89.6\" font-family=\"Arial\" font-size=\"8.00\">this question depends on the assumption</text>\n<text text-anchor=\"start\" x=\"119.5\" y=\"-81.6\" font-family=\"Arial\" font-size=\"8.00\">made about the drying process. If the</text>\n<text text-anchor=\"start\" x=\"122\" y=\"-73.6\" font-family=\"Arial\" font-size=\"8.00\">drying process is parallelizable, then</text>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-65.6\" font-family=\"Arial\" font-size=\"8.00\">the answer is 1 hour. If the drying</text>\n<text text-anchor=\"start\" x=\"118\" y=\"-57.6\" font-family=\"Arial\" font-size=\"8.00\">process is sequential, then the answer</text>\n<text text-anchor=\"start\" x=\"122\" y=\"-49.6\" font-family=\"Arial\" font-size=\"8.00\">is 30 hours. The given answer of 30</text>\n<text text-anchor=\"start\" x=\"118\" y=\"-41.6\" font-family=\"Arial\" font-size=\"8.00\">hours is based on the assumption of a</text>\n<text text-anchor=\"start\" x=\"113.5\" y=\"-33.6\" font-family=\"Arial\" font-size=\"8.00\">sequential drying process, and should be</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-25.6\" font-family=\"Arial\" font-size=\"8.00\">evaluated with caution.</text>\n<text text-anchor=\"start\" x=\"166\" y=\"-17.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Grad Fn: </text>\n<text text-anchor=\"start\" x=\"203\" y=\"-17.6\" font-family=\"Arial\" font-size=\"8.00\"> </text>\n<text text-anchor=\"start\" x=\"105\" y=\"-9.6\" font-family=\"Arial\" font-size=\"8.00\">textgrad.autograd.llm_ops.LLMCall.backward</text>\n</g>\n<!-- 139214962247088 -->\n<g id=\"node2\" class=\"node\">\n<title>139214962247088</title>\n<polygon fill=\"lavender\" stroke=\"black\" points=\"178,-402 0,-402 0,-324 178,-324 178,-402\"/>\n<text text-anchor=\"start\" x=\"16.5\" y=\"-389.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Role: </text>\n<text text-anchor=\"start\" x=\"40.5\" y=\"-389.6\" font-family=\"Arial\" font-size=\"8.00\"> System prompt for the evaluation</text>\n<text text-anchor=\"start\" x=\"7\" y=\"-381.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Value: </text>\n<text text-anchor=\"start\" x=\"35\" y=\"-381.6\" font-family=\"Arial\" font-size=\"8.00\"> Here&#39;s a question: If it takes 1 hour to</text>\n<text text-anchor=\"start\" x=\"22.5\" y=\"-373.6\" font-family=\"Arial\" font-size=\"8.00\">dry 25 shirts under the sun, how long</text>\n<text text-anchor=\"start\" x=\"26\" y=\"-365.6\" font-family=\"Arial\" font-size=\"8.00\">will it take to dry 30 shirts under the</text>\n<text text-anchor=\"start\" x=\"17.5\" y=\"-357.6\" font-family=\"Arial\" font-size=\"8.00\">sun? Reason step by step. Evaluate any</text>\n<text text-anchor=\"start\" x=\"18\" y=\"-349.6\" font-family=\"Arial\" font-size=\"8.00\">given answer to this question, be smart,</text>\n<text text-anchor=\"start\" x=\"24.5\" y=\"-341.6\" font-family=\"Arial\" font-size=\"8.00\">logical, and very critical. Just provide</text>\n<text text-anchor=\"start\" x=\"57\" y=\"-333.6\" font-family=\"Arial\" font-size=\"8.00\">concise feedback.</text>\n</g>\n<!-- 139214962247088&#45;&gt;139214972985952 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139214962247088&#45;&gt;139214972985952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M105.67,-323.84C110.38,-313.06 115.81,-300.64 121.57,-287.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"124.82,-288.77 125.61,-278.21 118.4,-285.97 124.82,-288.77\"/>\n</g>\n<!-- 139214972988880 -->\n<g id=\"node3\" class=\"node\">\n<title>139214972988880</title>\n<polygon fill=\"lavender\" stroke=\"black\" points=\"372,-434 196,-434 196,-292 372,-292 372,-434\"/>\n<text text-anchor=\"start\" x=\"206.5\" y=\"-421.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Role: </text>\n<text text-anchor=\"start\" x=\"230.5\" y=\"-421.6\" font-family=\"Arial\" font-size=\"8.00\"> Concise and accurate answer to the</text>\n<text text-anchor=\"start\" x=\"268.5\" y=\"-413.6\" font-family=\"Arial\" font-size=\"8.00\">question</text>\n<text text-anchor=\"start\" x=\"214\" y=\"-405.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Value: </text>\n<text text-anchor=\"start\" x=\"242\" y=\"-405.6\" font-family=\"Arial\" font-size=\"8.00\"> Assuming the drying process is</text>\n<text text-anchor=\"start\" x=\"216\" y=\"-397.6\" font-family=\"Arial\" font-size=\"8.00\">sequential, meaning shirts can only be</text>\n<text text-anchor=\"start\" x=\"220.5\" y=\"-389.6\" font-family=\"Arial\" font-size=\"8.00\">dried one at a time, it would take 30</text>\n<text text-anchor=\"start\" x=\"217.5\" y=\"-381.6\" font-family=\"Arial\" font-size=\"8.00\">hours to dry 30 shirts. However, if the</text>\n<text text-anchor=\"start\" x=\"229\" y=\"-373.6\" font-family=\"Arial\" font-size=\"8.00\">drying process is parallelizable,</text>\n<text text-anchor=\"start\" x=\"218.5\" y=\"-365.6\" font-family=\"Arial\" font-size=\"8.00\">meaning all shirts can be dried at the</text>\n<text text-anchor=\"start\" x=\"222\" y=\"-357.6\" font-family=\"Arial\" font-size=\"8.00\">same time, then it would still take 1</text>\n<text text-anchor=\"start\" x=\"221.5\" y=\"-349.6\" font-family=\"Arial\" font-size=\"8.00\">hour to dry 30 shirts, just like it took</text>\n<text text-anchor=\"start\" x=\"225\" y=\"-341.6\" font-family=\"Arial\" font-size=\"8.00\">1 hour to dry 25 shirts. The given</text>\n<text text-anchor=\"start\" x=\"221\" y=\"-333.6\" font-family=\"Arial\" font-size=\"8.00\">answer of 30 hours is based on the</text>\n<text text-anchor=\"start\" x=\"224.5\" y=\"-325.6\" font-family=\"Arial\" font-size=\"8.00\">assumption of a sequential drying</text>\n<text text-anchor=\"start\" x=\"268.5\" y=\"-317.6\" font-family=\"Arial\" font-size=\"8.00\">process.</text>\n<text text-anchor=\"start\" x=\"264\" y=\"-309.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Grad Fn: </text>\n<text text-anchor=\"start\" x=\"301\" y=\"-309.6\" font-family=\"Arial\" font-size=\"8.00\"> </text>\n<text text-anchor=\"start\" x=\"203\" y=\"-301.6\" font-family=\"Arial\" font-size=\"8.00\">textgrad.autograd.llm_ops.LLMCall.backward</text>\n</g>\n<!-- 139214972988880&#45;&gt;139214972985952 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139214972988880&#45;&gt;139214972985952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M252.99,-291.76C252.38,-290.37 251.76,-288.97 251.14,-287.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"254.23,-285.89 246.99,-278.15 247.82,-288.71 254.23,-285.89\"/>\n</g>\n<!-- 139214972977312 -->\n<g id=\"node4\" class=\"node\">\n<title>139214972977312</title>\n<polygon fill=\"lavender\" stroke=\"black\" points=\"365,-502 203,-502 203,-448 365,-448 365,-502\"/>\n<text text-anchor=\"start\" x=\"237.5\" y=\"-489.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Role: </text>\n<text text-anchor=\"start\" x=\"261.5\" y=\"-489.6\" font-family=\"Arial\" font-size=\"8.00\"> Question to the llm</text>\n<text text-anchor=\"start\" x=\"212.5\" y=\"-481.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Value: </text>\n<text text-anchor=\"start\" x=\"240.5\" y=\"-481.6\" font-family=\"Arial\" font-size=\"8.00\"> If it takes 1 hour to dry 25 shirts</text>\n<text text-anchor=\"start\" x=\"218\" y=\"-473.6\" font-family=\"Arial\" font-size=\"8.00\">under the sun, how long will it take to</text>\n<text text-anchor=\"start\" x=\"210\" y=\"-465.6\" font-family=\"Arial\" font-size=\"8.00\">dry 30 shirts under the sun? Reason step</text>\n<text text-anchor=\"start\" x=\"271\" y=\"-457.6\" font-family=\"Arial\" font-size=\"8.00\">by step</text>\n</g>\n<!-- 139214972977312&#45;&gt;139214972988880 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139214972977312&#45;&gt;139214972988880</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-447.93C284,-446.72 284,-445.48 284,-444.21\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-444.14 284,-434.14 280.5,-444.14 287.5,-444.14\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7e9d830332e0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Promp optimization\n",
        "\n",
        "- We will use `MultiFieldTokenParsedEvaluation` to compute the loss\n",
        "     - This is"
      ],
      "metadata": {
        "id": "t-_rRDJhosYH"
      },
      "id": "t-_rRDJhosYH"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "XU8EHKzp-b9n"
      },
      "id": "XU8EHKzp-b9n",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initial Prompt\n",
        "from textwrap import dedent\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT=dedent(\n",
        "    f\"\"\"Your goal is to analyze the user message and generate sub-task based on available tools.\n",
        "\n",
        "    ## Available Tools:\n",
        "    Identify missing data in the platform - This tool is only to get user personal details related to their immigration. You should ONLY use this tool if user wants to know any missing or required details on the platform.\n",
        "    Ask immigration wikipedia - Use this tool to get internal immigration wiki that helps you find about policies, immigration law, how-to and frequently asked questions. Remember to pass question as argument.\n",
        "    Relocation services provided to user - Use this tool to provide what services we offer to the user. This tool also provide users next step or task for their relocation journey\n",
        "\n",
        "    ## Output:\n",
        "    1. Using the message provided and the list of available tools, decompose the task into smaller, manageable sub-tasks. For each sub-task, specify which tool can be used to complete it. Ensure the sub-tasks cover all aspects of the original message to provide a comprehensive response.\n",
        "    2. Your final output should be JSON format.\n",
        "\n",
        "\n",
        "    ## Examples:\n",
        "\n",
        "    Example 1:\n",
        "    User Message:\n",
        "    I recently moved to Germany with my family in April 2022. We need to register for health insurance and enroll our children in school. Could you assist us with these processes? Additionally, we haven't received our residency permits yet. Can you help with that? Best regards, Alex Johnson\n",
        "\n",
        "    Final output:\n",
        "    ```json\n",
        "      [\n",
        "        {{\n",
        "          \"task\": \"Search for list of health insurance providers in Germany.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Search what is the process for health insurance registration.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Search the documents needed for health insurance registration.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Search information on local schools and enrollment procedures.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Search how long the the residency permit process.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "         }}\n",
        "      ]\n",
        "    ```\n",
        "    Example 2:\n",
        "    User Message:\n",
        "    I have added all the info and uploaded the documents. Please let me know if everything is corrected and if something is missing also what would be our next step?\n",
        "\n",
        "    Final output:\n",
        "    ```json\n",
        "      [\n",
        "        {{\n",
        "          \"task\": \"Check if there is still missing data in the platform.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Check what is the next task assigned to user for their relocation.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }}\n",
        "      ]\n",
        "    ```\n",
        "\n",
        "    Begin!\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "9Zs-k0HI8oKn"
      },
      "id": "9Zs-k0HI8oKn",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textgrad as tg\n",
        "\n",
        "# Testing the 0-shot performance of the evaluation engine\n",
        "system_prompt = tg.Variable(SYSTEM_PROMPT,\n",
        "                            requires_grad=True,\n",
        "                            role_description=\"system prompt to the language model\")\n",
        "\n",
        "user_message = tg.Variable(\"\"\"Should I print the signed documents and bring them to the immigration office, or will you update them electronically? Alternatively, can a local HR fill out and sign the EzB form, and if so, what should I do with the correct versions of the documents?\"\"\", requires_grad=False, role_description=\"query to the language model\")\n",
        "model = tg.BlackboxLLM(engine=engine, system_prompt=system_prompt)\n",
        "answer = model(user_message)\n",
        "# answer\n",
        "# loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math question.\n",
        "# Do not attempt to solve it yourself, do not give a solution, only identify errors. Be super concise.\"\"\",\n",
        "#                                  requires_grad=False,\n",
        "#                                  role_description=\"system prompt\")\n",
        "\n",
        "# loss_fn = tg.TextLoss(loss_system_prompt, engine=eval_engine)\n",
        "# # We have optimizer here that adjust the `solution`\n",
        "# optimizer = tg.TGD(parameters=[system_prompt], engine=eval_engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i00AS7_ovF_",
        "outputId": "a22c44f5-31f9-449b-89f3-267f416fcff9"
      },
      "id": "4i00AS7_ovF_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "# answer.set_role_description(\"Each task is clear and has all necessary information base on original task without any ambuiguity.\")\n",
        "optimizer = tg.TGD(parameters=[system_prompt], engine=eval_engine)\n",
        "evaluation_instruction = dedent(f\"\"\"\n",
        "        Evaluate each subtask to answer user question. Each subtask is an independent smaller task to accomplish one part of the question.\n",
        "\n",
        "\n",
        "        Subtask Evaluation Process:\n",
        "        - Subtask has all necessary details from the main question to avoid assumptions.\n",
        "        - Subtask has no vague terms that could lead to different interpretations.\n",
        "        - Check if the subtask includes all relevant details mentioned in the main question (e.g., specific forms, documents, country)\n",
        "\n",
        "        Output:\n",
        "        Provide feedback that must apply on the system_prompt to improve the answer.\n",
        "\n",
        "\n",
        "        <user_message>\n",
        "        {user_message.value}\n",
        "        </user_message>\n",
        "\n",
        "        <system_prompt>\n",
        "        {system_prompt.value}\n",
        "        </system_prompt>\n",
        "      \"\"\")\n",
        "\n",
        "\n",
        "# TextLoss is a natural-language specified loss function that describes\n",
        "# how we want to evaluate the reasoning.\n",
        "loss_fn = tg.TextLoss(evaluation_instruction, engine=eval_engine)"
      ],
      "metadata": {
        "id": "LmpyMAmhAiJ3"
      },
      "id": "LmpyMAmhAiJ3",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(12)"
      ],
      "metadata": {
        "id": "TieHr6fy6bp6"
      },
      "id": "TieHr6fy6bp6",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always clear previous gradient to not affect the subsequent gradient computation\n",
        "optimizer.zero_grad()\n",
        "loss = loss_fn(answer)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6Bxa_Wn6tM6",
        "outputId": "5eee2b42-c052-4467-ff8d-c8857793c930"
      },
      "id": "v6Bxa_Wn6tM6",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=The provided subtasks are well-structured and cover the main aspects of the user's query. However, there are a few areas for improvement to ensure clarity and completeness:\n",
              "\n",
              "1. **Clarify the User's Role and Context**: The user's role (e.g., employee, employer) and the specific context (e.g., country of immigration) should be considered to avoid assumptions.\n",
              "2. **Ensure All Aspects are Covered**: The user's question has multiple parts, including printing and bringing documents, updating them electronically, and the role of local HR. Each part should be addressed explicitly.\n",
              "3. **Avoid Vague Terms**: Ensure that terms like \"correct versions of the documents\" are clearly defined or explained.\n",
              "\n",
              "Here is the revised version:\n",
              "\n",
              "```json\n",
              "[\n",
              "    { \n",
              "      \"task\": \"Check if there is still missing data in the platform.\", \n",
              "      \"tool\": \"Identify missing data in the platform\",\n",
              "      \"explanation\": \"This tool will help determine if there are any required documents or information missing in the platform that need to be completed or uploaded.\"\n",
              "    },\n",
              "    { \n",
              "      \"task\": \"Search for information on updating signed documents electronically with the immigration office.\", \n",
              "      \"tool\": \"Ask immigration wikipedia\",\n",
              "      \"explanation\": \"The internal immigration wiki can provide guidance on whether signed documents can be updated electronically with the immigration office and the process for doing so.\"\n",
              "    },\n",
              "    { \n",
              "      \"task\": \"Find out if a local HR can fill out and sign the EzB form, and what to do with the correct versions of the documents.\", \n",
              "      \"tool\": \"Ask immigration wikipedia\",\n",
              "      \"explanation\": \"The immigration wiki can offer information on whether a local HR can fill out and sign the EzB form, as well as provide guidance on the next steps for handling the correct versions of the documents.\"\n",
              "    },\n",
              "    { \n",
              "      \"task\": \"Check what is the next task assigned to user for their relocation.\", \n",
              "      \"tool\": \"Relocation services provided to user\",\n",
              "      \"explanation\": \"This tool will provide the user with the next steps or tasks in their relocation journey, ensuring they are aware of all necessary actions.\"\n",
              "    }\n",
              "]\n",
              "```\n",
              "\n",
              "**Feedback Summary**:\n",
              "- Ensure the user's role and context are clear to avoid assumptions.\n",
              "- Explicitly address each part of the user's question.\n",
              "- Avoid vague terms and ensure all aspects are covered comprehensively., role=response from the language model, grads=)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward(engine=eval_engine)\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibhZ4QUY74k4",
        "outputId": "3458bec5-c78d-43a3-ed93-315bb73846ca"
      },
      "id": "ibhZ4QUY74k4",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How much is the loss\n",
        "print(f\"Loss: {loss.value}\")\n",
        "print('---' * 100)\n",
        "# Display the gradient that would improve the next iteration.\n",
        "print(f\"Gradient: {answer.gradients}\")\n",
        "print('---' * 100)\n",
        "# Display the actual answer\n",
        "print(f\"Answer: {answer}\")\n",
        "print('---' * 100)\n",
        "print(f\"Value: {answer.value}\")\n",
        "print('---' * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZLMbpgq8DVL",
        "outputId": "ba6838ea-e4c9-4a7b-ea85-28993089d2da"
      },
      "id": "eZLMbpgq8DVL",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: The provided subtasks are well-structured and cover the main aspects of the user's query. However, there are a few areas for improvement to ensure clarity and completeness:\n",
            "\n",
            "1. **Clarify the User's Role and Context**: The user's role (e.g., employee, employer) and the specific context (e.g., country of immigration) should be considered to avoid assumptions.\n",
            "2. **Ensure All Aspects are Covered**: The user's question has multiple parts, including printing and bringing documents, updating them electronically, and the role of local HR. Each part should be addressed explicitly.\n",
            "3. **Avoid Vague Terms**: Ensure that terms like \"correct versions of the documents\" are clearly defined or explained.\n",
            "\n",
            "Here is the revised version:\n",
            "\n",
            "```json\n",
            "[\n",
            "    { \n",
            "      \"task\": \"Check if there is still missing data in the platform.\", \n",
            "      \"tool\": \"Identify missing data in the platform\",\n",
            "      \"explanation\": \"This tool will help determine if there are any required documents or information missing in the platform that need to be completed or uploaded.\"\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Search for information on updating signed documents electronically with the immigration office.\", \n",
            "      \"tool\": \"Ask immigration wikipedia\",\n",
            "      \"explanation\": \"The internal immigration wiki can provide guidance on whether signed documents can be updated electronically with the immigration office and the process for doing so.\"\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Find out if a local HR can fill out and sign the EzB form, and what to do with the correct versions of the documents.\", \n",
            "      \"tool\": \"Ask immigration wikipedia\",\n",
            "      \"explanation\": \"The immigration wiki can offer information on whether a local HR can fill out and sign the EzB form, as well as provide guidance on the next steps for handling the correct versions of the documents.\"\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Check what is the next task assigned to user for their relocation.\", \n",
            "      \"tool\": \"Relocation services provided to user\",\n",
            "      \"explanation\": \"This tool will provide the user with the next steps or tasks in their relocation journey, ensuring they are aware of all necessary actions.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "\n",
            "**Feedback Summary**:\n",
            "- Ensure the user's role and context are clear to avoid assumptions.\n",
            "- Explicitly address each part of the user's question.\n",
            "- Avoid vague terms and ensure all aspects are covered comprehensively.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Gradient: {Variable(value=To improve the response from the language model, consider the following feedback:\n",
            "\n",
            "1. **Clarify the User's Role and Context**:\n",
            "   - The response should explicitly identify the user's role (e.g., employee, employer) and the specific context (e.g., country of immigration). This will help avoid assumptions and ensure the subtasks are tailored to the user's specific situation.\n",
            "   - For example, if the user is an employee, the tasks might differ slightly compared to if the user is an employer.\n",
            "\n",
            "2. **Ensure All Aspects are Covered**:\n",
            "   - The user's question includes multiple parts: printing and bringing documents, updating them electronically, and the role of local HR. Each part should be addressed explicitly in the subtasks.\n",
            "   - For instance, there should be a subtask specifically addressing whether the user needs to print and bring the signed documents to the immigration office.\n",
            "\n",
            "3. **Avoid Vague Terms**:\n",
            "   - Terms like \"correct versions of the documents\" should be clearly defined or explained. This will help avoid any ambiguity and ensure the user understands exactly what is required.\n",
            "   - For example, specify what constitutes the \"correct versions\" and what steps the user should take to ensure they have the correct versions.\n",
            "\n",
            "4. **Add a Subtask for Printing and Bringing Documents**:\n",
            "   - The user's question explicitly asks whether they need to print and bring the signed documents to the immigration office. This should be addressed with a specific subtask.\n",
            "   - For example, a subtask could be: \"Check if the user needs to print and bring the signed documents to the immigration office.\"\n",
            "\n",
            "5. **Ensure Consistency in Subtask Structure**:\n",
            "   - Ensure that each subtask follows a consistent structure, including a clear task description, the appropriate tool, and a detailed explanation.\n",
            "   - This consistency will make it easier for the user to follow and understand the response.\n",
            "\n",
            "By addressing these points, the response from the language model will be more comprehensive, clear, and tailored to the user's specific needs, thereby improving the objective function., role=feedback to response from the language model, grads=)}\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Answer: ```json\n",
            "[\n",
            "    { \n",
            "      \"task\": \"Check if there is still missing data in the platform.\", \n",
            "      \"tool\": \"Identify missing data in the platform\",\n",
            "      \"explanation\": \"This tool will help determine if there are any required documents or information missing in the platform that need to be completed or uploaded.\"\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Search for information on updating signed documents electronically with the immigration office.\", \n",
            "      \"tool\": \"Ask immigration wikipedia\",\n",
            "      \"explanation\": \"The internal immigration wiki can provide guidance on whether signed documents can be updated electronically with the immigration office and the process for doing so.\"\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Find out if a local HR can fill out and sign the EzB form, and what to do with the correct versions of the documents.\", \n",
            "      \"tool\": \"Ask immigration wikipedia\",\n",
            "      \"explanation\": \"The immigration wiki can offer information on whether a local HR can fill out and sign the EzB form, as well as provide guidance on the next steps for handling the correct versions of the documents.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Value: ```json\n",
            "[\n",
            "    { \n",
            "      \"task\": \"Check if there is still missing data in the platform.\", \n",
            "      \"tool\": \"Identify missing data in the platform\",\n",
            "      \"explanation\": \"This tool will help determine if there are any required documents or information missing in the platform that need to be completed or uploaded.\"\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Search for information on updating signed documents electronically with the immigration office.\", \n",
            "      \"tool\": \"Ask immigration wikipedia\",\n",
            "      \"explanation\": \"The internal immigration wiki can provide guidance on whether signed documents can be updated electronically with the immigration office and the process for doing so.\"\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Find out if a local HR can fill out and sign the EzB form, and what to do with the correct versions of the documents.\", \n",
            "      \"tool\": \"Ask immigration wikipedia\",\n",
            "      \"explanation\": \"The immigration wiki can offer information on whether a local HR can fill out and sign the EzB form, as well as provide guidance on the next steps for handling the correct versions of the documents.\"\n",
            "    }\n",
            "]\n",
            "```\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"system_prompt gradient: {system_prompt.gradients}\")\n",
        "print('---' * 100)\n",
        "print(f\"system_prompt: {system_prompt}\")\n",
        "print('---' * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Mg65Bt8jjl",
        "outputId": "44dc70f6-8724-4d43-fde7-539cd0d4bad8"
      },
      "id": "F8Mg65Bt8jjl",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system_prompt gradient: {Variable(value=1. **Clarify the User's Role and Context**:\n",
            "   - **Incorporate User Role Identification**: Add a directive to the system prompt that instructs the language model to identify and explicitly state the user's role (e.g., employee, employer) and the specific context (e.g., country of immigration) based on the user message. This will help tailor the subtasks more accurately to the user's situation.\n",
            "   - **Example Addition**: \"Identify the user's role (e.g., employee, employer) and the specific context (e.g., country of immigration) from the user message and include this information in the subtasks.\"\n",
            "\n",
            "2. **Ensure All Aspects are Covered**:\n",
            "   - **Explicitly Address Each Part of the User's Query**: Modify the system prompt to ensure that the language model addresses each part of the user's query explicitly. This can be achieved by instructing the model to break down the user message into distinct components and generate subtasks for each.\n",
            "   - **Example Addition**: \"Ensure that each part of the user's query is addressed explicitly in the subtasks. Break down the user message into distinct components and generate subtasks for each.\"\n",
            "\n",
            "3. **Avoid Vague Terms**:\n",
            "   - **Define Ambiguous Terms**: Add a directive to the system prompt to avoid using vague terms and to provide clear definitions or explanations for any potentially ambiguous terms.\n",
            "   - **Example Addition**: \"Avoid using vague terms. Provide clear definitions or explanations for any potentially ambiguous terms to ensure the user understands exactly what is required.\"\n",
            "\n",
            "4. **Add a Subtask for Printing and Bringing Documents**:\n",
            "   - **Include Specific Subtasks for User Actions**: Modify the system prompt to ensure that specific user actions, such as printing and bringing documents, are included as distinct subtasks.\n",
            "   - **Example Addition**: \"Include specific subtasks for user actions mentioned in the query, such as printing and bringing documents to the immigration office.\"\n",
            "\n",
            "5. **Ensure Consistency in Subtask Structure**:\n",
            "   - **Standardize Subtask Format**: Add a directive to the system prompt to ensure that each subtask follows a consistent structure, including a clear task description, the appropriate tool, and a detailed explanation.\n",
            "   - **Example Addition**: \"Ensure that each subtask follows a consistent structure, including a clear task description, the appropriate tool, and a detailed explanation.\"\n",
            "\n",
            "By incorporating these improvements into the system prompt, the language model will be better equipped to generate comprehensive, clear, and tailored responses that address all aspects of the user's query, thereby improving the objective function., role=feedback to system prompt to the language model, grads=)}\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "system_prompt: Your goal is to analyze the user message and generate sub-tasks based on available tools.\n",
            "\n",
            "## Available Tools:\n",
            "Identify missing data in the platform - This tool is only to get user personal details related to their immigration. You should ONLY use this tool if the user wants to know any missing or required details on the platform.\n",
            "Ask immigration wikipedia - Use this tool to get internal immigration wiki that helps you find information about policies, immigration law, how-to guides, and frequently asked questions. Remember to pass the question as an argument.\n",
            "Relocation services provided to user - Use this tool to provide information on what services we offer to the user. This tool also provides users with the next steps or tasks for their relocation journey.\n",
            "\n",
            "## Output:\n",
            "1. Identify the user's role (e.g., employee, employer) and the specific context (e.g., country of immigration) from the user message and include this information in the subtasks.\n",
            "2. Using the message provided and the list of available tools, decompose the task into smaller, manageable sub-tasks. For each sub-task, specify which tool can be used to complete it. Ensure the sub-tasks cover all aspects of the original message to provide a comprehensive response.\n",
            "3. Ensure that each part of the user's query is addressed explicitly in the subtasks. Break down the user message into distinct components and generate subtasks for each.\n",
            "4. Avoid using vague terms. Provide clear definitions or explanations for any potentially ambiguous terms to ensure the user understands exactly what is required.\n",
            "5. Include specific subtasks for user actions mentioned in the query, such as printing and bringing documents to the immigration office.\n",
            "6. Ensure that each subtask follows a consistent structure, including a clear task description, the appropriate tool, and a detailed explanation.\n",
            "\n",
            "Your final output should be in JSON format.\n",
            "\n",
            "## Examples:\n",
            "\n",
            "Example 1:\n",
            "User Message:\n",
            "I recently moved to Germany with my family in April 2022. We need to register for health insurance and enroll our children in school. Could you assist us with these processes? Additionally, we haven't received our residency permits yet. Can you help with that? Best regards, Alex Johnson\n",
            "\n",
            "Final output:\n",
            "```json\n",
            "  [\n",
            "    { \n",
            "      \"task\": \"Search for a list of health insurance providers in Germany.\", \n",
            "      \"tool\": <Tool that can accomplish the task>,\n",
            "      \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Search for the process of health insurance registration.\", \n",
            "      \"tool\": <Tool that can accomplish the task>,\n",
            "      \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Search for the documents needed for health insurance registration.\", \n",
            "      \"tool\": <Tool that can accomplish the task>,\n",
            "      \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Search for information on local schools and enrollment procedures.\", \n",
            "      \"tool\": <Tool that can accomplish the task>,\n",
            "      \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Search for the duration of the residency permit process.\", \n",
            "      \"tool\": <Tool that can accomplish the task>,\n",
            "      \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
            "     }\n",
            "  ]\n",
            "``` \n",
            "Example 2:\n",
            "User Message:\n",
            "I have added all the info and uploaded the documents. Please let me know if everything is correct and if something is missing, also what would be our next step?\n",
            "\n",
            "Final output:\n",
            "```json\n",
            "  [\n",
            "    { \n",
            "      \"task\": \"Check if there is still missing data in the platform.\", \n",
            "      \"tool\": <Tool that can accomplish the task>,\n",
            "      \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
            "    },\n",
            "    { \n",
            "      \"task\": \"Check what is the next task assigned to the user for their relocation.\", \n",
            "      \"tool\": <Tool that can accomplish the task>,\n",
            "      \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
            "    }\n",
            "  ]\n",
            "``` \n",
            "\n",
            "Begin!\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt.get_value()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "NgiWbgRCK786",
        "outputId": "9ee55f5e-6dd2-4f58-86c1-552cb01d861c"
      },
      "id": "NgiWbgRCK786",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your goal is to analyze the user message and generate sub-task based on available tools.\\n\\n    ## Available Tools:\\n    Identify missing data in the platform - This tool is only to get user personal details related to their immigration. You should ONLY use this tool if user wants to know any missing or required details on the platform.\\n    Ask immigration wikipedia - Use this tool to get internal immigration wiki that helps you find about policies, immigration law, how-to and frequently asked questions. Remember to pass question as argument.\\n    Relocation services provided to user - Use this tool to provide what services we offer to the user. This tool also provide users next step or task for their relocation journey\\n\\n    ## Output:\\n    1. Using the message provided and the list of available tools, decompose the task into smaller, manageable sub-tasks. For each sub-task, specify which tool can be used to complete it. Ensure the sub-tasks cover all aspects of the original message to provide a comprehensive response.\\n    2. Your final output should be JSON format.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textgrad.loss import MultiFieldTokenParsedEvaluation\n",
        "\n",
        "role_descriptions = [\n",
        "    \"Question for the task\",\n",
        "    \"Ground truth answer\",\n",
        "    \"Reasoning and prediction from the language model\"\n",
        "]\n",
        "\n",
        "evaluation_instruction = \"Below is a question from a question-answering task, the ground truth answer, and reasoning with the final prediction. Is the final prediction correct, i.e. the same as the ground truth answer? Say only 1 (yes) or 0 (no). Return your response within <ACCURACY> </ACCURACY> tags. e.g.<ACCURACY> 0 </ACCURACY> or <ACCURACY> 1 </ACCURACY>\"\n",
        "eval_instruction = Variable(evaluation_instruction, requires_grad=False, role_description=\"evaluation instruction for the task\")\n",
        "eval_fn = MultiFieldTokenParsedEvaluation(\n",
        "    eval_instruction,\n",
        "    engine=eval_engine,\n",
        "    role_descriptions=role_descriptions,\n",
        "    parse_tags=[\"<ACCURACY>\", \"</ACCURACY>\"]\n",
        ")"
      ],
      "metadata": {
        "id": "3RfsvOEbq7xT"
      },
      "id": "3RfsvOEbq7xT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}