{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuaalpuerto/ML-guide/blob/main/Learning_TextGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36a9c615-17a0-455c-8f9c-f0d25fb8824b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:43:10.594204491Z",
          "start_time": "2024-06-11T15:43:10.589328053Z"
        },
        "id": "36a9c615-17a0-455c-8f9c-f0d25fb8824b",
        "outputId": "88ee8607-b371-4a5e-a18f-d3d25d96f08c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for textgrad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU textgrad --progress-bar off\n",
        "!pip install -qU openai --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load fireworks API key\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/MyDrive/env/env.json') as jsonfile:\n",
        "    env = json.load(jsonfile)\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = env['fireworks.ai']['apiKey']\n",
        "# os.environ['OPENAI_BASE_URL'] = \"https://api.fireworks.ai/inference/v1\"\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = env['openai']['apiKey']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_loXjG44xfJ",
        "outputId": "0c91db8f-da16-4b3b-d406-8472b510cf6a"
      },
      "id": "k_loXjG44xfJ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "from textgrad.engine import get_engine\n",
        "from textgrad import Variable\n",
        "from textgrad.optimizer import TextualGradientDescent\n",
        "from textgrad.loss import TextLoss\n",
        "from textgrad.engine.openai import ChatOpenAI\n",
        "\n",
        "engine = ChatOpenAI(model_string='gpt-3.5-turbo')\n",
        "eval_engine = ChatOpenAI(model_string='gpt-4o')"
      ],
      "metadata": {
        "id": "imcaXNen5ats"
      },
      "id": "imcaXNen5ats",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c65fb4456d84c8fc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:43:17.669096228Z",
          "start_time": "2024-06-11T15:43:17.665325560Z"
        },
        "id": "c65fb4456d84c8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aed2e1ab-1ba3-4583-caec-e634986a81ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! I'm here and ready to assist you. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x = Variable(\"A sntence with a typo\", role_description=\"The input sentence\", requires_grad=True)\n",
        "engine.generate(\"Hello how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b627edc07c0d3737",
      "metadata": {
        "collapsed": false,
        "id": "b627edc07c0d3737"
      },
      "source": [
        "## Introduction: Loss\n",
        "\n",
        "Again, Loss in TextGrad is the metaphorical equivalent of loss in PyTorch. We use Losses in different form in TextGrad but for now we will focus on a simple TextLoss. TextLoss is going to evaluate the loss wrt a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "252e0a0152b81f14",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:32.894722136Z",
          "start_time": "2024-06-11T15:44:32.890708561Z"
        },
        "id": "252e0a0152b81f14"
      },
      "outputs": [],
      "source": [
        "system_prompt = Variable(\"Evaluate the correctness of this sentence\", role_description=\"The system prompt\")\n",
        "loss = TextLoss(system_prompt, engine=engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff137c99e0659dcc",
      "metadata": {
        "collapsed": false,
        "id": "ff137c99e0659dcc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6f05ec2bf907b3ba",
      "metadata": {
        "collapsed": false,
        "id": "6f05ec2bf907b3ba"
      },
      "source": [
        "## Introduction: Optimizer\n",
        "\n",
        "Keeping on the analogy with PyTorch, the optimizer in TextGrad is the object that will update the parameters of the model. In this case, the parameters are the variables that have `requires_grad` set to `True`.\n",
        "\n",
        "**NOTE** This is a text optimizer! It will do all operations with text!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f93f80b9e3ad36",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:33.741130951Z",
          "start_time": "2024-06-11T15:44:33.734977769Z"
        },
        "id": "78f93f80b9e3ad36"
      },
      "outputs": [],
      "source": [
        "optimizer = TextualGradientDescent(parameters=[x], engine=engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26883eb74ce0d01",
      "metadata": {
        "collapsed": false,
        "id": "d26883eb74ce0d01"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "We can now put all the pieces together. We have a variable, an engine, a loss, and an optimizer. We can now run a single optimization step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9817e0ae0179376d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:41.730132530Z",
          "start_time": "2024-06-11T15:44:34.997777872Z"
        },
        "id": "9817e0ae0179376d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0c7126-5b84-4d23-bf4f-7157f75c93ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ],
      "source": [
        "l = loss(x)\n",
        "l.backward(engine)\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e3fab0efdd579e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:41.738985151Z",
          "start_time": "2024-06-11T15:44:41.731989729Z"
        },
        "id": "77e3fab0efdd579e",
        "outputId": "6abafe38-9030-4c5b-f12c-6f295826553c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A sentence with a typo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "x.value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution optimization\n",
        "\n",
        "From this [notebook](https://colab.research.google.com/github/zou-group/TextGrad/blob/main/examples/notebooks/Tutorial-Solution-Optimization.ipynb) the result from our model is not good compared gpt4o."
      ],
      "metadata": {
        "id": "2v3f5XKg-7p4"
      },
      "id": "2v3f5XKg-7p4"
    },
    {
      "cell_type": "code",
      "source": [
        "import textgrad as tg\n",
        "\n",
        "initial_solution = \"\"\"To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula:\n",
        "x = (-b ± √(b^2 - 4ac)) / 2a\n",
        "a = 3, b = -7, c = 2\n",
        "x = (7 ± √((-7)^2 + 4(3)(2))) / 6\n",
        "x = (7 ± √73) / 6\n",
        "The solutions are:\n",
        "x1 = (7 + √73)\n",
        "x2 = (7 - √73)\"\"\"\n",
        "\n",
        "solution = tg.Variable(initial_solution,\n",
        "                       requires_grad=True,\n",
        "                       role_description=\"solution to the math question\")\n",
        "\n",
        "loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math question.\n",
        "Do not attempt to solve it yourself, do not give a solution, only identify errors. Be super concise.\"\"\",\n",
        "                                 requires_grad=False,\n",
        "                                 role_description=\"system prompt\")\n",
        "\n",
        "loss_fn = tg.TextLoss(loss_system_prompt, engine=eval_engine)\n",
        "# We have optimizer here that adjust the `solution`\n",
        "optimizer = tg.TGD(parameters=[solution], engine=eval_engine)"
      ],
      "metadata": {
        "id": "vkmGWyGv_FWi"
      },
      "id": "vkmGWyGv_FWi",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We compute the loss or in this contet create feedback base on our loss_system_prompt\n",
        "loss = loss_fn(solution)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAXPaDlJ_gyh",
        "outputId": "d7f7a545-2212-43d7-c820-14b5b17a38b8"
      },
      "id": "VAXPaDlJ_gyh",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=The solutions are missing the denominator 6.\n",
              "\n",
              "Correct solutions:\n",
              "x1 = (7 + √73) / 6\n",
              "x2 = (7 - √73) / 6, role=response from the language model, grads=)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we will create compute the natural language gradients (feedback) and store it in solution Variable (variable will be the parameters)\n",
        "loss.backward(engine=eval_engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "074069iT_9H6",
        "outputId": "ab8f9013-9982-49b6-babd-e2c0c32180a0"
      },
      "id": "074069iT_9H6",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i8_UnBvcqb3",
        "outputId": "15359fd6-900b-42d4-8791-7b1d3268bc20"
      },
      "id": "7i8_UnBvcqb3",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The solutions are missing the denominator 6.\n",
            "\n",
            "Correct solutions:\n",
            "x1 = (7 + √73) / 6\n",
            "x2 = (7 - √73) / 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we base on natuarl laguage gradients we update the text/solution/prompt\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVxMtkgNcZTc",
        "outputId": "45cdb0b4-ec26-4542-ba8c-35b646de9292"
      },
      "id": "lVxMtkgNcZTc",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SwyMFaXccfO",
        "outputId": "e482a942-bfef-44cd-8e2f-f77cf9aaea7e"
      },
      "id": "9SwyMFaXccfO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula:\n",
            "x = (-b ± √(b^2 - 4ac)) / 2a\n",
            "a = 3, b = -7, c = 2\n",
            "x = (7 ± √((-7)^2 + 4(3)(2))) / 6\n",
            "x = (7 ± √73) / 6\n",
            "The solutions are:\n",
            "x1 = (7 + √73) / 6\n",
            "x2 = (7 - √73) / 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Another solution optimization example\n",
        "\n",
        "import textgrad as tg\n",
        "\n",
        "\n",
        "model = tg.BlackboxLLM(engine=engine)\n",
        "question_string = (\"If it takes 1 hour to dry 25 shirts under the sun, \"\n",
        "                   \"how long will it take to dry 30 shirts under the sun? \"\n",
        "                   \"Reason step by step\")\n",
        "\n",
        "question = tg.Variable(question_string,\n",
        "                       role_description=\"question to the LLM\",\n",
        "                       requires_grad=False)\n",
        "\n",
        "answer = model(question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gko3gsprwqqM",
        "outputId": "1eda9752-145a-4589-80d0-69a96585359c",
        "cellView": "form"
      },
      "id": "Gko3gsprwqqM",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=Sure, I'd be happy to help you reason through this problem!\n",
              "\n",
              "1. First, let's consider how long it takes to dry one shirt in the sun. From the information given, we know that 25 shirts take 1 hour to dry. So, if we divide the number of shirts by the time it takes to dry them, we can find out how long it takes to dry one shirt. That is, 25 shirts / 1 hour = 1 shirt per hour.\n",
              "\n",
              "2. Now, if we want to dry 30 shirts, and it takes 1 hour to dry 1 shirt, then to find out how long it will take to dry 30 shirts, we simply multiply the number of shirts by the time it takes to dry one shirt. That is, 30 shirts * 1 hour per shirt = 30 hours.\n",
              "\n",
              "However, this answer assumes that you can only dry one shirt at a time, which may not be the case. If you have space to lay out all 30 shirts at once, then the drying time would still be 1 hour, just like it was for 25 shirts. The drying time depends on the rate of drying (which seems to be about 25 shirts per hour in this case), not the total amount of time each shirt needs to dry.\n",
              "\n",
              "So, the answer could be either 30 hours (if you can only dry one shirt at a time) or 1 hour (if you can dry all 30 shirts at once). I would need more information about your drying setup to give a more precise answer., role=response from the language model, grads=)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer.set_role_description(\"concise and accurate answer to the question\")\n",
        "\n",
        "optimizer = tg.TGD(parameters=[answer], engine=eval_engine)\n",
        "evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
        "                           \"Evaluate any given answer to this question, \"\n",
        "                           \"be smart, logical, and very critical. \"\n",
        "                           \"Just provide concise feedback.\")\n",
        "\n",
        "\n",
        "# TextLoss is a natural-language specified loss function that describes\n",
        "# how we want to evaluate the reasoning.\n",
        "loss_fn = tg.TextLoss(evaluation_instruction, engine=eval_engine)"
      ],
      "metadata": {
        "id": "e3TE4ZH81q5o"
      },
      "id": "e3TE4ZH81q5o",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always clear previous gradient to not affect the subsequent gradient computation\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "wzBFevE210Zb"
      },
      "id": "wzBFevE210Zb",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(answer)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnUJF8uanYCj",
        "outputId": "4213e342-f269-4ec4-89ff-b2f5e27d1979"
      },
      "id": "nnUJF8uanYCj",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=The answer to this question depends on the assumption made about the drying process. If we assume that the drying process is parallelizable, meaning that all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. However, if we assume that the drying process is sequential, meaning that shirts can only be dried one at a time, then it would take 30 hours to dry 30 shirts.\n",
              "\n",
              "The given answer of 30 hours is based on the assumption that the drying process is sequential. However, this assumption may not be valid, as it is not explicitly stated in the question. Therefore, the answer of 30 hours should be evaluated with caution, and the assumption of a sequential drying process should be made explicit.\n",
              "\n",
              "In conclusion, the answer to this question depends on the assumption made about the drying process. If the drying process is parallelizable, then the answer is 1 hour. If the drying process is sequential, then the answer is 30 hours. The given answer of 30 hours is based on the assumption of a sequential drying process, and should be evaluated with caution., role=response from the language model, grads=)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward(engine=eval_engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMUPl89nnc0R",
        "outputId": "939079f2-2042-4f5d-f8dd-9a5e7c5979ad"
      },
      "id": "VMUPl89nnc0R",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0qxTtcUneSX",
        "outputId": "8f58b193-4108-4385-e406-9c1153951148"
      },
      "id": "c0qxTtcUneSX",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygnITwpunfRj",
        "outputId": "f2d04a94-c321-4512-a4ae-4168d6f91fa7"
      },
      "id": "ygnITwpunfRj",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=Assuming the drying process is sequential, meaning shirts can only be dried one at a time, it would take 30 hours to dry 30 shirts. However, if the drying process is parallelizable, meaning all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. The given answer of 30 hours is based on the assumption of a sequential drying process., role=concise and accurate answer to the question, grads=Here is a conversation:\n",
              "\n",
              "<CONVERSATION><LM_SYSTEM_PROMPT> Here's a question: If it takes 1 hour to dry 25 shirts under the sun, how long will it take to dry 30 shirts under the sun? Reason step by step. Evaluate any given answer to this question, be smart, logical, and very critical. Just provide concise feedback. </LM_SYSTEM_PROMPT>\n",
              "\n",
              "<LM_INPUT> Sure, I'd be happy to help you reason through this problem!\n",
              "\n",
              "1. First, let's consider how long it takes to dry one shirt in the sun. From the information given, we know that 25 shirts take 1 hour to dry. So, if we divide the number of shirts by the time it takes to dry them, we can find out how long it takes to dry one shirt. That is, 25 shirts / 1 hour = 1 shirt per hour.\n",
              "\n",
              "2. Now, if we want to dry 30 shirts, and it takes 1 hour to dry 1 shirt, then to find out how long it will take to dry 30 shirts, we simply multiply the number of shirts by the time it takes to dry one shirt. That is, 30 shirts * 1 hour per shirt = 30 hours.\n",
              "\n",
              "However, this answer assumes that you can only dry one shirt at a time, which may not be the case. If you have space to lay out all 30 shirts at once, then the drying time would still be 1 hour, just like it was for 25 shirts. The drying time depends on the rate of drying (which seems to be about 25 shirts per hour in this case), not the total amount of time each shirt needs to dry.\n",
              "\n",
              "So, the answer could be either 30 hours (if you can only dry one shirt at a time) or 1 hour (if you can dry all 30 shirts at once). I would need more information about your drying setup to give a more precise answer. </LM_INPUT>\n",
              "\n",
              "<LM_OUTPUT> The answer to this question depends on the assumption made about the drying process. If we assume that the drying process is parallelizable, meaning that all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. However, if we assume that the drying process is sequential, meaning that shirts can only be dried one at a time, then it would take 30 hours to dry 30 shirts.\n",
              "\n",
              "The given answer of 30 hours is based on the assumption that the drying process is sequential. However, this assumption may not be valid, as it is not explicitly stated in the question. Therefore, the answer of 30 hours should be evaluated with caution, and the assumption of a sequential drying process should be made explicit.\n",
              "\n",
              "In conclusion, the answer to this question depends on the assumption made about the drying process. If the drying process is parallelizable, then the answer is 1 hour. If the drying process is sequential, then the answer is 30 hours. The given answer of 30 hours is based on the assumption of a sequential drying process, and should be evaluated with caution. </LM_OUTPUT>\n",
              "\n",
              "</CONVERSATION>\n",
              "\n",
              "This conversation is potentially part of a larger system. The output is used as response from the language model\n",
              "\n",
              "Here is the feedback we got for concise and accurate answer to the question in the conversation:\n",
              "\n",
              "<FEEDBACK>The variable provides a detailed and well-reasoned response to the question, but it could be improved in a few ways to better meet the objective of being a concise and accurate answer.\n",
              "\n",
              "1. Simplify the explanation: The variable provides a detailed step-by-step explanation, which is helpful for understanding the reasoning process. However, it could be simplified to make it more concise. For example, the variable could state the assumption that the drying process is sequential and then directly provide the answer of 30 hours.\n",
              "2. Clarify the assumption: The variable mentions that the drying process could be parallelizable, but it does not explicitly state that the answer of 30 hours is based on the assumption of a sequential drying process. This could be made more explicit to avoid confusion.\n",
              "3. Provide a more precise answer: The variable ends by stating that more information is needed to provide a more precise answer. However, it could be improved by providing a more definitive answer based on the information given. For example, the variable could state that the answer is 30 hours, assuming a sequential drying process, and then provide an alternative answer if the drying process is parallelizable.\n",
              "\n",
              "Overall, the variable provides a good response to the question, but it could be improved by simplifying the explanation, clarifying the assumption, and providing a more precise answer.</FEEDBACK>\n",
              "\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How much is the loss\n",
        "print(f\"Loss: {loss.value}\")\n",
        "print('---' * 100)\n",
        "# Display the gradient that would improve the next iteration.\n",
        "print(f\"Gradient: {answer.gradients}\")\n",
        "print('---' * 100)\n",
        "# Display the actual answer\n",
        "print(f\"Answer: {answer}\")\n",
        "print('---' * 100)\n",
        "print(f\"Value: {answer.value}\")\n",
        "print('---' * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDnbca6Z6xGc",
        "outputId": "dae34fce-33c7-411d-958c-3ca59e6b1157"
      },
      "id": "XDnbca6Z6xGc",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: The answer to this question depends on the assumption made about the drying process. If we assume that the drying process is parallelizable, meaning that all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. However, if we assume that the drying process is sequential, meaning that shirts can only be dried one at a time, then it would take 30 hours to dry 30 shirts.\n",
            "\n",
            "The given answer of 30 hours is based on the assumption that the drying process is sequential. However, this assumption may not be valid, as it is not explicitly stated in the question. Therefore, the answer of 30 hours should be evaluated with caution, and the assumption of a sequential drying process should be made explicit.\n",
            "\n",
            "In conclusion, the answer to this question depends on the assumption made about the drying process. If the drying process is parallelizable, then the answer is 1 hour. If the drying process is sequential, then the answer is 30 hours. The given answer of 30 hours is based on the assumption of a sequential drying process, and should be evaluated with caution.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Gradient: {Variable(value=The variable provides a detailed and well-reasoned response to the question, but it could be improved in a few ways to better meet the objective of being a concise and accurate answer.\n",
            "\n",
            "1. Simplify the explanation: The variable provides a detailed step-by-step explanation, which is helpful for understanding the reasoning process. However, it could be simplified to make it more concise. For example, the variable could state the assumption that the drying process is sequential and then directly provide the answer of 30 hours.\n",
            "2. Clarify the assumption: The variable mentions that the drying process could be parallelizable, but it does not explicitly state that the answer of 30 hours is based on the assumption of a sequential drying process. This could be made more explicit to avoid confusion.\n",
            "3. Provide a more precise answer: The variable ends by stating that more information is needed to provide a more precise answer. However, it could be improved by providing a more definitive answer based on the information given. For example, the variable could state that the answer is 30 hours, assuming a sequential drying process, and then provide an alternative answer if the drying process is parallelizable.\n",
            "\n",
            "Overall, the variable provides a good response to the question, but it could be improved by simplifying the explanation, clarifying the assumption, and providing a more precise answer., role=feedback to concise and accurate answer to the question, grads=)}\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Answer: Assuming the drying process is sequential, meaning shirts can only be dried one at a time, it would take 30 hours to dry 30 shirts. However, if the drying process is parallelizable, meaning all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. The given answer of 30 hours is based on the assumption of a sequential drying process.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Value: Assuming the drying process is sequential, meaning shirts can only be dried one at a time, it would take 30 hours to dry 30 shirts. However, if the drying process is parallelizable, meaning all shirts can be dried at the same time, then it would still take 1 hour to dry 30 shirts, just like it took 1 hour to dry 25 shirts. The given answer of 30 hours is based on the assumption of a sequential drying process.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize our computation graph.\n",
        "loss.generate_graph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RIi3NdJL7RhG",
        "outputId": "5c803952-2dfd-47b5-b719-fc649f809120"
      },
      "id": "RIi3NdJL7RhG",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"380pt\" height=\"510pt\"\n viewBox=\"0.00 0.00 380.00 510.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 506)\">\n<title>%3</title>\n<polygon fill=\"lightgrey\" stroke=\"transparent\" points=\"-4,4 -4,-506 376,-506 376,4 -4,4\"/>\n<!-- 139214972985952 -->\n<g id=\"node1\" class=\"node\">\n<title>139214972985952</title>\n<polygon fill=\"lavender\" stroke=\"black\" points=\"279.5,-278 92.5,-278 92.5,0 279.5,0 279.5,-278\"/>\n<text text-anchor=\"start\" x=\"109\" y=\"-265.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Role: </text>\n<text text-anchor=\"start\" x=\"133\" y=\"-265.6\" font-family=\"Arial\" font-size=\"8.00\"> Response from the language model</text>\n<text text-anchor=\"start\" x=\"99.5\" y=\"-257.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Value: </text>\n<text text-anchor=\"start\" x=\"127.5\" y=\"-257.6\" font-family=\"Arial\" font-size=\"8.00\"> The answer to this question depends on</text>\n<text text-anchor=\"start\" x=\"117.5\" y=\"-249.6\" font-family=\"Arial\" font-size=\"8.00\">the assumption made about the drying</text>\n<text text-anchor=\"start\" x=\"119.5\" y=\"-241.6\" font-family=\"Arial\" font-size=\"8.00\">process. If we assume that the drying</text>\n<text text-anchor=\"start\" x=\"118.5\" y=\"-233.6\" font-family=\"Arial\" font-size=\"8.00\">process is parallelizable, meaning that</text>\n<text text-anchor=\"start\" x=\"126\" y=\"-225.6\" font-family=\"Arial\" font-size=\"8.00\">all shirts can be dried at the same</text>\n<text text-anchor=\"start\" x=\"121\" y=\"-217.6\" font-family=\"Arial\" font-size=\"8.00\">time, then it would still take 1 hour to</text>\n<text text-anchor=\"start\" x=\"124.5\" y=\"-209.6\" font-family=\"Arial\" font-size=\"8.00\">dry 30 shirts, just like it took 1 hour</text>\n<text text-anchor=\"start\" x=\"116\" y=\"-201.6\" font-family=\"Arial\" font-size=\"8.00\">to dry 25 shirts. However, if we assume</text>\n<text text-anchor=\"start\" x=\"121\" y=\"-193.6\" font-family=\"Arial\" font-size=\"8.00\">that the drying process is sequential,</text>\n<text text-anchor=\"start\" x=\"120.5\" y=\"-185.6\" font-family=\"Arial\" font-size=\"8.00\">meaning that shirts can only be dried</text>\n<text text-anchor=\"start\" x=\"123.5\" y=\"-177.6\" font-family=\"Arial\" font-size=\"8.00\">one at a time, then it would take 30</text>\n<text text-anchor=\"start\" x=\"114\" y=\"-169.6\" font-family=\"Arial\" font-size=\"8.00\">hours to dry 30 shirts. The given answer</text>\n<text text-anchor=\"start\" x=\"116\" y=\"-161.6\" font-family=\"Arial\" font-size=\"8.00\">of 30 hours is based on the assumption</text>\n<text text-anchor=\"start\" x=\"121\" y=\"-153.6\" font-family=\"Arial\" font-size=\"8.00\">that the drying process is sequential.</text>\n<text text-anchor=\"start\" x=\"119.5\" y=\"-145.6\" font-family=\"Arial\" font-size=\"8.00\">However, this assumption may not be</text>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-137.6\" font-family=\"Arial\" font-size=\"8.00\">valid, as it is not explicitly stated in</text>\n<text text-anchor=\"start\" x=\"117\" y=\"-129.6\" font-family=\"Arial\" font-size=\"8.00\">the question. Therefore, the answer of</text>\n<text text-anchor=\"start\" x=\"125\" y=\"-121.6\" font-family=\"Arial\" font-size=\"8.00\">30 hours should be evaluated with</text>\n<text text-anchor=\"start\" x=\"128\" y=\"-113.6\" font-family=\"Arial\" font-size=\"8.00\">caution, and the assumption of a</text>\n<text text-anchor=\"start\" x=\"111\" y=\"-105.6\" font-family=\"Arial\" font-size=\"8.00\">sequential drying process should be made</text>\n<text text-anchor=\"start\" x=\"122.5\" y=\"-97.6\" font-family=\"Arial\" font-size=\"8.00\">explicit. In conclusion, the answer to</text>\n<text text-anchor=\"start\" x=\"113.5\" y=\"-89.6\" font-family=\"Arial\" font-size=\"8.00\">this question depends on the assumption</text>\n<text text-anchor=\"start\" x=\"119.5\" y=\"-81.6\" font-family=\"Arial\" font-size=\"8.00\">made about the drying process. If the</text>\n<text text-anchor=\"start\" x=\"122\" y=\"-73.6\" font-family=\"Arial\" font-size=\"8.00\">drying process is parallelizable, then</text>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-65.6\" font-family=\"Arial\" font-size=\"8.00\">the answer is 1 hour. If the drying</text>\n<text text-anchor=\"start\" x=\"118\" y=\"-57.6\" font-family=\"Arial\" font-size=\"8.00\">process is sequential, then the answer</text>\n<text text-anchor=\"start\" x=\"122\" y=\"-49.6\" font-family=\"Arial\" font-size=\"8.00\">is 30 hours. The given answer of 30</text>\n<text text-anchor=\"start\" x=\"118\" y=\"-41.6\" font-family=\"Arial\" font-size=\"8.00\">hours is based on the assumption of a</text>\n<text text-anchor=\"start\" x=\"113.5\" y=\"-33.6\" font-family=\"Arial\" font-size=\"8.00\">sequential drying process, and should be</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-25.6\" font-family=\"Arial\" font-size=\"8.00\">evaluated with caution.</text>\n<text text-anchor=\"start\" x=\"166\" y=\"-17.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Grad Fn: </text>\n<text text-anchor=\"start\" x=\"203\" y=\"-17.6\" font-family=\"Arial\" font-size=\"8.00\"> </text>\n<text text-anchor=\"start\" x=\"105\" y=\"-9.6\" font-family=\"Arial\" font-size=\"8.00\">textgrad.autograd.llm_ops.LLMCall.backward</text>\n</g>\n<!-- 139214962247088 -->\n<g id=\"node2\" class=\"node\">\n<title>139214962247088</title>\n<polygon fill=\"lavender\" stroke=\"black\" points=\"178,-402 0,-402 0,-324 178,-324 178,-402\"/>\n<text text-anchor=\"start\" x=\"16.5\" y=\"-389.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Role: </text>\n<text text-anchor=\"start\" x=\"40.5\" y=\"-389.6\" font-family=\"Arial\" font-size=\"8.00\"> System prompt for the evaluation</text>\n<text text-anchor=\"start\" x=\"7\" y=\"-381.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Value: </text>\n<text text-anchor=\"start\" x=\"35\" y=\"-381.6\" font-family=\"Arial\" font-size=\"8.00\"> Here&#39;s a question: If it takes 1 hour to</text>\n<text text-anchor=\"start\" x=\"22.5\" y=\"-373.6\" font-family=\"Arial\" font-size=\"8.00\">dry 25 shirts under the sun, how long</text>\n<text text-anchor=\"start\" x=\"26\" y=\"-365.6\" font-family=\"Arial\" font-size=\"8.00\">will it take to dry 30 shirts under the</text>\n<text text-anchor=\"start\" x=\"17.5\" y=\"-357.6\" font-family=\"Arial\" font-size=\"8.00\">sun? Reason step by step. Evaluate any</text>\n<text text-anchor=\"start\" x=\"18\" y=\"-349.6\" font-family=\"Arial\" font-size=\"8.00\">given answer to this question, be smart,</text>\n<text text-anchor=\"start\" x=\"24.5\" y=\"-341.6\" font-family=\"Arial\" font-size=\"8.00\">logical, and very critical. Just provide</text>\n<text text-anchor=\"start\" x=\"57\" y=\"-333.6\" font-family=\"Arial\" font-size=\"8.00\">concise feedback.</text>\n</g>\n<!-- 139214962247088&#45;&gt;139214972985952 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139214962247088&#45;&gt;139214972985952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M105.67,-323.84C110.38,-313.06 115.81,-300.64 121.57,-287.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"124.82,-288.77 125.61,-278.21 118.4,-285.97 124.82,-288.77\"/>\n</g>\n<!-- 139214972988880 -->\n<g id=\"node3\" class=\"node\">\n<title>139214972988880</title>\n<polygon fill=\"lavender\" stroke=\"black\" points=\"372,-434 196,-434 196,-292 372,-292 372,-434\"/>\n<text text-anchor=\"start\" x=\"206.5\" y=\"-421.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Role: </text>\n<text text-anchor=\"start\" x=\"230.5\" y=\"-421.6\" font-family=\"Arial\" font-size=\"8.00\"> Concise and accurate answer to the</text>\n<text text-anchor=\"start\" x=\"268.5\" y=\"-413.6\" font-family=\"Arial\" font-size=\"8.00\">question</text>\n<text text-anchor=\"start\" x=\"214\" y=\"-405.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Value: </text>\n<text text-anchor=\"start\" x=\"242\" y=\"-405.6\" font-family=\"Arial\" font-size=\"8.00\"> Assuming the drying process is</text>\n<text text-anchor=\"start\" x=\"216\" y=\"-397.6\" font-family=\"Arial\" font-size=\"8.00\">sequential, meaning shirts can only be</text>\n<text text-anchor=\"start\" x=\"220.5\" y=\"-389.6\" font-family=\"Arial\" font-size=\"8.00\">dried one at a time, it would take 30</text>\n<text text-anchor=\"start\" x=\"217.5\" y=\"-381.6\" font-family=\"Arial\" font-size=\"8.00\">hours to dry 30 shirts. However, if the</text>\n<text text-anchor=\"start\" x=\"229\" y=\"-373.6\" font-family=\"Arial\" font-size=\"8.00\">drying process is parallelizable,</text>\n<text text-anchor=\"start\" x=\"218.5\" y=\"-365.6\" font-family=\"Arial\" font-size=\"8.00\">meaning all shirts can be dried at the</text>\n<text text-anchor=\"start\" x=\"222\" y=\"-357.6\" font-family=\"Arial\" font-size=\"8.00\">same time, then it would still take 1</text>\n<text text-anchor=\"start\" x=\"221.5\" y=\"-349.6\" font-family=\"Arial\" font-size=\"8.00\">hour to dry 30 shirts, just like it took</text>\n<text text-anchor=\"start\" x=\"225\" y=\"-341.6\" font-family=\"Arial\" font-size=\"8.00\">1 hour to dry 25 shirts. The given</text>\n<text text-anchor=\"start\" x=\"221\" y=\"-333.6\" font-family=\"Arial\" font-size=\"8.00\">answer of 30 hours is based on the</text>\n<text text-anchor=\"start\" x=\"224.5\" y=\"-325.6\" font-family=\"Arial\" font-size=\"8.00\">assumption of a sequential drying</text>\n<text text-anchor=\"start\" x=\"268.5\" y=\"-317.6\" font-family=\"Arial\" font-size=\"8.00\">process.</text>\n<text text-anchor=\"start\" x=\"264\" y=\"-309.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Grad Fn: </text>\n<text text-anchor=\"start\" x=\"301\" y=\"-309.6\" font-family=\"Arial\" font-size=\"8.00\"> </text>\n<text text-anchor=\"start\" x=\"203\" y=\"-301.6\" font-family=\"Arial\" font-size=\"8.00\">textgrad.autograd.llm_ops.LLMCall.backward</text>\n</g>\n<!-- 139214972988880&#45;&gt;139214972985952 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139214972988880&#45;&gt;139214972985952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M252.99,-291.76C252.38,-290.37 251.76,-288.97 251.14,-287.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"254.23,-285.89 246.99,-278.15 247.82,-288.71 254.23,-285.89\"/>\n</g>\n<!-- 139214972977312 -->\n<g id=\"node4\" class=\"node\">\n<title>139214972977312</title>\n<polygon fill=\"lavender\" stroke=\"black\" points=\"365,-502 203,-502 203,-448 365,-448 365,-502\"/>\n<text text-anchor=\"start\" x=\"237.5\" y=\"-489.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Role: </text>\n<text text-anchor=\"start\" x=\"261.5\" y=\"-489.6\" font-family=\"Arial\" font-size=\"8.00\"> Question to the llm</text>\n<text text-anchor=\"start\" x=\"212.5\" y=\"-481.6\" font-family=\"Arial\" font-weight=\"bold\" font-size=\"8.00\" fill=\"darkblue\">Value: </text>\n<text text-anchor=\"start\" x=\"240.5\" y=\"-481.6\" font-family=\"Arial\" font-size=\"8.00\"> If it takes 1 hour to dry 25 shirts</text>\n<text text-anchor=\"start\" x=\"218\" y=\"-473.6\" font-family=\"Arial\" font-size=\"8.00\">under the sun, how long will it take to</text>\n<text text-anchor=\"start\" x=\"210\" y=\"-465.6\" font-family=\"Arial\" font-size=\"8.00\">dry 30 shirts under the sun? Reason step</text>\n<text text-anchor=\"start\" x=\"271\" y=\"-457.6\" font-family=\"Arial\" font-size=\"8.00\">by step</text>\n</g>\n<!-- 139214972977312&#45;&gt;139214972988880 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139214972977312&#45;&gt;139214972988880</title>\n<path fill=\"none\" stroke=\"black\" d=\"M284,-447.93C284,-446.72 284,-445.48 284,-444.21\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.5,-444.14 284,-434.14 280.5,-444.14 287.5,-444.14\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7e9d830332e0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Promp optimization\n",
        "\n",
        "- We will use `MultiFieldTokenParsedEvaluation` to compute the loss\n",
        "     - This is"
      ],
      "metadata": {
        "id": "t-_rRDJhosYH"
      },
      "id": "t-_rRDJhosYH"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "XU8EHKzp-b9n"
      },
      "id": "XU8EHKzp-b9n",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initial Prompt\n",
        "from textwrap import dedent\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT=dedent(\n",
        "    f\"\"\"Your goal is to analyze the user message and generate sub-task based on available tools.\n",
        "\n",
        "    ## Available Tools:\n",
        "    Identify missing data in the platform - This tool is only to get user personal details related to their immigration. You should ONLY use this tool if user wants to know any missing or required details on the platform.\n",
        "    Ask immigration wikipedia - Use this tool to get internal immigration wiki that helps you find about policies, immigration law, how-to and frequently asked questions. Remember to pass question as argument.\n",
        "    Relocation services provided to user - Use this tool to provide what services we offer to the user. This tool also provide users next step or task for their relocation journey\n",
        "\n",
        "    ## Output:\n",
        "    1. Using the message provided and the list of available tools, decompose the task into smaller, manageable sub-tasks. For each sub-task, specify which tool can be used to complete it. Ensure the sub-tasks cover all aspects of the original message to provide a comprehensive response.\n",
        "    2. Your final output should be JSON format.\n",
        "\n",
        "\n",
        "    ## Examples:\n",
        "\n",
        "    Example 1:\n",
        "    User Message:\n",
        "    I recently moved to Germany with my family in April 2022. We need to register for health insurance and enroll our children in school. Could you assist us with these processes? Additionally, we haven't received our residency permits yet. Can you help with that? Best regards, Alex Johnson\n",
        "\n",
        "    Final output:\n",
        "    ```json\n",
        "      [\n",
        "        {{\n",
        "          \"task\": \"Search for list of health insurance providers in Germany.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Search what is the process for health insurance registration.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Search the documents needed for health insurance registration.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Search information on local schools and enrollment procedures.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Search how long the the residency permit process.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "         }}\n",
        "      ]\n",
        "    ```\n",
        "    Example 2:\n",
        "    User Message:\n",
        "    I have added all the info and uploaded the documents. Please let me know if everything is corrected and if something is missing also what would be our next step?\n",
        "\n",
        "    Final output:\n",
        "    ```json\n",
        "      [\n",
        "        {{\n",
        "          \"task\": \"Check if there is still missing data in the platform.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }},\n",
        "        {{\n",
        "          \"task\": \"Check what is the next task assigned to user for their relocation.\",\n",
        "          \"tool\": <Tool that can accomplish the task>,\n",
        "          \"explanation\": <Your detailed explanation why you choose the tool for this task>\n",
        "        }}\n",
        "      ]\n",
        "    ```\n",
        "\n",
        "    Begin!\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "9Zs-k0HI8oKn"
      },
      "id": "9Zs-k0HI8oKn",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textgrad as tg\n",
        "\n",
        "# Testing the 0-shot performance of the evaluation engine\n",
        "system_prompt = tg.Variable(SYSTEM_PROMPT,\n",
        "                            requires_grad=True,\n",
        "                            role_description=\"system prompt to the language model\")\n",
        "\n",
        "user_message = tg.Variable(\"\"\"Should I print the signed documents and bring them to the immigration office, or will you update them electronically? Alternatively, can a local HR fill out and sign the EzB form, and if so, what should I do with the correct versions of the documents?\"\"\", requires_grad=False, role_description=\"query to the language model\")\n",
        "model = tg.BlackboxLLM(engine=engine, system_prompt=system_prompt)\n",
        "answer = model(user_message)\n",
        "# answer\n",
        "# loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math question.\n",
        "# Do not attempt to solve it yourself, do not give a solution, only identify errors. Be super concise.\"\"\",\n",
        "#                                  requires_grad=False,\n",
        "#                                  role_description=\"system prompt\")\n",
        "\n",
        "# loss_fn = tg.TextLoss(loss_system_prompt, engine=eval_engine)\n",
        "# # We have optimizer here that adjust the `solution`\n",
        "# optimizer = tg.TGD(parameters=[system_prompt], engine=eval_engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i00AS7_ovF_",
        "outputId": "76fe728a-752b-48db-f18a-13f07d7b198b"
      },
      "id": "4i00AS7_ovF_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "# answer.set_role_description(\"Each task is clear and has all necessary information base on original task without any ambuiguity.\")\n",
        "optimizer = tg.TGD(parameters=[system_prompt], engine=eval_engine)\n",
        "evaluation_instruction = dedent(f\"\"\"\n",
        "        Evaluate each subtask to answer user question. Each subtask is an independent smaller task to accomplish one part of the question.\n",
        "\n",
        "\n",
        "        Subtask Evaluation Process:\n",
        "        - Subtask has all necessary details from the main question to avoid assumptions.\n",
        "        - Subtask has no vague terms that could lead to different interpretations.\n",
        "        - Check if the subtask includes all relevant details mentioned in the main question (e.g., specific forms, documents, country)\n",
        "\n",
        "        Output:\n",
        "        Provide feedback that must apply on the system_prompt to improve the answer.\n",
        "\n",
        "\n",
        "        <user_message>\n",
        "        {user_message.value}\n",
        "        </user_message>\n",
        "\n",
        "        <system_prompt>\n",
        "        {system_prompt.value}\n",
        "        </system_prompt>\n",
        "      \"\"\")\n",
        "\n",
        "\n",
        "# TextLoss is a natural-language specified loss function that describes\n",
        "# how we want to evaluate the reasoning.\n",
        "loss_fn = tg.TextLoss(evaluation_instruction, engine=eval_engine)"
      ],
      "metadata": {
        "id": "LmpyMAmhAiJ3"
      },
      "id": "LmpyMAmhAiJ3",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(12)"
      ],
      "metadata": {
        "id": "TieHr6fy6bp6"
      },
      "id": "TieHr6fy6bp6",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always clear previous gradient to not affect the subsequent gradient computation\n",
        "optimizer.zero_grad()\n",
        "loss = loss_fn(answer)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6Bxa_Wn6tM6",
        "outputId": "3053aa7d-b357-49e5-eb32-de3e17d2772a"
      },
      "id": "v6Bxa_Wn6tM6",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable(value=```json\n",
              "[\n",
              "  {\n",
              "    \"task\": \"Confirm if signed documents need to be printed and brought to the immigration office or if they will be updated electronically\",\n",
              "    \"tool\": \"Contact the immigration office directly via phone or email\",\n",
              "    \"explanation\": \"The most direct and reliable way to confirm the requirements for signed documents is to contact the immigration office directly. They provide the most accurate and current information. If no response is received, follow up with a visit to the office.\"\n",
              "  },\n",
              "  {\n",
              "    \"task\": \"Inquire about the possibility of a local HR filling out and signing the EzB form\",\n",
              "    \"tool\": \"Consult with the HR department\",\n",
              "    \"explanation\": \"The HR department will have the necessary information and authority to determine if they can fill out and sign the EzB form. They can also provide guidance on any company policies or legal requirements that need to be followed. If further clarification is needed, consult with a legal advisor.\"\n",
              "  },\n",
              "  {\n",
              "    \"task\": \"Identify the correct versions of the documents and determine the next steps\",\n",
              "    \"tool\": \"Review the guidelines provided by the immigration office or relocation service\",\n",
              "    \"explanation\": \"The guidelines provided by the immigration office or the relocation service will outline the necessary steps and the correct versions of the documents required. This ensures compliance with official procedures and avoids any potential issues. If the guidelines are unclear, contact the respective office for further clarification.\"\n",
              "  }\n",
              "]\n",
              "```, role=response from the language model, grads=)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward(engine=eval_engine)\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibhZ4QUY74k4",
        "outputId": "4d538e8f-4c68-4dd7-c094-3e456172ebc9"
      },
      "id": "ibhZ4QUY74k4",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How much is the loss\n",
        "print(f\"Loss: {loss.value}\")\n",
        "print('---' * 100)\n",
        "# Display the gradient that would improve the next iteration.\n",
        "print(f\"Gradient: {answer.gradients}\")\n",
        "print('---' * 100)\n",
        "# Display the actual answer\n",
        "print(f\"Answer: {answer}\")\n",
        "print('---' * 100)\n",
        "print(f\"Value: {answer.value}\")\n",
        "print('---' * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZLMbpgq8DVL",
        "outputId": "187efd8b-bce6-43c7-d3c5-e714ff710aca"
      },
      "id": "eZLMbpgq8DVL",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: ```json\n",
            "[\n",
            "  {\n",
            "    \"task\": \"Confirm if signed documents need to be printed and brought to the immigration office or if they will be updated electronically\",\n",
            "    \"tool\": \"Contact the immigration office directly via phone or email\",\n",
            "    \"explanation\": \"The most direct and reliable way to confirm the requirements for signed documents is to contact the immigration office directly. They provide the most accurate and current information. If no response is received, follow up with a visit to the office.\"\n",
            "  },\n",
            "  {\n",
            "    \"task\": \"Inquire about the possibility of a local HR filling out and signing the EzB form\",\n",
            "    \"tool\": \"Consult with the HR department\",\n",
            "    \"explanation\": \"The HR department will have the necessary information and authority to determine if they can fill out and sign the EzB form. They can also provide guidance on any company policies or legal requirements that need to be followed. If further clarification is needed, consult with a legal advisor.\"\n",
            "  },\n",
            "  {\n",
            "    \"task\": \"Identify the correct versions of the documents and determine the next steps\",\n",
            "    \"tool\": \"Review the guidelines provided by the immigration office or relocation service\",\n",
            "    \"explanation\": \"The guidelines provided by the immigration office or the relocation service will outline the necessary steps and the correct versions of the documents required. This ensures compliance with official procedures and avoids any potential issues. If the guidelines are unclear, contact the respective office for further clarification.\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Gradient: {Variable(value=1. **Consistency in Formatting**: Ensure that the JSON format is consistent throughout. The input and output JSON structures should match exactly, including the use of quotation marks and spacing. This helps in maintaining clarity and avoiding parsing errors.\n",
            "\n",
            "2. **Specificity in Tools**: The tools suggested are somewhat generic. For instance, \"Contact the immigration office directly via phone or email\" could be more specific by providing the contact details or suggesting the best time to call. Similarly, \"Consult with the HR department\" could specify the role or title of the person to contact within the HR department.\n",
            "\n",
            "3. **Detailed Explanations**: While the explanations are generally good, they could be more detailed. For example, the explanation for contacting the immigration office could include potential questions to ask or documents to have on hand during the call. This would provide more actionable advice.\n",
            "\n",
            "4. **Alternative Tools**: Consider suggesting alternative tools or methods. For example, for confirming document requirements, an online FAQ or a chatbot on the immigration office's website might also be useful. This provides multiple avenues for the user to get the information they need.\n",
            "\n",
            "5. **Clarification on Follow-ups**: The suggestion to follow up with a visit to the office if no response is received could be expanded. Specify a reasonable timeframe for waiting for a response before deciding to visit the office. This helps in setting clear expectations.\n",
            "\n",
            "6. **Legal and Compliance Considerations**: For tasks involving legal documents, it might be beneficial to mention consulting with a legal advisor or compliance officer to ensure all actions are in line with legal requirements. This adds an extra layer of diligence.\n",
            "\n",
            "7. **User Guidance**: Provide guidance on what to do if the initial tool does not yield results. For example, if the HR department is unable to help with the EzB form, suggest the next steps, such as contacting a specific external agency or using an online resource.\n",
            "\n",
            "8. **Error Handling**: Include advice on what to do if there are discrepancies or issues with the information received. For instance, if the guidelines from the immigration office are unclear or contradictory, suggest a method for escalating the issue or seeking further clarification.\n",
            "\n",
            "By addressing these points, the response from the language model can be made more robust, actionable, and user-friendly, thereby improving the overall objective function., role=feedback to response from the language model, grads=)}\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Answer: [\n",
            "  {\n",
            "    \"task\": \"Confirm if signed documents need to be printed and brought to the immigration office or if they will be updated electronically\",\n",
            "    \"tool\": \"Contact the immigration office directly via phone or email\",\n",
            "    \"explanation\": \"The most direct and reliable way to confirm the requirements for signed documents is to contact the immigration office directly. They provide the most accurate and current information. Have your identification and any relevant document references on hand during the call. If no response is received within 48 hours, follow up with a visit to the office. Alternatively, check the immigration office's website for an FAQ section or a chatbot that might provide the needed information.\"\n",
            "  },\n",
            "  {\n",
            "    \"task\": \"Inquire about the possibility of a local HR filling out and signing the EzB form\",\n",
            "    \"tool\": \"Consult with the HR department, specifically the HR manager or a senior HR representative\",\n",
            "    \"explanation\": \"The HR department, particularly the HR manager or a senior HR representative, will have the necessary information and authority to determine if they can fill out and sign the EzB form. They can also provide guidance on any company policies or legal requirements that need to be followed. If further clarification is needed, consult with a legal advisor to ensure compliance with all legal requirements. If the HR department is unable to assist, consider contacting an external agency that specializes in such forms.\"\n",
            "  },\n",
            "  {\n",
            "    \"task\": \"Identify the correct versions of the documents and determine the next steps\",\n",
            "    \"tool\": \"Review the guidelines provided by the immigration office or relocation service\",\n",
            "    \"explanation\": \"The guidelines provided by the immigration office or the relocation service will outline the necessary steps and the correct versions of the documents required. This ensures compliance with official procedures and avoids any potential issues. If the guidelines are unclear or contradictory, contact the respective office for further clarification. If discrepancies persist, escalate the issue by requesting to speak with a supervisor or a legal advisor.\"\n",
            "  }\n",
            "]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Value: [\n",
            "  {\n",
            "    \"task\": \"Confirm if signed documents need to be printed and brought to the immigration office or if they will be updated electronically\",\n",
            "    \"tool\": \"Contact the immigration office directly via phone or email\",\n",
            "    \"explanation\": \"The most direct and reliable way to confirm the requirements for signed documents is to contact the immigration office directly. They provide the most accurate and current information. Have your identification and any relevant document references on hand during the call. If no response is received within 48 hours, follow up with a visit to the office. Alternatively, check the immigration office's website for an FAQ section or a chatbot that might provide the needed information.\"\n",
            "  },\n",
            "  {\n",
            "    \"task\": \"Inquire about the possibility of a local HR filling out and signing the EzB form\",\n",
            "    \"tool\": \"Consult with the HR department, specifically the HR manager or a senior HR representative\",\n",
            "    \"explanation\": \"The HR department, particularly the HR manager or a senior HR representative, will have the necessary information and authority to determine if they can fill out and sign the EzB form. They can also provide guidance on any company policies or legal requirements that need to be followed. If further clarification is needed, consult with a legal advisor to ensure compliance with all legal requirements. If the HR department is unable to assist, consider contacting an external agency that specializes in such forms.\"\n",
            "  },\n",
            "  {\n",
            "    \"task\": \"Identify the correct versions of the documents and determine the next steps\",\n",
            "    \"tool\": \"Review the guidelines provided by the immigration office or relocation service\",\n",
            "    \"explanation\": \"The guidelines provided by the immigration office or the relocation service will outline the necessary steps and the correct versions of the documents required. This ensures compliance with official procedures and avoids any potential issues. If the guidelines are unclear or contradictory, contact the respective office for further clarification. If discrepancies persist, escalate the issue by requesting to speak with a supervisor or a legal advisor.\"\n",
            "  }\n",
            "]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"system_prompt gradient: {system_prompt.gradients}\")\n",
        "print('---' * 100)\n",
        "print(f\"system_prompt: {system_prompt}\")\n",
        "print('---' * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Mg65Bt8jjl",
        "outputId": "c08c0143-f08d-4f0d-e5d4-219036f26e09"
      },
      "id": "F8Mg65Bt8jjl",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system_prompt gradient: {Variable(value=1. **Consistency in Formatting**: \n",
            "   - **Clarify JSON Structure**: Explicitly define the expected JSON structure in the system prompt. This includes specifying the exact keys, their data types, and any nested structures. For example, provide a sample JSON output to illustrate the required format.\n",
            "   - **Enforce Quotation Marks and Spacing**: Emphasize the importance of using consistent quotation marks (e.g., double quotes) and spacing in the JSON output. This can be achieved by including a note in the prompt about adhering to JSON formatting standards.\n",
            "\n",
            "2. **Specificity in Tools**:\n",
            "   - **Detail Tool Usage**: In the system prompt, provide more detailed instructions on how to use each tool. For example, specify that the language model should include contact details or suggest the best time to call when recommending contacting the immigration office.\n",
            "   - **Role-Specific Guidance**: Encourage the language model to specify roles or titles when suggesting consulting with departments, such as \"Consult with the HR manager\" instead of just \"Consult with the HR department.\"\n",
            "\n",
            "3. **Detailed Explanations**:\n",
            "   - **Expand on Explanations**: Instruct the language model to provide more detailed explanations for each sub-task. This includes potential questions to ask, documents to have on hand, and any preparatory steps the user should take.\n",
            "   - **Actionable Advice**: Emphasize the need for actionable advice in the explanations, ensuring that users have clear and practical steps to follow.\n",
            "\n",
            "4. **Alternative Tools**:\n",
            "   - **Suggest Multiple Avenues**: Encourage the language model to suggest alternative tools or methods for each sub-task. For example, mention online FAQs, chatbots, or other resources that might be useful in addition to the primary tool.\n",
            "   - **Provide Examples**: Include examples of alternative tools in the system prompt to guide the language model in generating comprehensive responses.\n",
            "\n",
            "5. **Clarification on Follow-ups**:\n",
            "   - **Specify Timeframes**: Instruct the language model to specify reasonable timeframes for follow-ups. For example, mention how long to wait for a response before deciding to visit the office.\n",
            "   - **Set Clear Expectations**: Emphasize the importance of setting clear expectations for follow-up actions, ensuring users know what to do if they don't receive a timely response.\n",
            "\n",
            "6. **Legal and Compliance Considerations**:\n",
            "   - **Highlight Legal Advice**: Include a note in the system prompt to mention consulting with legal advisors or compliance officers for tasks involving legal documents. This adds an extra layer of diligence and ensures users are aware of legal considerations.\n",
            "   - **Provide Legal Context**: Encourage the language model to provide context on why legal advice might be necessary, helping users understand the importance of compliance.\n",
            "\n",
            "7. **User Guidance**:\n",
            "   - **Next Steps if Initial Tool Fails**: Instruct the language model to provide guidance on what to do if the initial tool does not yield results. For example, suggest contacting a specific external agency or using an online resource if the HR department is unable to help.\n",
            "   - **Escalation Paths**: Include instructions for the language model to suggest escalation paths if users encounter issues or discrepancies with the information received.\n",
            "\n",
            "8. **Error Handling**:\n",
            "   - **Advice on Discrepancies**: Instruct the language model to include advice on what to do if there are discrepancies or issues with the information received. For example, suggest methods for escalating the issue or seeking further clarification.\n",
            "   - **Provide Examples**: Include examples of common issues and how to handle them, guiding the language model in generating robust and user-friendly responses.\n",
            "\n",
            "By incorporating these improvements into the system prompt, the language model will be better equipped to generate responses that are consistent, specific, detailed, and user-friendly, thereby enhancing the overall objective function., role=feedback to system prompt to the language model, grads=)}\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "system_prompt: Your goal is to analyze the user message and generate sub-tasks based on available tools.\n",
            "\n",
            "## Available Tools:\n",
            "1. **Identify missing data in the platform**: This tool is only to get user personal details related to their immigration. You should ONLY use this tool if the user wants to know any missing or required details on the platform.\n",
            "2. **Ask immigration wikipedia**: Use this tool to get internal immigration wiki that helps you find information about policies, immigration law, how-to guides, and frequently asked questions. Remember to pass the question as an argument.\n",
            "3. **Relocation services provided to user**: Use this tool to provide information on the services we offer to the user. This tool also provides users with the next steps or tasks for their relocation journey.\n",
            "\n",
            "## Output:\n",
            "1. Using the message provided and the list of available tools, decompose the task into smaller, manageable sub-tasks. For each sub-task, specify which tool can be used to complete it. Ensure the sub-tasks cover all aspects of the original message to provide a comprehensive response.\n",
            "2. Your final output should be in JSON format. Ensure consistent use of double quotes and proper spacing. The JSON structure should include the following keys:\n",
            "   - \"task\": A description of the sub-task.\n",
            "   - \"tool\": The tool to be used for the sub-task.\n",
            "   - \"explanation\": A detailed explanation of the sub-task, including potential questions to ask, documents to have on hand, and any preparatory steps the user should take.\n",
            "\n",
            "## Example JSON Output:\n",
            "```json\n",
            "[\n",
            "  {\n",
            "    \"task\": \"Confirm if signed documents need to be printed and brought to the immigration office or if they will be updated electronically\",\n",
            "    \"tool\": \"Contact the immigration office directly via phone or email\",\n",
            "    \"explanation\": \"The most direct and reliable way to confirm the requirements for signed documents is to contact the immigration office directly. They provide the most accurate and current information. If no response is received, follow up with a visit to the office.\"\n",
            "  },\n",
            "  {\n",
            "    \"task\": \"Inquire about the possibility of a local HR filling out and signing the EzB form\",\n",
            "    \"tool\": \"Consult with the HR manager\",\n",
            "    \"explanation\": \"The HR manager will have the necessary information and authority to determine if they can fill out and sign the EzB form. They can also provide guidance on any company policies or legal requirements that need to be followed. If further clarification is needed, consult with a legal advisor.\"\n",
            "  },\n",
            "  {\n",
            "    \"task\": \"Identify the correct versions of the documents and determine the next steps\",\n",
            "    \"tool\": \"Review the guidelines provided by the immigration office or relocation service\",\n",
            "    \"explanation\": \"The guidelines provided by the immigration office or the relocation service will outline the necessary steps and the correct versions of the documents required. This ensures compliance with official procedures and avoids any potential issues. If the guidelines are unclear, contact the respective office for further clarification.\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "\n",
            "## Additional Instructions:\n",
            "- **Detail Tool Usage**: When recommending contacting an office or department, include contact details or suggest the best time to call.\n",
            "- **Role-Specific Guidance**: Specify roles or titles when suggesting consulting with departments, such as \"Consult with the HR manager.\"\n",
            "- **Alternative Tools**: Suggest alternative tools or methods for each sub-task, such as online FAQs, chatbots, or other resources.\n",
            "- **Follow-ups**: Specify reasonable timeframes for follow-ups and set clear expectations for follow-up actions.\n",
            "- **Legal and Compliance Considerations**: Mention consulting with legal advisors or compliance officers for tasks involving legal documents and provide context on why legal advice might be necessary.\n",
            "- **Error Handling**: Include advice on what to do if there are discrepancies or issues with the information received, and provide examples of common issues and how to handle them.\n",
            "\n",
            "By following these guidelines, you will generate responses that are consistent, specific, detailed, and user-friendly.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt.get_value()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "NgiWbgRCK786",
        "outputId": "9ee55f5e-6dd2-4f58-86c1-552cb01d861c"
      },
      "id": "NgiWbgRCK786",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your goal is to analyze the user message and generate sub-task based on available tools.\\n\\n    ## Available Tools:\\n    Identify missing data in the platform - This tool is only to get user personal details related to their immigration. You should ONLY use this tool if user wants to know any missing or required details on the platform.\\n    Ask immigration wikipedia - Use this tool to get internal immigration wiki that helps you find about policies, immigration law, how-to and frequently asked questions. Remember to pass question as argument.\\n    Relocation services provided to user - Use this tool to provide what services we offer to the user. This tool also provide users next step or task for their relocation journey\\n\\n    ## Output:\\n    1. Using the message provided and the list of available tools, decompose the task into smaller, manageable sub-tasks. For each sub-task, specify which tool can be used to complete it. Ensure the sub-tasks cover all aspects of the original message to provide a comprehensive response.\\n    2. Your final output should be JSON format.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textgrad.loss import MultiFieldTokenParsedEvaluation\n",
        "\n",
        "role_descriptions = [\n",
        "    \"Question for the task\",\n",
        "    \"Ground truth answer\",\n",
        "    \"Reasoning and prediction from the language model\"\n",
        "]\n",
        "\n",
        "evaluation_instruction = \"Below is a question from a question-answering task, the ground truth answer, and reasoning with the final prediction. Is the final prediction correct, i.e. the same as the ground truth answer? Say only 1 (yes) or 0 (no). Return your response within <ACCURACY> </ACCURACY> tags. e.g.<ACCURACY> 0 </ACCURACY> or <ACCURACY> 1 </ACCURACY>\"\n",
        "eval_instruction = Variable(evaluation_instruction, requires_grad=False, role_description=\"evaluation instruction for the task\")\n",
        "eval_fn = MultiFieldTokenParsedEvaluation(\n",
        "    eval_instruction,\n",
        "    engine=eval_engine,\n",
        "    role_descriptions=role_descriptions,\n",
        "    parse_tags=[\"<ACCURACY>\", \"</ACCURACY>\"]\n",
        ")"
      ],
      "metadata": {
        "id": "3RfsvOEbq7xT"
      },
      "id": "3RfsvOEbq7xT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}