{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuaalpuerto/ML-guide/blob/main/Learning_TextGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36a9c615-17a0-455c-8f9c-f0d25fb8824b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:43:10.594204491Z",
          "start_time": "2024-06-11T15:43:10.589328053Z"
        },
        "id": "36a9c615-17a0-455c-8f9c-f0d25fb8824b",
        "outputId": "8adc7fa0-9b96-4932-de77-d331a3b6db05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for textgrad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU textgrad --progress-bar off\n",
        "!pip install -qU openai --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load fireworks API key\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/MyDrive/env/env.json') as jsonfile:\n",
        "    env = json.load(jsonfile)\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = env['fireworks.ai']['apiKey']\n",
        "os.environ['OPENAI_BASE_URL'] = \"https://api.fireworks.ai/inference/v1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_loXjG44xfJ",
        "outputId": "ec1ef734-80a6-4387-8fa5-262e0bce152c"
      },
      "id": "k_loXjG44xfJ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "from textgrad.engine import get_engine\n",
        "from textgrad import Variable\n",
        "from textgrad.optimizer import TextualGradientDescent\n",
        "from textgrad.loss import TextLoss\n",
        "from textgrad.engine.openai import ChatOpenAI\n",
        "\n",
        "engine = ChatOpenAI(model_string='accounts/fireworks/models/mixtral-8x7b-instruct')"
      ],
      "metadata": {
        "id": "imcaXNen5ats"
      },
      "id": "imcaXNen5ats",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c65fb4456d84c8fc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:43:17.669096228Z",
          "start_time": "2024-06-11T15:43:17.665325560Z"
        },
        "id": "c65fb4456d84c8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "084f2b50-2439-4b8a-f2f3-8fef50e31668"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! I'm an artificial intelligence, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "x = Variable(\"A sntence with a typo\", role_description=\"The input sentence\", requires_grad=True)\n",
        "engine.generate(\"Hello how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b627edc07c0d3737",
      "metadata": {
        "collapsed": false,
        "id": "b627edc07c0d3737"
      },
      "source": [
        "## Introduction: Loss\n",
        "\n",
        "Again, Loss in TextGrad is the metaphorical equivalent of loss in PyTorch. We use Losses in different form in TextGrad but for now we will focus on a simple TextLoss. TextLoss is going to evaluate the loss wrt a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "252e0a0152b81f14",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:32.894722136Z",
          "start_time": "2024-06-11T15:44:32.890708561Z"
        },
        "id": "252e0a0152b81f14"
      },
      "outputs": [],
      "source": [
        "system_prompt = Variable(\"Evaluate the correctness of this sentence\", role_description=\"The system prompt\")\n",
        "loss = TextLoss(system_prompt, engine=engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff137c99e0659dcc",
      "metadata": {
        "collapsed": false,
        "id": "ff137c99e0659dcc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6f05ec2bf907b3ba",
      "metadata": {
        "collapsed": false,
        "id": "6f05ec2bf907b3ba"
      },
      "source": [
        "## Introduction: Optimizer\n",
        "\n",
        "Keeping on the analogy with PyTorch, the optimizer in TextGrad is the object that will update the parameters of the model. In this case, the parameters are the variables that have `requires_grad` set to `True`.\n",
        "\n",
        "**NOTE** This is a text optimizer! It will do all operations with text!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "78f93f80b9e3ad36",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:33.741130951Z",
          "start_time": "2024-06-11T15:44:33.734977769Z"
        },
        "id": "78f93f80b9e3ad36"
      },
      "outputs": [],
      "source": [
        "optimizer = TextualGradientDescent(parameters=[x], engine=engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26883eb74ce0d01",
      "metadata": {
        "collapsed": false,
        "id": "d26883eb74ce0d01"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "We can now put all the pieces together. We have a variable, an engine, a loss, and an optimizer. We can now run a single optimization step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9817e0ae0179376d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:41.730132530Z",
          "start_time": "2024-06-11T15:44:34.997777872Z"
        },
        "id": "9817e0ae0179376d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0c7126-5b84-4d23-bf4f-7157f75c93ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:_backward_through_llm prompt\n",
            "INFO:textgrad:_backward_through_llm gradient\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n"
          ]
        }
      ],
      "source": [
        "l = loss(x)\n",
        "l.backward(engine)\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "77e3fab0efdd579e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T15:44:41.738985151Z",
          "start_time": "2024-06-11T15:44:41.731989729Z"
        },
        "id": "77e3fab0efdd579e",
        "outputId": "6abafe38-9030-4c5b-f12c-6f295826553c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A sentence with a typo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "x.value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a8aab93b80fb82c",
      "metadata": {
        "collapsed": false,
        "id": "6a8aab93b80fb82c"
      },
      "source": [
        "While here it is not going to be useful, we can also do multiple optimization steps in a loop! Do not forget to reset the gradients after each step!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6bb8d0dcc2539d1",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-06-11T15:44:30.989940227Z"
        },
        "id": "d6bb8d0dcc2539d1"
      },
      "outputs": [],
      "source": [
        "optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a84aad4cd58737",
      "metadata": {
        "id": "a3a84aad4cd58737"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}