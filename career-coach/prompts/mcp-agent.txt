Directory structure:
└── joshuaalpuerto-mcp-agent/
    ├── README.md
    ├── package.json
    ├── pnpm-lock.yaml
    ├── theory_on_ai.md
    ├── tsconfig.json
    ├── .env-example
    ├── demo/
    │   ├── express/
    │   │   ├── README.md
    │   │   ├── nodemon.json
    │   │   ├── package.json
    │   │   ├── pnpm-lock.yaml
    │   │   ├── tsconfig.json
    │   │   ├── .env-example
    │   │   └── src/
    │   │       ├── index.ts
    │   │       └── router.ts
    │   ├── servers/
    │   │   ├── readLocalFileSystem.ts
    │   │   └── searchWeb.ts
    │   ├── standalone/
    │   │   └── index.ts
    │   └── tools/
    │       └── writeLocalSystem.ts
    ├── src/
    │   ├── agent.ts
    │   ├── app.ts
    │   ├── config.ts
    │   ├── index.ts
    │   ├── llm/
    │   │   ├── llmFireworks.ts
    │   │   ├── serviceCaller.ts
    │   │   └── types.ts
    │   ├── logger/
    │   │   └── index.ts
    │   ├── mcp/
    │   │   ├── mcpConnectionManager.ts
    │   │   ├── mcpServerAggregator.ts
    │   │   └── types.ts
    │   ├── memory/
    │   │   └── index.ts
    │   ├── tools/
    │   │   └── types.ts
    │   └── workflows/
    │       ├── orchestrator/
    │       │   ├── orchestrator.ts
    │       │   ├── prompt.ts
    │       │   └── types.ts
    │       └── router/
    │           └── prompt.ts
    └── .github/
        └── workflows/
            └── publish.yml

================================================
FILE: README.md
================================================
# mcp-agent

**Build Effective Agents with Model Context Protocol in TypeScript**

**mcp-agent** is a TypeScript framework inspired by the Python [lastmile-ai/mcp-agent](https://github.com/lastmile-ai/mcp-agent) project. It provides a simple, composable, and type-safe way to build AI agents leveraging the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) in JavaScript and TypeScript environments.

This library aims to bring the powerful patterns and architecture of `mcp-agent` to the JavaScript ecosystem, enabling developers to create robust and controllable AI agents that can interact with MCP-aware services and tools.

## Installation
```bash
npm install @joshuacalpuerto/mcp-agent
```

## Key Capabilities

**mcp-agent** empowers you to build sophisticated AI agents with the following core capabilities:

*   **Agent Abstraction:** Define intelligent agents with clear instructions, access to tools (both local functions and MCP servers), and integrated LLM capabilities.
*   **Model Context Protocol (MCP) Integration:** Seamlessly connect and interact with services and tools exposed through MCP servers.
*   **Local Function Tools:** Extend agent capabilities with custom, in-process JavaScript/TypeScript functions that act as tools, alongside MCP server-based tools.
*   **LLM Flexibility:** Integrate with various Large Language Models (LLMs). The library includes an example implementation for Fireworks AI, demonstrating extensibility for different LLM providers.
*   **Memory Management:** Basic in-memory message history to enable conversational agents.
*   **Workflows:** Implement complex agent workflows like the `Orchestrator` pattern to break down tasks into steps and coordinate multiple agents. Support for additional patterns from Anthropic's [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) and OpenAI's [Swarm](https://github.com/openai/swarm) coming soon.
*   **TypeScript & Type Safety:** Built with TypeScript, providing strong typing, improved code maintainability, and enhanced developer experience.

## Quick Start

### Standalone Usage

Get started quickly with a basic example (Using as standalone):

```js
import { fileURLToPath } from 'url';
import path from 'path';
import { Agent, LLMFireworks, Orchestrator } from 'mcp-agent'; // Import from your library name!
import { writeLocalSystem } from './tools/writeLocalSystem'; // Assuming you have example tools

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

async function runOrchestrator() {
  const llm = new LLMFireworks("accounts/fireworks/models/deepseek-v3", { // Example LLM from Fireworks
    maxTokens: 2048,
    temperature: 0.1
  });

  const researcher = await Agent.initialize({
    llm,
    name: "researcher",
    description: `Your expertise is to find information.`,
    serverConfigs: [ // Example MCP Server Configurations
      {
        name: "read_file_from_local_file_system",
        type: "stdio",
        command: "node",
        args: ['--loader', 'ts-node/esm', path.resolve(__dirname, 'servers', 'readLocalFileSystem.ts'),]
      },
      {
        name: "search_web",
        type: "ws",
        url: createSmitheryUrl( // Example using community mcp server via @smithery/sdk
          "https://server.smithery.ai/exa/ws",
          {
            exaApiKey: process.env.EXA_API_KEY
          }
        )
      },
    ],
  });

  const writer = await Agent.initialize({
    llm
    name: "writer",
    description: `Your expertise is to write information to a file.`,
    functions: [writeLocalSystem], // Example local function tool
    llm,
  });

  const orchestrator = new Orchestrator({
    llm,
    agents: [researcher, writer],
  });

  const result = await orchestrator.generate('Search new latest developemnt about AI and write about it to `theory_on_ai.md` on my local machine. no need to verify the result.');
  console.log(JSON.stringify(result));

  await researcher.close();
  await writer.close();
}

runOrchestrator().catch(console.error);
```

https://github.com/user-attachments/assets/122a388b-0dc8-4984-b189-22408a308d7f

**To run this example:**

1.  **Install Dependencies:**
    ```bash
    pnpm install
    ```
2.  **Set Environment Variables:** Create a `.env` file (or set environment variables directly) and add your API keys (e.g., `EXA_API_KEY`, Fireworks AI API key if needed).
3.  **Run the Demo:**
    ```bash
    node --loader ts-node/esm ./demo/standalone/index.ts
    ```

### Rest server Integration
For a complete Express.js integration example with multi-agent orchestration, check out the [demo/express/README.md](./demo/express/README.md).

## Core Concepts

*   **Agent:** The fundamental building block. An `Agent` is an autonomous entity with a specific role, instructions, and access to tools. Agents can invoke tools to perform actions and interact with external services.
*   **MCP Server Aggregator (`MCPServerAggregator`):** Manages the tools available to each individual agent. Each agent has its own aggregator that provides access to the specific tools that agent needs. The aggregator acts as a tool provider for its assigned agent.
*   **MCP Connection Manager (`MCPConnectionManager`):** Central repository that manages the lifecycle and reuse of ALL MCP server connections across the entire application. This is a global collection of all available tools that any agent can potentially use.
    * **Supported Transport**: `stdio`, `sse`, `streamable-http` & `websockets`
*   **LLM Integration (`LLMInterface`, `LLMFireworks`):**  Abstracts interaction with Large Language Models.  `LLMFireworks` is an example implementation for Fireworks AI models.
*   **Tools:**  Functions or MCP server capabilities that Agents can use to perform actions. Tools can be:
    *   **MCP Server Tools:** Capabilities exposed by external MCP servers (e.g., file system access, web search).
    *   **Local Function Tools:**  JavaScript/TypeScript functions defined directly within your application.
*   **Workflows:**  Composable patterns for building complex agent behaviors (see anthropic blog [here](https://www.anthropic.com/research/building-effective-agents)).
    *   **Orchestrator** - workflow demonstrates how to coordinate multiple agents to achieve a larger objective.
    *   **Prompt chaining** - coming soon.
    *   **Routing** - coming soon.
    *   **Parallelization** - coming soon.
    *   **Evaluator-optimizer** - coming soon.
*   **Memory (`SimpleMemory`):**  Provides basic in-memory message history for conversational agents.

## Architecture: Why Connection Manager + Aggregator?

The framework uses a two-layer architecture to efficiently manage MCP server connections:

**Connection Manager (Global):** 
- Maintains a single instance of each MCP server connection across the entire application
- Prevents duplicate server connections when multiple agents need the same tool
- Example: If Agent1 and Agent2 both need a file system tool, only one file system server is spawned

**Aggregator (Per-Agent):**
- Each agent has its own aggregator that provides access to its specific set of tools
- The aggregator references tools from the global connection manager
- Acts as a tool provider interface for its assigned agent

**Benefits:**
- **Resource Efficiency:** Avoid spinning multiple instances of the same MCP server
- **Connection Reuse:** Share server connections across agents when possible
- **Isolation:** Each agent only sees the tools it's configured to use
- **Scalability:** Add new agents without duplicating existing server connections

## Acknowledgements

This project is heavily inspired by and builds upon the concepts and architecture of the excellent [lastmile-ai/mcp-agent](https://github.com/lastmile-ai/mcp-agent) Python framework

We encourage you to explore their repository for a deeper understanding of the underlying principles and patterns that have informed this TypeScript implementation.

## Contributing

Contributions are welcome!

================================================
FILE: demo/express/README.md
================================================
# MCP Agent Express Demo

A demo Express.js application showcasing MCP Agent integration with a REST API server. This demo demonstrates how to integrate MCP's multi-agent orchestration capabilities within an Express.js application.

## Setup

1. Install dependencies:
```bash
npm install
```

2. Configure environment variables:
```bash
EXA_API_KEY=your_api_key_here
PORT=3000 # optional
```

3. Start the server:
```bash
# Development mode with auto-reload
npm run dev

# Build TypeScript files
npm run build

# Production mode (requires build first)
npm start
```

## Project Structure
```
src/
├── router/        # API route handlers with MCP agent implementations
├── types/         # TypeScript type definitions
└── index.ts       # Main application with MCP Agent setup
```

## MCP Integration
```typescript
import { App, Orchestrator, Agent, LLMFireworks, ServerConfig } from 'mcp-agent';

// Initialize MCP connection
const mcpApp = new App();
const app = express();

 app.get('/api/generate_blog_post', async (req: Request, res: Response) => {
    const { topic } = req.query as { topic: string };
    const llm = new LLMFireworks("accounts/fireworks/models/deepseek-v3", {
      maxTokens: 2048,
      temperature: 0.1
    })

    const exaServerConfig: ServerConfig = {
      name: "search_web",
      type: "ws",
      url: createSmitheryUrl(
        "https://server.smithery.ai/exa/ws",
        {
          exaApiKey: process.env.EXA_API_KEY
        }
      )
    }

    const researcher = await Agent.initialize({
      name: "researcher",
      description: `Your expertise is to find information in the web`,
      serverConfigs: [exaServerConfig],
      llm,
    });

    const editor = await Agent.initialize({
      name: "editor",
      description: `Your expertise to review the information and make it more comprehensive.`,
      serverConfigs: [exaServerConfig],
      llm,
    });

    const orchestrator = new Orchestrator({
      llm,
      agents: [researcher, editor],
    })

    const result = await orchestrator.generate(`Create comprehensive blog post releated to the topic: ${topic}`);

    res.json({ content: result });
  });

// Graceful shutdown handling
app.on('close', () => {
  // close MCP connection gracefully
  mcpApp.close();
});
```

## API Endpoints

### Blog Post Generation
- `GET /api/generate_blog_post?topic=your_topic` - Generates a blog post using multi-agent orchestration
  - Uses a researcher agent to gather information
  - Uses an editor agent to refine and improve the content
  - Returns the generated blog post content

## Example Usage

```bash
# Check health and MCP Agent status
curl http://localhost:3000/api/health

# Generate a blog post about AI
curl "http://localhost:3000/api/generate_blog_post?topic=artificial%20intelligence"
```

## Key Features

1. **MCP Agent Lifecycle Management**
   - Initializes MCP Agent connection on server startup
   - Maintains persistent connection throughout server lifecycle
   - Gracefully closes MCP connection on server shutdown

2. **Multi-Agent Orchestration**
   - Researcher agent for information gathering
   - Editor agent for content refinement
   - Orchestrator for managing agent interactions

3. **LLM Integration**
   - Uses Fireworks LLM model
   - Configurable parameters (tokens, temperature)
   - Integrated with agent workflows

## Important Notes

1. Ensure `EXA_API_KEY` is properly configured in your environment
2. The MCP Agent connection is initialized at server startup
3. All API endpoints are available under the `/api` prefix
4. Agents are initialized per request in the blog post generation endpoint 


================================================
FILE: demo/express/src/index.ts
================================================
import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';
import { App } from '../../../src/app';

import createRouter from './router';

// Load environment variables
dotenv.config();

// initialize MCP connection that will persist server that agents initialized
const mcpApp = new App();
const app = express();
const port = process.env.PORT || 3000;

// Middleware
app.use(cors());
app.use(express.json());
app.use('/api', createRouter());

// Start server
app.listen(port, () => {
  console.log(`Server running at http://localhost:${port}`);
});

app.on('close', () => {
  console.log('Closing server');
  // close MCP connection gracefully
  mcpApp.close();
});


================================================
FILE: demo/express/src/router.ts
================================================
import express, { Request, Response } from 'express';
import { createSmitheryUrl } from "@smithery/sdk/config"
import { Orchestrator, Agent, LLMFireworks, ServerConfig } from '../../../src/';


function createRouter() {
  const router = express.Router();

  router.get('/health', (req: Request, res: Response) => {
    res.json({ status: 'ok', timestamp: new Date().toISOString() });
  });

  router.get('/generate_blog_post', async (req: Request, res: Response) => {
    const { topic } = req.query as { topic: string };
    const llm = new LLMFireworks("accounts/fireworks/models/deepseek-v3", {
      maxTokens: 2048,
      temperature: 0.1,
      stream: true
    })

    const exaServerConfig: ServerConfig = {
      name: "search_web",
      type: "ws",
      url: createSmitheryUrl(
        "https://server.smithery.ai/exa/ws",
        {
          exaApiKey: process.env.EXA_API_KEY
        }
      )
    }

    const researcher = await Agent.initialize({
      name: "researcher",
      description: `Your expertise is to find information in the web`,
      serverConfigs: [exaServerConfig],
      llm,
    });

    const editor = await Agent.initialize({
      name: "editor",
      description: `Your expertise is to review the information and make it more comprehensive.`,
      serverConfigs: [exaServerConfig],
      llm,
    });

    const orchestrator = new Orchestrator({
      llm,
      agents: [researcher, editor],
    })

    const result = await orchestrator.generate(`Create comprehensive blog post releated to the topic: ${topic}`);

    res.json({ content: result });
  });

  return router;
}

export default createRouter;


================================================
FILE: demo/servers/readLocalFileSystem.ts
================================================
#!/usr/bin/env node
import fs from 'fs';
import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { type CallToolResult } from '@modelcontextprotocol/sdk/types.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import { z } from 'zod';

// Create an MCP server instance
const server = new McpServer({
  name: 'read-local-file-system',
  version: '1.0.0',
});

// Add a tool that echoes back the input
server.tool(
  'read_file_from_local_file_system',
  "Read a file from the local file system",
  { file_path: z.string() }, // Define input schema using zod
  async ({ file_path }): Promise<CallToolResult> => {
    const content = fs.readFileSync(file_path, 'utf8');
    return {
      content: [{
        type: 'text', text:
          `file read from ${file_path}: ${content}`
      }],
    }
  }
);

// Create a transport (stdio for this example)
const transport = new StdioServerTransport();

// Connect the server to the transport
server.connect(transport);


================================================
FILE: demo/servers/searchWeb.ts
================================================
#!/usr/bin/env node

import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { type CallToolResult } from '@modelcontextprotocol/sdk/types.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import { z } from 'zod';

// Create an MCP server instance
const server = new McpServer({
  name: 'echo-2',
  version: '1.0.0',
});

// Add a tool that echoes back the input
server.tool(
  'search_web',
  "Search the web for information",
  { message: z.string() }, // Define input schema using zod
  async ({ message }): Promise<CallToolResult> => ({
    content: [{
      type: 'text',
      text: `Here's a random blog post:
      
Title: The Future of AI Development in 2024

Artificial Intelligence continues to evolve at a breakneck pace, transforming industries and reshaping how we approach software development. As we move further into 2024, several key trends have emerged that are worth watching.

First, we're seeing increased focus on responsible AI development, with major tech companies implementing stricter ethical guidelines. This shift comes as public awareness of AI's societal impact grows.

Additionally, the rise of specialized AI models has created new opportunities for developers. Rather than relying solely on large language models, teams are now deploying targeted solutions for specific use cases.

The open source AI community has also flourished, with collaborative projects pushing the boundaries of what's possible. This democratization of AI technology has led to innovative applications across various sectors.

As we look ahead, it's clear that AI will continue to play a crucial role in shaping the future of technology. The key will be balancing rapid advancement with responsible implementation.

Thanks for reading!` }],
  })
);

// Create a transport (stdio for this example)
const transport = new StdioServerTransport();

// Connect the server to the transport
server.connect(transport);


================================================
FILE: demo/standalone/index.ts
================================================

import dotenv from 'dotenv';
import { createSmitheryUrl } from "@smithery/sdk/config"
import { fileURLToPath } from 'url';
import path from 'path';
import { Agent, LLMFireworks, Orchestrator } from '../../src';
import { writeLocalSystem } from '../tools/writeLocalSystem';

dotenv.config();

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

async function runOrchestrator() {
  const llm = new LLMFireworks("accounts/fireworks/models/deepseek-v3", {
    maxTokens: 2048,
    temperature: 0.1,
    stream: true
  })

  const researcher = await Agent.initialize({
    llm,
    name: "researcher",
    description: `Your expertise is to find information.`,
    serverConfigs: [
      {
        name: "read_file_from_local_file_system",
        type: "stdio",
        command: "node",
        args: ['--loader', 'ts-node/esm', path.resolve(__dirname, '..', 'servers', 'readLocalFileSystem.ts'),]
      },
      {
        name: "search_web",
        type: "http",
        url: createSmitheryUrl(
          "https://server.smithery.ai/exa",
          {
            exaApiKey: process.env.EXA_API_KEY
          }
        )
      }
    ],
  });

  const writer = await Agent.initialize({
    name: "writer",
    description: `Your expertise is to write information to a file.`,
    functions: [writeLocalSystem],
    llm,
  });

  const orchestrator = new Orchestrator({
    llm,
    agents: [researcher, writer],
  })

  const result = await orchestrator.generate('Search new latest developemnt about AI and write about it to `theory_on_ai.md` on my local machine. no need to verify the result.');
  console.log(JSON.stringify(result))

  await researcher.close()
  await writer.close()
}

runOrchestrator().catch(console.error);


================================================
FILE: demo/tools/writeLocalSystem.ts
================================================
import fs from 'fs';
import { type CallToolResult, FunctionToolInterface } from '../../src';

export const writeLocalSystem: FunctionToolInterface = {
  name: 'writeLocalSystem',
  description: 'Write a file to the local system',
  parameters: {
    type: 'object',
    properties: {
      path: {
        type: 'string',
        description: 'The path to the file to write',
      },
      content: {
        type: 'string',
        description: 'The content to write to the file',
      },
    },
    required: ['path', 'content'],
  },
  execute: async (args: any): Promise<CallToolResult> => {
    const { path, content } = args;
    fs.writeFileSync(path, content);
    return { content: [{ type: 'text', text: `File written successfully in ${path}` }] };
  }
}


================================================
FILE: src/agent.ts
================================================
import OpenAI from 'openai';
import MCPServerAggregator from './mcp/mcpServerAggregator';
import { SimpleMemory, Memory } from './memory';
import { LLMConfig, LLMInterface } from './llm/types';
import { FunctionToolInterface } from './tools/types';
import { ServerConfig } from './mcp/types';
import { Logger } from './logger';
interface AgentConfig {
  name: string;
  description: string;
  serverConfigs?: ServerConfig[];
  history?: Memory<OpenAI.ChatCompletionMessageParam>;
  functions?: FunctionToolInterface[];
  llm: LLMInterface;
  maxIterations?: number;
  logger?: Logger;
}

export class Agent {
  public name: string;
  public description: string;
  public serverConfigs?: ServerConfig[];
  public functions?: Record<string, FunctionToolInterface>;
  private history: Memory<OpenAI.ChatCompletionMessageParam>;
  private llm: LLMInterface | null;
  private aggregator?: MCPServerAggregator;
  private maxIterations: number;
  private systemPrompt: string;
  private logger: Logger;

  constructor(config: AgentConfig & {
    // optional aggregator
    // this is used when agent is used in a workflow that doesn't need to access MCP
    aggregator?: MCPServerAggregator;
  }) {
    this.logger = config.logger || Logger.getInstance();
    this.name = config.name;
    this.description = config.description;
    this.serverConfigs = config.serverConfigs;
    this.llm = config.llm;
    this.maxIterations = config.maxIterations || 5;

    if (config.functions?.length) {
      this.functions = {}
      for (const tool of config.functions) {
        this.functions[tool.name] = tool
      }
    }

    if (config.aggregator) {
      this.aggregator = config.aggregator;
    }

    this.history = config.history || new SimpleMemory<OpenAI.ChatCompletionMessageParam>();
    this.systemPrompt = `You are a ${this.name}. ${this.description} \n\n You have ability to use tools to help you complete the task.`
  }

  static async initialize(config: AgentConfig) {
    // if we have server names then initialize Agent with MCP
    if (config.serverConfigs?.length) {
      const aggregator = await MCPServerAggregator.load(config.serverConfigs);
      const agent = new Agent({ ...config, aggregator });

      return agent
    }

    return new Agent(config);
  }

  public async generate(prompt: string, config?: LLMConfig) {
    if (!this.llm) {
      throw new Error(`Agent: ${this.name} LLM is not initialized`);
    }

    this.logger.info(`[Agent: ${this.name}] woking on user task: ${prompt}`);

    this.history.append({
      role: 'user',
      content: prompt,
    })

    let messages: OpenAI.ChatCompletionMessageParam[] = [
      { role: 'system', content: this.systemPrompt },
      ...this.history.get(),
    ]
    let iterations = 0;

    while (iterations < this.maxIterations) {
      const tools = await this.listTools();
      const result = await this.llm.generate({
        messages: messages,
        config: {
          ...config,
          tools
        }
      });

      messages.push({
        role: 'assistant',
        content: result.content,
        tool_calls: result.toolCalls,
      })

      if ((result.finishReason === 'tool_calls' || result.finishReason === 'function_call') && result.toolCalls?.length) {
        for (const toolCall of result.toolCalls) {
          this.logger.info(`[Agent: ${this.name}] executing tool: ${toolCall.function.name}`);
          const toolResult = await this.callTool(toolCall.function.name, typeof toolCall.function.arguments === 'string'
            ? JSON.parse(toolCall.function.arguments)
            : toolCall.function.arguments || {});

          if (!toolResult.content.length) {
            throw new Error(`Tool: ${toolCall.function.name} call failed`);
          }

          const toolResultContent = JSON.stringify(toolResult) as string
          this.logger.info(`[Agent: ${this.name}] tool: ${toolCall.function.name} call result: ${toolResultContent}`);
          messages.push({
            role: 'tool',
            content: toolResultContent,
            tool_call_id: toolCall.id,
          })
        }
      } else {
        this.logger.info(`[Agent: ${this.name}] final response: ${result.content}`);
        // We only care about the actual result from the task
        this.history.append({
          role: 'assistant',
          content: result.content,
        })
        break;
      }

      iterations++;
    }

    return this.history.get();
  }

  public async generateStr(prompt: string, config?: LLMConfig): Promise<string> {
    const result = await this.generate(prompt, config);
    const lastMessage = result[result.length - 1];
    const content = lastMessage.content;
    return content as string;
  }

  public async generateStructuredResult(prompt: string, config?: LLMConfig): Promise<any> {
    const result = await this.generate(prompt, config);
    // get the last message
    const lastMessage = result[result.length - 1];
    return JSON.parse(lastMessage.content as string);
  }


  public async listTools(): Promise<OpenAI.ChatCompletionTool[]> {
    // Get base tools from the aggregator
    let result: OpenAI.ChatCompletionTool[] = [];
    if (this.aggregator) {
      const baseTools = this.aggregator.getAllTools();
      result = baseTools.map(({ tool }) => ({
        type: 'function',
        function: tool
      }));
    }

    // include internal functions
    if (this.functions) {
      result = result.concat(Object.values(this.functions).map(({ name, parameters, description }) => ({
        type: 'function',
        function: { name, parameters, description }
      })));
    }

    return result;
  }

  // we can apply some logic here once we run the tool
  private async callTool(toolName: string, args: Object): Promise<any> {
    const isMCPTool = this.aggregator?.findTool(toolName);
    if (isMCPTool && this.aggregator) {
      return this.aggregator.executeTool(toolName, args);
    }

    if (this.functions?.[toolName]) {
      return this.functions[toolName].execute(args);
    }

    throw new Error(`Tool: ${toolName} not found`);
  }

  public async close() {
    if (this.aggregator) {
      await this.aggregator.close();
    }
  }
}


================================================
FILE: src/app.ts
================================================
import MCPConnectionManager from "./mcp/mcpConnectionManager";

/**
 * Persistent MCP connection throught the app lifecycle
 */
export class App {
  private mcpConnectionManager: MCPConnectionManager;

  constructor() {
    this.mcpConnectionManager = MCPConnectionManager.getInstance();
  }

  close() {
    this.mcpConnectionManager.disconnectAll();
  }
}


================================================
FILE: src/config.ts
================================================
import dotenv from 'dotenv';

dotenv.config();

// config keys
export const LANGSMITH_DEBUG = 'LANGSMITH_DEBUG';
export const DEBUG_MODE = 'DEBUG_MODE';
export const OPENAI_API_KEY = 'OPENAI_API_KEY';
export const OPENAI_BASE_URL = 'OPENAI_BASE_URL';
export const OPENAI_MAX_TOKENS = 'OPENAI_MAX_TOKENS';
export const LANGSMITH_TRACING = 'LANGSMITH_TRACING';
export const LANGSMITH_ENDPOINT = 'LANGSMITH_ENDPOINT';
export const LANGSMITH_API_KEY = 'LANGSMITH_API_KEY';
export const LANGSMITH_PROJECT = 'LANGSMITH_PROJECT';

const allConfigFromEnv = () => {
  return {
    [OPENAI_API_KEY]: process.env[OPENAI_API_KEY] || undefined,
    [OPENAI_BASE_URL]: process.env[OPENAI_BASE_URL] || undefined,
    [OPENAI_MAX_TOKENS]: process.env[OPENAI_MAX_TOKENS] || undefined,
    [LANGSMITH_DEBUG]: process.env[LANGSMITH_DEBUG] || undefined,
    [DEBUG_MODE]: process.env[DEBUG_MODE] || undefined,
    [LANGSMITH_TRACING]: process.env[LANGSMITH_TRACING] || undefined,
    [LANGSMITH_ENDPOINT]: process.env[LANGSMITH_ENDPOINT] || undefined,
    [LANGSMITH_API_KEY]: process.env[LANGSMITH_API_KEY] || undefined,
    [LANGSMITH_PROJECT]: process.env[LANGSMITH_PROJECT] || undefined,
  };
};

let userConfig: ReturnType<typeof allConfigFromEnv> = {} as any;

export const getAIConfig = (
  configKey: keyof typeof userConfig,
): string | undefined => {
  if (typeof userConfig[configKey] !== 'undefined') {
    return userConfig[configKey];
  }
  return allConfigFromEnv()[configKey];
};



================================================
FILE: src/index.ts
================================================
export { Agent } from './agent'
export { Orchestrator } from './workflows/orchestrator/orchestrator'
export { LLMFireworks } from './llm/llmFireworks'
export { App } from './app'
// types
export { type LLMConfig, type LLMInterface, type LLMResult, type AIResponseFormat } from './llm/types'
export { type CallToolResult, type FunctionToolInterface } from './tools/types'
export { type ServerConfig } from './mcp/types'
export { type LoggerInterface, Logger } from './logger'


================================================
FILE: src/llm/llmFireworks.ts
================================================
import OpenAI from 'openai';
import { getAIConfig, OPENAI_API_KEY, OPENAI_BASE_URL } from '../config';
import { AIResponseFormat, LLMInterface, LLMConfig, LLMResult } from './types';
import { createChatClient } from './serviceCaller';

type FireworksResponseFormat = {
  type: AIResponseFormat.JSON;
  schema: Record<string, any>
}

export class LLMFireworks implements LLMInterface {
  private client: OpenAI;
  private model: string;
  private config: LLMConfig;

  constructor(model: string, config: LLMConfig) {
    this.client = createChatClient({
      baseURL: getAIConfig(OPENAI_BASE_URL),
      apiKey: getAIConfig(OPENAI_API_KEY),
      ...config,

    });
    this.model = model;
    this.config = config;
  }

  async generate(params: {
    messages: OpenAI.ChatCompletionMessageParam[];
    config: LLMConfig;
  }): Promise<LLMResult> {
    const { messages, config } = params;
    const configToUse = { ...this.config, ...config };
    const commonConfig: OpenAI.ChatCompletionCreateParams = {
      model: this.model,
      messages,
      tools: configToUse?.tools,
      max_tokens: configToUse?.maxTokens ?? 2048,
      temperature: configToUse?.temperature ?? 0.1,
    }

    // if response format is available lets format it a bit as fireworks expect a bit different format
    if (configToUse?.responseFormat?.json_schema) {
      const responseFormat: OpenAI.ChatCompletionCreateParams['response_format']
        | OpenAI.ResponseFormatJSONSchema
        | FireworksResponseFormat = {
        type: AIResponseFormat.JSON,
        schema: configToUse?.responseFormat?.json_schema?.schema,
      };
      commonConfig.response_format = responseFormat;
    }

    try {
      if (configToUse?.stream) {
        const completion = await this.client.chat.completions.create({
          ...commonConfig,
          stream: true,
        } as OpenAI.ChatCompletionCreateParamsStreaming);

        const result = await this.handleStreamResponse(completion);

        return {
          content: result.content,
          usage: result.usage,
          toolCalls: result.toolCalls,
          finishReason: result.finishReason,
        }
      } else {
        const completion: OpenAI.ChatCompletion = await this.client.chat.completions.create({
          ...commonConfig,
          stream: false,
        } as OpenAI.ChatCompletionCreateParamsNonStreaming);

        return {
          content: completion.choices[0].message.content || '',
          usage: completion.usage,
          toolCalls: completion.choices[0].message.tool_calls,
          finishReason: completion.choices[0].finish_reason,
        };
      }
    } catch (error) {
      throw this.handleError(error);
    }
  }

  private async handleStreamResponse(stream: AsyncIterable<OpenAI.ChatCompletionChunk>): Promise<LLMResult> {
    let contentChunk = '';
    let toolCall: Record<number, OpenAI.ChatCompletionMessageToolCall> | undefined = {};
    let usage: OpenAI.CompletionUsage | undefined;
    let finishReason: string | undefined;

    return new Promise(async (resolve) => {
      for await (const chunk of stream) {
        contentChunk += (chunk.choices[0]?.delta?.content || '');
        if (chunk.choices[0]?.delta?.tool_calls?.length) {
          // LLM could return multiple tool_calls so we need to register them in their index.
          const chunkToolCall = chunk.choices[0].delta.tool_calls[0] as OpenAI.ChatCompletionMessageToolCall & { index: number };
          if (!toolCall?.[chunkToolCall.index]) {
            // When the first tool call is received, we need to initialize the tool call object which have initial id, type and function.name
            toolCall[chunkToolCall.index] = chunk.choices[0].delta.tool_calls[0] as OpenAI.ChatCompletionMessageToolCall;
          }
          // function argumetn might come in multiple chunks, so we need to concatenate them
          toolCall[chunkToolCall.index].function.arguments += chunk.choices[0].delta.tool_calls[0].function?.arguments || ''
        }
        usage = chunk.usage || undefined
        finishReason = chunk.choices[0]?.finish_reason || undefined
      }

      resolve({ content: contentChunk, toolCalls: toolCall ? Object.values(toolCall) : [], usage, finishReason })
    })
  }

  private handleError(error: any): Error {
    if (error instanceof OpenAI.APIError) {
      return new Error(`LLM API error (${error.status}): ${error.message}`);
    }
    return new Error(`Request failed: ${error.message}`);
  }
}



================================================
FILE: src/llm/serviceCaller.ts
================================================
import OpenAI from 'openai';
import { wrapOpenAI } from 'langsmith/wrappers';
import { getAIConfig, OPENAI_API_KEY, OPENAI_BASE_URL, LANGSMITH_TRACING } from '../config';

export function createChatClient(config: any): OpenAI {

  let openai = new OpenAI({
    baseURL: getAIConfig(OPENAI_BASE_URL),
    apiKey: getAIConfig(OPENAI_API_KEY),
    ...config
  });

  if (openai && getAIConfig(LANGSMITH_TRACING)) {
    console.log('DEBUGGING MODE: langsmith wrapper enabled');
    openai = wrapOpenAI(openai);
  }

  return openai;
}


================================================
FILE: src/llm/types.ts
================================================
import OpenAI from 'openai';

export type AIUsageInfo = Record<string, any> & {
  prompt_tokens: number;
  completion_tokens: number;
  total_tokens: number;
};

export enum AIResponseFormat {
  JSON = 'json_object',
  TEXT = 'text',
}

export type LLMConfig = Partial<{
  maxTokens: number;
  temperature: number;
  stream: boolean;
  tools: OpenAI.ChatCompletionTool[];
  responseFormat: OpenAI.ResponseFormatJSONSchema | OpenAI.ResponseFormatJSONSchema
}>

export type LLMResult = Partial<{
  content: string | null;
  usage: AIUsageInfo;
  toolCalls: OpenAI.ChatCompletionMessageToolCall[];
  finishReason: string;
}>

export interface LLMInterface {
  generate(params: {
    messages: OpenAI.ChatCompletionMessageParam[];
    config: LLMConfig;
  }): Promise<LLMResult>;
}


================================================
FILE: src/logger/index.ts
================================================
// Logging module exports
import util from 'util';

// ANSI escape codes for colors
export const COLORS = {
  RESET: "\x1b[0m",
  RED: "\x1b[31m",
  YELLOW: "\x1b[33m",
  BLUE: "\x1b[34m",
  CYAN: "\x1b[36m",
  MAGENTA: "\x1b[35m",
  DIM: "\x1b[2m",
}

export enum LogLevel {
  DEBUG = 'debug',
  INFO = 'info',
  WARN = 'warn',
  ERROR = 'error'
}

export interface LogEntry {
  timestamp: number;
  level: LogLevel;
  message: string;
  context?: Record<string, unknown>;
}

export interface LoggerInterface {
  info(message: string, context?: any): void;
  warn(message: string, context?: any): void;
  error(message: string, context?: any): void;
  debug(message: string, context?: any): void;
  log?(level: string, message: string, context?: any): void;
}

export class Logger {
  private static instance: Logger;
  private logs: LogEntry[] = [];
  private logger: LoggerInterface;

  // default to console logger
  private constructor(logger: LoggerInterface) {
    this.logger = logger;
  }

  public static getInstance(logger: LoggerInterface = console): Logger {
    if (!Logger.instance) {
      Logger.instance = new Logger(logger);
    }
    return Logger.instance;
  }

  public getLogs(level?: LogLevel): LogEntry[] {
    return level
      ? this.logs.filter(log => log.level === level)
      : this.logs;
  }

  public clearLogs(): void {
    this.logs = [];
  }

  public exportLogs(format: 'json' | 'text' = 'json'): string {
    if (format === 'json') {
      return JSON.stringify(this.logs, null, 2);
    }

    return this.logs.map(log =>
      `[${new Date(log.timestamp).toISOString()}] [${log.level.toUpperCase()}] ${log.message}` +
      (log.context ? ` | ${JSON.stringify(log.context)}` : '')
    ).join('\n');
  }

  public info(message: string, context?: any): void {
    this.log(LogLevel.INFO, message, context);
  }

  public warn(message: string, context?: any): void {
    this.log(LogLevel.WARN, message, context);
  }

  public error(message: string, context?: any): void {
    this.log(LogLevel.ERROR, message, context);
  }

  public debug(message: string, context?: any): void {
    this.log(LogLevel.DEBUG, message, context);
  }

  private log(level: LogLevel, message: string, context?: any): void {
    if (level in LogLevel) {
      const entry: LogEntry = {
        timestamp: Date.now(),
        level,
        message,
        context
      };
      this.outputLog(entry);
    }
  }

  private outputLog(entry: LogEntry): void {
    const formattedTimestamp = new Date(entry.timestamp).toISOString();
    const contextString = entry.context
      ? `${COLORS.DIM} | Context: ${util.inspect(entry.context, { depth: null, colors: true })}${COLORS.RESET}`
      : '';

    let levelColor = COLORS.RESET;
    let levelTag = entry.level.toUpperCase();
    let consoleMethod: (...data: any[]) => void = console.log;

    switch (entry.level) {
      case LogLevel.DEBUG:
        levelColor = COLORS.BLUE;
        levelTag = 'DEBUG';
        break;
      case LogLevel.INFO:
        levelColor = COLORS.CYAN;
        levelTag = 'INFO';
        break;
      case LogLevel.WARN:
        levelColor = COLORS.YELLOW;
        levelTag = 'WARN';
        break;
      case LogLevel.ERROR:
        levelColor = COLORS.RED;
        levelTag = 'ERROR';
        break;
    }

    const coloredLevelTag = `${levelColor}[${levelTag}]${COLORS.RESET}`;
    const coloredTimestamp = `${COLORS.DIM}[${formattedTimestamp}]${COLORS.RESET}`;

    this.logger[entry.level](`${coloredTimestamp} ${coloredLevelTag} ${entry.message}${contextString}`);
  }
}



================================================
FILE: src/mcp/mcpConnectionManager.ts
================================================
import { Client } from '@modelcontextprotocol/sdk/client/index';
import { Transport } from '@modelcontextprotocol/sdk/shared/transport';
import { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio';
import { WebSocketClientTransport } from "@modelcontextprotocol/sdk/client/websocket.js"
import { SSEClientTransport } from '@modelcontextprotocol/sdk/client/sse.js';
import { StreamableHTTPClientTransport } from "@modelcontextprotocol/sdk/client/streamableHttp.js";
import { type ServerConfig } from './types';
import { Logger } from '../logger';

class MCPConnectionManager {
  private static instance: MCPConnectionManager;
  private runningServers: Map<string, Client> = new Map();
  private maxReconnectAttempts: number = 2;
  private reconnectDelay: number = 1000; // 1 second
  private logger: Logger;

  constructor() {
    this.logger = Logger.getInstance();
  }

  public static getInstance(): MCPConnectionManager {
    if (!MCPConnectionManager.instance) {
      MCPConnectionManager.instance = new MCPConnectionManager();
    }
    return MCPConnectionManager.instance;
  }

  async launchServer(id: string, config: ServerConfig): Promise<Client> {

    // if the server is already running, return the client
    if (this.runningServers.has(id)) {
      return this.runningServers.get(id) as Client;
    }

    // Create appropriate transport
    let transport;
    switch (config.type) {
      case 'stdio':
        // how can we run this as child process?
        transport = new StdioClientTransport({
          command: config.command,
          args: config.args || []
        });
        break;
      case "ws":
        const wsUrl = config.url instanceof URL ? config.url : new URL(config.url);
        // NOTE: ws transport requires node >= 21.0.0
        transport = new WebSocketClientTransport(wsUrl);
        break;
      case 'sse':
        const sseUrl = config.url instanceof URL ? config.url : new URL(config.url);
        transport = new SSEClientTransport(sseUrl);
        break;
      case 'http':
        const httpUrl = config.url instanceof URL ? config.url : new URL(config.url);
        transport = new StreamableHTTPClientTransport(httpUrl);
        break;
    }

    // this  might throw error but to satisfy typescript we need to cast to Client
    const client = await this.connectClient(id, transport) as Client;

    // Store the connection
    this.runningServers.set(id, client);

    return client;
  }

  private async connectClient(id: string, transport: Transport, connectAttempts: number = 0): Promise<Client | void> {
    try {
      this.logger.info(`Connecting to server: ${id}`);
      // Create a new client
      const client = new Client(
        { name: `multi-server-client-${id}`, version: '1.0.0' },
        { capabilities: { tools: {}, resources: {}, prompts: {} } }
      );
      // Connect and initialize
      await client.connect(transport);
      this.logger.info(`Connected to server: ${id}`);
      return client;
    } catch (error) {
      if (connectAttempts >= this.maxReconnectAttempts) {
        this.logger.error(`Max reconnection attempts reached for server ${id}`);
        throw error;
      }

      const delay = this.reconnectDelay * Math.pow(2, connectAttempts - 1); // Exponential backoff
      this.logger.info(`Attempting to reconnect to server ${id} (attempt ${connectAttempts + 1}) in ${delay}ms`);
      await new Promise(resolve => setTimeout(resolve, delay));
      return this.connectClient(id, transport, connectAttempts + 1);
    }
  }

  getClient(id: string): Client | undefined {
    return this.runningServers.get(id);
  }

  getAllClients(): Client[] {
    return Array.from(this.runningServers.values());
  }

  async disconnectServer(id: string): Promise<void> {
    const client = this.runningServers.get(id);
    if (!client) {
      this.logger.info(`Server ${id} not found or already disconnected`);
      return;
    }

    try {
      await client.close();
      this.runningServers.delete(id);
      this.logger.info(`Disconnected from server: ${id}`);
    } catch (error) {
      this.logger.error(`Error disconnecting from server ${id}: ${error}`);
    }
  }

  async disconnectAll(): Promise<void> {
    const disconnections = Array.from(this.runningServers.entries()).map(async ([id, client]) => {
      try {
        await client.close();
        this.logger.info(`Disconnected from server: ${id}`);
      } catch (error) {
        this.logger.error(`Error disconnecting from server ${id}: ${error}`);
      }
    });

    await Promise.all(disconnections);
    this.runningServers.clear();
  }

  // Get all available tools across all servers
  async getAllTools(): Promise<{ serverId: string, tools: any[] }[]> {
    const results = await Promise.all(
      Array.from(this.runningServers.entries()).map(async ([id, client]) => {
        try {
          const tools = await client.listTools();
          return { serverId: id, tools: tools.tools };
        } catch (error) {
          this.logger.error(`Error getting tools from server ${id}: ${error}`);
          return { serverId: id, tools: [] };
        }
      })
    );

    return results;
  }
}

export default MCPConnectionManager


================================================
FILE: src/mcp/mcpServerAggregator.ts
================================================
import { Client } from '@modelcontextprotocol/sdk/client/index';
import MCPConnectionManager from './mcpConnectionManager';
import { CallToolResult, Tool } from '../tools/types';
import { ServerConfig } from './types';
import { Logger, LogLevel } from '../logger';
// It's called aggregator because it's aggregating tools from multiple servers
// agent has 1:1 connection with aggregator
class MCPServerAggregator {
  private servers: Map<string, {
    client: Client,
    config: any,
    capabilities: {
      tools: any[],
    }
  }> = new Map();
  private serverConfigs: ServerConfig[];
  private connectionManager: MCPConnectionManager;
  private logger: Logger;

  constructor({ serverConfigs }: { serverConfigs: ServerConfig[] }) {
    this.serverConfigs = serverConfigs;
    this.connectionManager = MCPConnectionManager.getInstance();
    this.logger = Logger.getInstance();
  }

  static async load(serverConfigs: ServerConfig[]): Promise<MCPServerAggregator> {
    const mcpServeAggregator = new MCPServerAggregator({ serverConfigs });
    await mcpServeAggregator.loadServers();
    return mcpServeAggregator;
  }

  async loadServers(): Promise<void> {
    for (const serverConfig of this.serverConfigs) {
      try {
        const client = await this.connectionManager.launchServer(serverConfig.name, serverConfig);

        const tools = await client.listTools();

        // Store server information
        this.servers.set(serverConfig.name, {
          client,
          config: serverConfig,
          capabilities: {
            tools: tools.tools,
          }
        });
      } catch (error) {
        this.logger.error(`Error adding server "${serverConfig.name}": ${error}`);
        throw new Error(`Error adding server "${serverConfig.name}": ${error}`);
      }
    }
  }

  // Get all tools across all servers
  getAllTools(): { id: string, tool: Tool }[] {
    const allTools: { id: string, tool: Tool }[] = [];
    for (const [id, server] of this.servers.entries()) {
      for (const tool of server.capabilities.tools) {
        allTools.push({
          id: id, tool: {
            name: tool.name,
            description: tool.description,
            parameters: tool.inputSchema,
          }
        });
      }
    }

    return allTools;
  }

  // Find a specific tool across all servers
  findTool(toolName: string): { id: string, tool: Tool } | null {
    const tool = this.getAllTools().find(t => t.tool.name === toolName);
    if (tool) return tool;
    return null;
  }

  // Execute a tool on the appropriate server
  async executeTool(toolName: string, args: any): Promise<CallToolResult> {
    const toolInfo = this.findTool(toolName);
    if (!toolInfo) {
      throw new Error(`Tool ${toolName} not found on any server`);
    }

    const client = this.connectionManager.getClient(toolInfo.id);
    if (!client) {
      throw new Error(`Client for server ${toolInfo.id} not found`);
    }
    
    return await client.callTool({ name: toolName, arguments: args });
  }

  async close(): Promise<void> {
    const closePromises = Array.from(this.servers.entries()).map(async ([id]) => {
      try {
        await this.connectionManager.disconnectServer(id);
      } catch (error) {
        this.logger.error(`Error closing server ${id}: ${error}`);
      }
    });

    await Promise.all(closePromises);
    this.servers.clear();
  }
}

export default MCPServerAggregator


================================================
FILE: src/mcp/types.ts
================================================

type StdioServerConfig = {
  name: string;
  type: 'stdio';
  command: string;
  args?: string[];
}

type SSEServerConfig = {
  name: string;
  type: 'sse';
  url: string | URL;
}

type StreamableHTTPConfig = {
  name: string;
  type: 'http';
  url: string | URL;
}

type WebSocketServerConfig = {
  name: string;
  type: 'ws';
  url: string | URL;
}

export type ServerConfig = StdioServerConfig | SSEServerConfig | WebSocketServerConfig | StreamableHTTPConfig


================================================
FILE: src/memory/index.ts
================================================
export interface Memory<MessageParamT> {
  extend(messages: MessageParamT[]): void;
  set(messages: MessageParamT[]): void;
  append(message: MessageParamT): void;
  get(): MessageParamT[];
  clear(): void;
}

export class SimpleMemory<MessageParamT> implements Memory<MessageParamT> {
  /**
   * Simple memory management for storing past interactions in-memory.
   */
  private history: MessageParamT[] = [];

  constructor() {
    this.history = [];
  }

  extend(messages: MessageParamT[]): void {
    this.history.push(...messages);
  }

  set(messages: MessageParamT[]): void {
    this.history = [...messages]; // Use spread syntax for copy
  }

  append(message: MessageParamT): void {
    this.history.push(message);
  }

  get(): MessageParamT[] {
    return this.history;
  }

  clear(): void {
    this.history = [];
  }
}


================================================
FILE: src/tools/types.ts
================================================
import { OpenAI } from 'openai';
import { type CallToolResult as CallToolResultType, type CompatibilityCallToolResult as CompatibilityCallToolResultType } from '@modelcontextprotocol/sdk/types';

export type Tool = OpenAI.ChatCompletionTool['function'];

export type FunctionToolInterface = OpenAI.ChatCompletionTool['function'] & {
  execute: (args: any) => Promise<any> | any;
}

export type CallToolResult = CallToolResultType | CompatibilityCallToolResultType



================================================
FILE: src/workflows/orchestrator/orchestrator.ts
================================================
import OpenAI from 'openai';
import { Agent } from '../../agent';
import { LLMInterface } from '../../llm/types';
import { fullPlanSchemaReponseFormat, generateFullPlanPrompt, generatePlanObjectivePrompt, generateTaskPrompt } from './prompt';
import { PlanResult, PlanStepResult, PlanTaskResult, PlanStatus } from './types';
import { Logger, LogLevel } from '../../logger';

class Orchestrator {
  llm: LLMInterface;
  planner: Agent;
  synthesizer: Agent;
  agents: Record<string, Agent>;
  maxIterations: number;
  logger: Logger;

  constructor(config: {
    llm: LLMInterface,
    agents: Agent[],
    planner?: Agent,
    synthesizer?: Agent,
    maxIterations?: number,
    logger?: Logger,
  }) {
    this.llm = config.llm;
    this.maxIterations = config.maxIterations || 2;
    this.logger = config.logger || Logger.getInstance();

    // load agent as it is as we don't need mcp here.
    this.planner = config.planner || new Agent({
      name: "LLM Orchestration Planner",
      description: `
                You are an expert planner. Given an objective task and a list of Agents (which are collections of servers), your job is to break down the objective into a series of steps,
                which can be performed by LLMs with access to the servers or agents.
                `,
      llm: config.llm,
    });

    this.synthesizer = config.synthesizer || new Agent({
      name: "LLM Orchestration Synthesizer",
      description: `
                You are an expert synthesizer. Given a list of steps and their results, your job is to synthesize the results into a cohesive result.
                `,
      llm: config.llm,
    });

    this.agents = {};
    for (const agent of config.agents) {
      this.agents[agent.name] = agent;
    }
  }

  async generate(
    message: string,
  ): Promise<string> {
    const objective = String(message);
    const planResult: PlanResult = {
      steps: [],
      objective: objective,
      result: '',
      status: PlanStatus.InProgress,
    };

    this.logger.info(`Generating full plan for objective: ${objective}`);
    const prompt = await this.prepareFullPlanPrompt(objective);
    const plan: PlanResult = await this.planner.generateStructuredResult(prompt, {
      responseFormat: fullPlanSchemaReponseFormat as OpenAI.ResponseFormatJSONSchema
    });

    for (const step of plan.steps) {
      const stepResult = await this.executeStep(step, planResult);
      planResult.steps.push(stepResult);
    }

    const planResultInfo = await this._formatPlanResultInfo(planResult);
    return this.synthesizer.generateStr(planResultInfo);
  }

  async executeStep(
    step: PlanStepResult,
    previousResult: PlanResult
  ): Promise<PlanStepResult> {

    const previousResultInfo = await this._formatPlanResultInfo(previousResult);
    const stepResult: PlanStepResult = {
      objective: step.objective,
      tasks: [],
      result: '',
    };

    for (const task of step.tasks) {
      const agent = this.agents[task.agent]
      if (!agent) {
        throw new Error(`No agent found matching ${task.agent}`);
      }

      const context = generateTaskPrompt({
        // overall objective
        objective: previousResult.objective,
        task: task.description,
        previousResultInfo: previousResultInfo,
      });

      const result = await agent.generateStr(context);
      this.logger.info(`[Agent: ${task.agent}]\nTask: ${task.description}\nResult: ${result}`);
      stepResult.tasks.push({
        ...task,
        result: result,
      });
    }

    return stepResult;
  }

  async prepareFullPlanPrompt(
    objective: string,
  ): Promise<string> {
    const agentsInfo = await this._formatAgentsInfo();

    return generateFullPlanPrompt({
      objective: objective,
      agents: agentsInfo,
    });
  }

  async _formatAgentsInfo(): Promise<string> {
    const agentsInfo = await Promise.all(Object.values(this.agents).map(async (agent, idx) => {
      const agentInfo = await this._formatAgentInfo(agent.name);
      return `${idx + 1}. ${agentInfo}`
    }))
    return agentsInfo.join('\n');
  }

  async _formatAgentInfo(agentName: string): Promise<string> {
    const agent = this.agents[agentName];
    if (!agent) {
      return "";
    }

    const tools = await agent.listTools()

    const servers = tools.map(tool =>
      `- ${tool.function.name} - ${tool.function.description}`
    ).join('\n');

    return `Agent Name: ${agent.name}
    Description: ${agent.description}
    Capabilities:
    ${servers}`;
  }

  async _formatPlanResultInfo(planResult: PlanResult): Promise<string> {
    const formatStepResult = (step: PlanStepResult): string => {
      const tasksStr = step.tasks
        .map((task: PlanTaskResult) => `- Task: ${task.description}\nResult: ${task.result || ''}`)
        .join("\n");

      return `Step: ${step.objective}\nStep Subtasks:\n${tasksStr}`;
    }

    const stepsStr = planResult?.steps
      ?.map((step: PlanStepResult, i) => `${i + 1}. ${formatStepResult(step)}`)
      ?.join("\n\n") ?? "No steps executed yet";

    return generatePlanObjectivePrompt({
      planResultString: stepsStr,
    })
  }
}



export { Orchestrator };


================================================
FILE: src/workflows/orchestrator/prompt.ts
================================================
export const generateTaskPrompt = (variables: { objective: string, task: string, previousResultInfo: string }) => `You are part of a larger workflow to achieve the objective: ${variables.objective}.
Your job is to accomplish only the following task: ${variables.task}.

Results so far that may provide helpful context:
${variables.previousResultInfo}`

export const generatePlanObjectivePrompt = (variables: { planResultString?: string }) => `Progress So Far (steps completed):
${variables.planResultString || ''}`

export const generateFullPlanPrompt = (variables: { objective: string, agents: string }) => `You are tasked with orchestrating a plan to complete an objective.
You can analyze results from the previous steps already executed to decide if the objective is complete.
Your plan must be structured in sequential steps, with each step containing independent parallel subtasks.

Objective: ${variables.objective}

You have access to the following Agents with their tools:

Agents:
${variables.agents}

Generate a plan with all remaining steps needed.
Steps are sequential, but each Step can have parallel subtasks.
For each Step, specify an objective of the step and independent subtasks that can run in parallel.
For each subtask specify:
    1. Clear description of the task that an LLM can execute  
    2. Name of 1 Agent OR List of MCP server names to use for the task
    
Return your response in the following JSON structure:
    {{
        "steps": [
            {{
                "objective": "Objective for this step 1",
                "tasks": [
                    {{
                        "description": "Description of task 1",
                        "agent": "agent_name"  # For AgentTask
                    }},
                    {{
                        "description": "Description of task 2", 
                        "agent": "agent_name2"
                    }}
                ]
            }}
        ],
        "isComplete": false
    }}

You must respond with valid JSON only, with no triple backticks. No markdown formatting.
No extra text. Do not wrap in \`\`\`json code fences.`;

export const fullPlanSchemaReponseFormat = {
  type: 'json_schema',
  json_schema: {
    name: 'plan',
    strict: true,
    description: "A structured plan containing sequential steps with parallel subtasks",
    schema: {
      type: "object",
      strict: true,
      properties: {
        steps: {
          type: "array",
          description: "An ordered list of steps to be executed sequentially",
          items: {
            type: "object",
            description: "A single step containing a high-level description and parallel subtasks",
            properties: {
              objective: {
                type: "string",
                description: "A clear objective of what this step aims to achieve in the overall plan"
              },
              tasks: {
                type: "array",
                description: "List of independent tasks that can be executed in parallel within this step",
                items: {
                  type: "object",
                  description: "A specific task to be executed by an agent",
                  properties: {
                    description: {
                      type: "string",
                      description: "Detailed description of what the LLM/agent needs to do, should be clear and actionable"
                    },
                    agent: {
                      type: "string",
                      description: "Name of the agent that will execute this task. Must match one of the available agent names provided"
                    }
                  },
                  required: ["description", "agent"]
                }
              }
            },
            required: ["objective", "tasks"]
          }
        },
        isComplete: {
          type: "boolean",
          description: "Indicates whether the objective has been fully achieved (true) or if more steps are needed (false)"
        }
      },
      required: ["steps", "isComplete"]
    }
  }
};




================================================
FILE: src/workflows/orchestrator/types.ts
================================================

export enum PlanStatus {
  InProgress = "In Progress",
  Complete = "Complete",
}

export type PlanTask = {
  description: string;
  agent: string;
}

export type PlanStep = {
  objective: string;
  tasks: PlanTask[];
}

export type Plan = {
  steps: PlanStep[];
}

export type TaskResult = {
  task: PlanTask;
  result: string;
}

export type PlanTaskResult = PlanTask & {
  result: string;
}

export type PlanStepResult = PlanStep & {
  tasks: PlanTaskResult[];
  result: string;
}

export type PlanResult = Plan & {
  steps: PlanStepResult[];
  objective: string;
  result: string;
  status: PlanStatus;
}


================================================
FILE: src/workflows/router/prompt.ts
================================================
export const generateRoutingPrompt = (variables: { context: string, top_k: number, request: string }) => `
You are a highly accurate request router that directs incoming requests to the most appropriate category.
A category is a specialized tool with a specific set of capabilities and purpose.
Below are the available routing categories, each with their capabilities and descriptions:

${variables.context}

Your task is to analyze the following request and determine the most appropriate categories from the options above. Consider:
- The specific capabilities and tools each destination offers
- How well the request matches the category's description
- Whether the request might benefit from multiple categories (up to ${variables.top_k})

Request: ${variables.request}

Respond in JSON format:
{{
    "categories": [
        {{
            "category": <category name>,
            "confidence": <high, medium or low>,
            "reasoning": <brief explanation>
        }}
    ]
}}

Only include categories that are truly relevant. You may return fewer than ${variables.top_k} if appropriate.
If none of the categories are relevant, return an empty list.
`
