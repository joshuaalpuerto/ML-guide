{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM2KFrtF6fzLUsnqZE9D9LH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuaalpuerto/ML-guide/blob/main/RepoMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq grep_ast pygments tree_sitter_languages --progress-bar off\n",
        "!pip install -qqq spacy networkx matplotlib textacy --progress-bar off\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "ghzdjWoQ-pXn",
        "outputId": "7fbdc672-ab05-44d9-bd42-8f357af70166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-20 08:33:25.458201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-20 08:33:25.458277: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-20 08:33:25.458339: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-20 08:33:28.971776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QyOOl71M0iQh",
        "outputId": "06a628aa-d95f-4969-8616-69ed043b6387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using NetworkX\n",
        "\n",
        "**Ref**  \n",
        "- https://baotramduong.medium.com/how-to-convert-any-text-into-a-graph-of-concepts-5afe5d129aba\n",
        "\n",
        "**Conclusion**\n",
        "NOT WORKING"
      ],
      "metadata": {
        "id": "tVVuPtHq1Ca0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textacy import preprocessing\n",
        "\n",
        "preproc = preprocessing.make_pipeline(\n",
        "    preprocessing.remove.html_tags,\n",
        "    preprocessing.normalize.bullet_points,\n",
        "    preprocessing.normalize.hyphenated_words,\n",
        "    preprocessing.normalize.unicode,\n",
        "    preprocessing.normalize.whitespace,\n",
        "    preprocessing.remove.accents,\n",
        "    # preprocessing.remove.punctuation,\n",
        "    preprocessing.replace.currency_symbols,\n",
        "    preprocessing.replace.emails,\n",
        "    preprocessing.replace.emojis,\n",
        "    preprocessing.replace.hashtags,\n",
        "    preprocessing.replace.numbers,\n",
        "    preprocessing.replace.phone_numbers,\n",
        "    preprocessing.replace.urls,\n",
        "    preprocessing.replace.user_handles,\n",
        ")\n"
      ],
      "metadata": {
        "id": "W-Ad23hF3BhS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_json_as_pd(file_path, subset):\n",
        "    data = pd.read_json(file_path)\n",
        "    return data\n",
        "\n",
        "\n",
        "def read_data():\n",
        "  data = load_json_as_pd(file_path='...path', subset='contentPlainText')\n",
        "  data['cleaned'] = data['contentPlainText'].apply(lambda x: preproc(x))\n",
        "  data = data.dropna()\n",
        "  data = data.drop_duplicates(subset='contentPlainText')\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "messages_df = read_data()"
      ],
      "metadata": {
        "id": "3ciZCVTu04VD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Load SpaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return (' ').join(tokens)"
      ],
      "metadata": {
        "id": "CkW7Yqce0-S9",
        "outputId": "04580250-41d5-4d78-a2d6-a033ff8fe7df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entity Recognition\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Concept Extraction\n",
        "def extract_concepts(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    freq_dist = FreqDist(tokens)\n",
        "    concepts = freq_dist.most_common(10)  # Extract top 10 concepts\n",
        "    return [concept[0] for concept in concepts]\n",
        "\n",
        "# Define Dependency Parsing Function\n",
        "def build_concept_graph_with_dependencies(text):\n",
        "    doc = nlp(text)\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for token in doc:\n",
        "        G.add_node(token.text)\n",
        "\n",
        "        if token.dep_ != 'punct':\n",
        "            G.add_edge(token.text, token.head.text, relation=token.dep_)\n",
        "\n",
        "    return G\n",
        "\n",
        "# Define Graph Visualization Function\n",
        "def visualize_graph(graph):\n",
        "    pos = nx.spring_layout(graph)\n",
        "    nx.draw(graph, pos, with_labels=True, font_weight='bold', node_color='skyblue', edge_color='gray', font_size=8)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "15fN67Ta1hQu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages_df['concepts'] =  messages_df['contentPlainText'].apply(lambda x: extract_concepts(x))\n",
        "messages_df.head(100)"
      ],
      "metadata": {
        "id": "AXPj_Blj1zt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code map"
      ],
      "metadata": {
        "id": "7x9VNPHo1Hk9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtIafVZV-WaZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Code for Repo map\n",
        "import colorsys\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from collections import Counter, defaultdict, namedtuple\n",
        "from pathlib import Path\n",
        "\n",
        "import networkx as nx\n",
        "import pkg_resources\n",
        "from diskcache import Cache\n",
        "from grep_ast import TreeContext, filename_to_lang\n",
        "from pygments.lexers import guess_lexer_for_filename\n",
        "from pygments.token import Token\n",
        "from pygments.util import ClassNotFound\n",
        "from tqdm import tqdm\n",
        "from tree_sitter_languages import get_language, get_parser\n",
        "\n",
        "\n",
        "Tag = namedtuple(\"Tag\", \"rel_fname fname line name kind\".split())\n",
        "\n",
        "def read_text(self, filename):\n",
        "  try:\n",
        "      with open(str(filename), \"r\", encoding=self.encoding) as f:\n",
        "          return f.read()\n",
        "  except FileNotFoundError:\n",
        "      print(f\"{filename}: file not found error\")\n",
        "      return\n",
        "  except IsADirectoryError:\n",
        "      print(f\"{filename}: is a directory\")\n",
        "      return\n",
        "  except UnicodeError as e:\n",
        "      print(f\"{filename}: {e}\")\n",
        "      print(\"Use --encoding to set the unicode encoding.\")\n",
        "      return\n",
        "\n",
        "class RepoMap:\n",
        "    CACHE_VERSION = 3\n",
        "    TAGS_CACHE_DIR = f\".aider.tags.cache.v{CACHE_VERSION}\"\n",
        "\n",
        "    cache_missing = False\n",
        "\n",
        "    warned_files = set()\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        map_tokens=1024,\n",
        "        root=None,\n",
        "        # TODO: implement later\n",
        "        main_model=None, # change this later to actual LLM\n",
        "        repo_content_prefix=None,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if not root:\n",
        "            root = os.getcwd()\n",
        "        self.root = root\n",
        "\n",
        "        self.load_tags_cache()\n",
        "\n",
        "        self.max_map_tokens = map_tokens\n",
        "\n",
        "        # self.tokenizer = main_model.tokenizer\n",
        "        self.repo_content_prefix = repo_content_prefix\n",
        "\n",
        "    def get_repo_map(self, chat_files, other_files):\n",
        "        if self.max_map_tokens <= 0:\n",
        "            return\n",
        "\n",
        "        if not other_files:\n",
        "            return\n",
        "\n",
        "        files_listing = self.get_ranked_tags_map(chat_files, other_files)\n",
        "        if not files_listing:\n",
        "            return\n",
        "\n",
        "        # num_tokens = self.token_count(files_listing)\n",
        "        num_tokens=0\n",
        "        if self.verbose:\n",
        "            print(f\"Repo-map: {num_tokens/1024:.1f} k-tokens\")\n",
        "\n",
        "        if chat_files:\n",
        "            other = \"other \"\n",
        "        else:\n",
        "            other = \"\"\n",
        "\n",
        "        if self.repo_content_prefix:\n",
        "            repo_content = self.repo_content_prefix.format(other=other)\n",
        "        else:\n",
        "            repo_content = \"\"\n",
        "\n",
        "        repo_content += files_listing\n",
        "\n",
        "        return repo_content\n",
        "\n",
        "    def token_count(self, string):\n",
        "        #return len(self.tokenizer.encode(string))\n",
        "        return 0\n",
        "\n",
        "    def get_rel_fname(self, fname):\n",
        "        return os.path.relpath(fname, self.root)\n",
        "\n",
        "    def split_path(self, path):\n",
        "        path = os.path.relpath(path, self.root)\n",
        "        return [path + \":\"]\n",
        "\n",
        "    def load_tags_cache(self):\n",
        "        path = Path(self.root) / self.TAGS_CACHE_DIR\n",
        "        if not path.exists():\n",
        "            self.cache_missing = True\n",
        "        self.TAGS_CACHE = Cache(path)\n",
        "\n",
        "    def save_tags_cache(self):\n",
        "        pass\n",
        "\n",
        "    def get_mtime(self, fname):\n",
        "        try:\n",
        "            return os.path.getmtime(fname)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found error: {fname}\")\n",
        "\n",
        "    def get_tags(self, fname, rel_fname):\n",
        "        # Check if the file is in the cache and if the modification time has not changed\n",
        "        file_mtime = self.get_mtime(fname)\n",
        "        if file_mtime is None:\n",
        "            return []\n",
        "\n",
        "        cache_key = fname\n",
        "        if cache_key in self.TAGS_CACHE and self.TAGS_CACHE[cache_key][\"mtime\"] == file_mtime:\n",
        "            return self.TAGS_CACHE[cache_key][\"data\"]\n",
        "\n",
        "        # miss!\n",
        "\n",
        "        data = list(self.get_tags_raw(fname, rel_fname))\n",
        "\n",
        "        # Update the cache\n",
        "        self.TAGS_CACHE[cache_key] = {\"mtime\": file_mtime, \"data\": data}\n",
        "        self.save_tags_cache()\n",
        "        return data\n",
        "\n",
        "    def get_tags_raw(self, fname, rel_fname):\n",
        "        lang = filename_to_lang(fname)\n",
        "        if not lang:\n",
        "            return\n",
        "\n",
        "        language = get_language(lang)\n",
        "        parser = get_parser(lang)\n",
        "\n",
        "        # Load the tags queries\n",
        "        scm_fname = pkg_resources.resource_filename(\n",
        "            __name__, os.path.join(\"queries\", f\"tree-sitter-{lang}-tags.scm\")\n",
        "        )\n",
        "        query_scm = Path(scm_fname)\n",
        "        if not query_scm.exists():\n",
        "            return\n",
        "        query_scm = query_scm.read_text()\n",
        "\n",
        "        code = read_text(fname)\n",
        "        if not code:\n",
        "            return\n",
        "        tree = parser.parse(bytes(code, \"utf-8\"))\n",
        "\n",
        "        # Run the tags queries\n",
        "        query = language.query(query_scm)\n",
        "        captures = query.captures(tree.root_node)\n",
        "\n",
        "        captures = list(captures)\n",
        "\n",
        "        saw = set()\n",
        "        for node, tag in captures:\n",
        "            if tag.startswith(\"name.definition.\"):\n",
        "                kind = \"def\"\n",
        "            elif tag.startswith(\"name.reference.\"):\n",
        "                kind = \"ref\"\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            saw.add(kind)\n",
        "\n",
        "            result = Tag(\n",
        "                rel_fname=rel_fname,\n",
        "                fname=fname,\n",
        "                name=node.text.decode(\"utf-8\"),\n",
        "                kind=kind,\n",
        "                line=node.start_point[0],\n",
        "            )\n",
        "\n",
        "            yield result\n",
        "\n",
        "        if \"ref\" in saw:\n",
        "            return\n",
        "        if \"def\" not in saw:\n",
        "            return\n",
        "\n",
        "        # We saw defs, without any refs\n",
        "        # Some tags files only provide defs (cpp, for example)\n",
        "        # Use pygments to backfill refs\n",
        "\n",
        "        try:\n",
        "            lexer = guess_lexer_for_filename(fname, code)\n",
        "        except ClassNotFound:\n",
        "            return\n",
        "\n",
        "        tokens = list(lexer.get_tokens(code))\n",
        "        tokens = [token[1] for token in tokens if token[0] in Token.Name]\n",
        "\n",
        "        for token in tokens:\n",
        "            yield Tag(\n",
        "                rel_fname=rel_fname,\n",
        "                fname=fname,\n",
        "                name=token,\n",
        "                kind=\"ref\",\n",
        "                line=-1,\n",
        "            )\n",
        "\n",
        "    def get_ranked_tags(self, chat_fnames, other_fnames):\n",
        "        defines = defaultdict(set)\n",
        "        references = defaultdict(list)\n",
        "        definitions = defaultdict(set)\n",
        "\n",
        "        personalization = dict()\n",
        "\n",
        "        fnames = set(chat_fnames).union(set(other_fnames))\n",
        "        chat_rel_fnames = set()\n",
        "\n",
        "        fnames = sorted(fnames)\n",
        "\n",
        "        if self.cache_missing:\n",
        "            fnames = tqdm(fnames)\n",
        "        self.cache_missing = False\n",
        "\n",
        "        for fname in fnames:\n",
        "            if not Path(fname).is_file():\n",
        "                if fname not in self.warned_files:\n",
        "                    if Path(fname).exists():\n",
        "                        print(\n",
        "                            f\"Repo-map can't include {fname}, it is not a normal file\"\n",
        "                        )\n",
        "                    else:\n",
        "                        print(f\"Repo-map can't include {fname}, it no longer exists\")\n",
        "\n",
        "                self.warned_files.add(fname)\n",
        "                continue\n",
        "\n",
        "            # dump(fname)\n",
        "            rel_fname = os.path.relpath(fname, self.root)\n",
        "\n",
        "            if fname in chat_fnames:\n",
        "                personalization[rel_fname] = 1.0\n",
        "                chat_rel_fnames.add(rel_fname)\n",
        "\n",
        "            tags = list(self.get_tags(fname, rel_fname))\n",
        "            if tags is None:\n",
        "                continue\n",
        "\n",
        "            for tag in tags:\n",
        "                if tag.kind == \"def\":\n",
        "                    defines[tag.name].add(rel_fname)\n",
        "                    key = (rel_fname, tag.name)\n",
        "                    definitions[key].add(tag)\n",
        "\n",
        "                if tag.kind == \"ref\":\n",
        "                    references[tag.name].append(rel_fname)\n",
        "\n",
        "        ##\n",
        "        # dump(defines)\n",
        "        # dump(references)\n",
        "\n",
        "        if not references:\n",
        "            references = dict((k, list(v)) for k, v in defines.items())\n",
        "\n",
        "        idents = set(defines.keys()).intersection(set(references.keys()))\n",
        "\n",
        "        G = nx.MultiDiGraph()\n",
        "\n",
        "        for ident in idents:\n",
        "            definers = defines[ident]\n",
        "            for referencer, num_refs in Counter(references[ident]).items():\n",
        "                for definer in definers:\n",
        "                    # if referencer == definer:\n",
        "                    #    continue\n",
        "                    G.add_edge(referencer, definer, weight=num_refs, ident=ident)\n",
        "\n",
        "        if not references:\n",
        "            pass\n",
        "\n",
        "        if personalization:\n",
        "            pers_args = dict(personalization=personalization, dangling=personalization)\n",
        "        else:\n",
        "            pers_args = dict()\n",
        "\n",
        "        try:\n",
        "            ranked = nx.pagerank(G, weight=\"weight\", **pers_args)\n",
        "        except ZeroDivisionError:\n",
        "            return []\n",
        "\n",
        "        # distribute the rank from each source node, across all of its out edges\n",
        "        ranked_definitions = defaultdict(float)\n",
        "        for src in G.nodes:\n",
        "            src_rank = ranked[src]\n",
        "            total_weight = sum(data[\"weight\"] for _src, _dst, data in G.out_edges(src, data=True))\n",
        "            # dump(src, src_rank, total_weight)\n",
        "            for _src, dst, data in G.out_edges(src, data=True):\n",
        "                data[\"rank\"] = src_rank * data[\"weight\"] / total_weight\n",
        "                ident = data[\"ident\"]\n",
        "                ranked_definitions[(dst, ident)] += data[\"rank\"]\n",
        "\n",
        "        ranked_tags = []\n",
        "        ranked_definitions = sorted(ranked_definitions.items(), reverse=True, key=lambda x: x[1])\n",
        "\n",
        "        # dump(ranked_definitions)\n",
        "\n",
        "        for (fname, ident), rank in ranked_definitions:\n",
        "            # print(f\"{rank:.03f} {fname} {ident}\")\n",
        "            if fname in chat_rel_fnames:\n",
        "                continue\n",
        "            ranked_tags += list(definitions.get((fname, ident), []))\n",
        "\n",
        "        rel_other_fnames_without_tags = set(\n",
        "            os.path.relpath(fname, self.root) for fname in other_fnames\n",
        "        )\n",
        "\n",
        "        fnames_already_included = set(rt[0] for rt in ranked_tags)\n",
        "\n",
        "        top_rank = sorted([(rank, node) for (node, rank) in ranked.items()], reverse=True)\n",
        "        for rank, fname in top_rank:\n",
        "            if fname in rel_other_fnames_without_tags:\n",
        "                rel_other_fnames_without_tags.remove(fname)\n",
        "            if fname not in fnames_already_included:\n",
        "                ranked_tags.append((fname,))\n",
        "\n",
        "        for fname in rel_other_fnames_without_tags:\n",
        "            ranked_tags.append((fname,))\n",
        "\n",
        "        return ranked_tags\n",
        "\n",
        "    def get_ranked_tags_map(self, chat_fnames, other_fnames=None):\n",
        "        if not other_fnames:\n",
        "            other_fnames = list()\n",
        "\n",
        "        ranked_tags = self.get_ranked_tags(chat_fnames, other_fnames)\n",
        "        num_tags = len(ranked_tags)\n",
        "\n",
        "        lower_bound = 0\n",
        "        upper_bound = num_tags\n",
        "        best_tree = None\n",
        "\n",
        "        while lower_bound <= upper_bound:\n",
        "            middle = (lower_bound + upper_bound) // 2\n",
        "            tree = self.to_tree(ranked_tags[:middle])\n",
        "            num_tokens = self.token_count(tree)\n",
        "\n",
        "            if num_tokens < self.max_map_tokens:\n",
        "                best_tree = tree\n",
        "                lower_bound = middle + 1\n",
        "            else:\n",
        "                upper_bound = middle - 1\n",
        "\n",
        "        return best_tree\n",
        "\n",
        "    def to_tree(self, tags):\n",
        "        if not tags:\n",
        "            return \"\"\n",
        "\n",
        "        tags = sorted(tags)\n",
        "\n",
        "        cur_fname = None\n",
        "        context = None\n",
        "        output = \"\"\n",
        "\n",
        "        # add a bogus tag at the end so we trip the this_fname != cur_fname...\n",
        "        dummy_tag = (None,)\n",
        "        for tag in tags + [dummy_tag]:\n",
        "            this_fname = tag[0]\n",
        "\n",
        "            # ... here ... to output the final real entry in the list\n",
        "            if this_fname != cur_fname:\n",
        "                if context:\n",
        "                    context.add_context()\n",
        "                    output += \"\\n\"\n",
        "                    output += cur_fname + \":\\n\"\n",
        "                    output += context.format()\n",
        "                    context = None\n",
        "                elif cur_fname:\n",
        "                    output += \"\\n\" + cur_fname + \"\\n\"\n",
        "\n",
        "                if type(tag) is Tag:\n",
        "                    code = read_text(tag.fname) or \"\"\n",
        "\n",
        "                    context = TreeContext(\n",
        "                        tag.rel_fname,\n",
        "                        code,\n",
        "                        color=False,\n",
        "                        line_number=False,\n",
        "                        child_context=False,\n",
        "                        last_line=False,\n",
        "                        margin=0,\n",
        "                        mark_lois=False,\n",
        "                        loi_pad=0,\n",
        "                        # header_max=30,\n",
        "                        show_top_of_file_parent_scope=False,\n",
        "                    )\n",
        "                cur_fname = this_fname\n",
        "\n",
        "            if context:\n",
        "                context.add_lines_of_interest([tag.line])\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def find_src_files(directory):\n",
        "    if not os.path.isdir(directory):\n",
        "        return [directory]\n",
        "\n",
        "    src_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            src_files.append(os.path.join(root, file))\n",
        "    return src_files\n",
        "\n",
        "\n",
        "def get_random_color():\n",
        "    hue = random.random()\n",
        "    r, g, b = [int(x * 255) for x in colorsys.hsv_to_rgb(hue, 1, 0.75)]\n",
        "    res = f\"#{r:02x}{g:02x}{b:02x}\"\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/joshuaalpuerto/node-ddd-boilerplate.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnLCGXPmARXw",
        "outputId": "a02a9c86-cb7d-4e3c-8891-d62f7304dfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'node-ddd-boilerplate'...\n",
            "remote: Enumerating objects: 981, done.\u001b[K\n",
            "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 981 (delta 224), reused 210 (delta 196), pack-reused 715\u001b[K\n",
            "Receiving objects: 100% (981/981), 1.73 MiB | 15.27 MiB/s, done.\n",
            "Resolving deltas: 100% (505/505), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_fnames=find_src_files(\"src/\")"
      ],
      "metadata": {
        "id": "STocmBMoBpgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm = RepoMap(\n",
        "    root=\".\",\n",
        "    # verbose=True,\n",
        ")\n",
        "print(rm.get_ranked_tags_map(chat_fnames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrUK70CSA1Pq",
        "outputId": "ae2d7117-e935-4a20-a4d3-7aa19ae639ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repo-map can't include src, it no longer exists\n",
            "\n"
          ]
        }
      ]
    }
  ]
}