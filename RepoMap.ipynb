{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN628okPoIw7hNUYqnSHocR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuaalpuerto/ML-guide/blob/main/RepoMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq grep_ast pygments tree_sitter_languages --progress-bar off"
      ],
      "metadata": {
        "id": "ghzdjWoQ-pXn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UtIafVZV-WaZ"
      },
      "outputs": [],
      "source": [
        "import colorsys\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from collections import Counter, defaultdict, namedtuple\n",
        "from pathlib import Path\n",
        "\n",
        "import networkx as nx\n",
        "import pkg_resources\n",
        "from diskcache import Cache\n",
        "from grep_ast import TreeContext, filename_to_lang\n",
        "from pygments.lexers import guess_lexer_for_filename\n",
        "from pygments.token import Token\n",
        "from pygments.util import ClassNotFound\n",
        "from tqdm import tqdm\n",
        "from tree_sitter_languages import get_language, get_parser\n",
        "\n",
        "\n",
        "Tag = namedtuple(\"Tag\", \"rel_fname fname line name kind\".split())\n",
        "\n",
        "def read_text(self, filename):\n",
        "  try:\n",
        "      with open(str(filename), \"r\", encoding=self.encoding) as f:\n",
        "          return f.read()\n",
        "  except FileNotFoundError:\n",
        "      print(f\"{filename}: file not found error\")\n",
        "      return\n",
        "  except IsADirectoryError:\n",
        "      print(f\"{filename}: is a directory\")\n",
        "      return\n",
        "  except UnicodeError as e:\n",
        "      print(f\"{filename}: {e}\")\n",
        "      print(\"Use --encoding to set the unicode encoding.\")\n",
        "      return\n",
        "\n",
        "class RepoMap:\n",
        "    CACHE_VERSION = 3\n",
        "    TAGS_CACHE_DIR = f\".aider.tags.cache.v{CACHE_VERSION}\"\n",
        "\n",
        "    cache_missing = False\n",
        "\n",
        "    warned_files = set()\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        map_tokens=1024,\n",
        "        root=None,\n",
        "        # TODO: implement later\n",
        "        main_model=None, # change this later to actual LLM\n",
        "        repo_content_prefix=None,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if not root:\n",
        "            root = os.getcwd()\n",
        "        self.root = root\n",
        "\n",
        "        self.load_tags_cache()\n",
        "\n",
        "        self.max_map_tokens = map_tokens\n",
        "\n",
        "        # self.tokenizer = main_model.tokenizer\n",
        "        self.repo_content_prefix = repo_content_prefix\n",
        "\n",
        "    def get_repo_map(self, chat_files, other_files):\n",
        "        if self.max_map_tokens <= 0:\n",
        "            return\n",
        "\n",
        "        if not other_files:\n",
        "            return\n",
        "\n",
        "        files_listing = self.get_ranked_tags_map(chat_files, other_files)\n",
        "        if not files_listing:\n",
        "            return\n",
        "\n",
        "        # num_tokens = self.token_count(files_listing)\n",
        "        num_tokens=0\n",
        "        if self.verbose:\n",
        "            print(f\"Repo-map: {num_tokens/1024:.1f} k-tokens\")\n",
        "\n",
        "        if chat_files:\n",
        "            other = \"other \"\n",
        "        else:\n",
        "            other = \"\"\n",
        "\n",
        "        if self.repo_content_prefix:\n",
        "            repo_content = self.repo_content_prefix.format(other=other)\n",
        "        else:\n",
        "            repo_content = \"\"\n",
        "\n",
        "        repo_content += files_listing\n",
        "\n",
        "        return repo_content\n",
        "\n",
        "    def token_count(self, string):\n",
        "        #return len(self.tokenizer.encode(string))\n",
        "        return 0\n",
        "\n",
        "    def get_rel_fname(self, fname):\n",
        "        return os.path.relpath(fname, self.root)\n",
        "\n",
        "    def split_path(self, path):\n",
        "        path = os.path.relpath(path, self.root)\n",
        "        return [path + \":\"]\n",
        "\n",
        "    def load_tags_cache(self):\n",
        "        path = Path(self.root) / self.TAGS_CACHE_DIR\n",
        "        if not path.exists():\n",
        "            self.cache_missing = True\n",
        "        self.TAGS_CACHE = Cache(path)\n",
        "\n",
        "    def save_tags_cache(self):\n",
        "        pass\n",
        "\n",
        "    def get_mtime(self, fname):\n",
        "        try:\n",
        "            return os.path.getmtime(fname)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found error: {fname}\")\n",
        "\n",
        "    def get_tags(self, fname, rel_fname):\n",
        "        # Check if the file is in the cache and if the modification time has not changed\n",
        "        file_mtime = self.get_mtime(fname)\n",
        "        if file_mtime is None:\n",
        "            return []\n",
        "\n",
        "        cache_key = fname\n",
        "        if cache_key in self.TAGS_CACHE and self.TAGS_CACHE[cache_key][\"mtime\"] == file_mtime:\n",
        "            return self.TAGS_CACHE[cache_key][\"data\"]\n",
        "\n",
        "        # miss!\n",
        "\n",
        "        data = list(self.get_tags_raw(fname, rel_fname))\n",
        "\n",
        "        # Update the cache\n",
        "        self.TAGS_CACHE[cache_key] = {\"mtime\": file_mtime, \"data\": data}\n",
        "        self.save_tags_cache()\n",
        "        return data\n",
        "\n",
        "    def get_tags_raw(self, fname, rel_fname):\n",
        "        lang = filename_to_lang(fname)\n",
        "        if not lang:\n",
        "            return\n",
        "\n",
        "        language = get_language(lang)\n",
        "        parser = get_parser(lang)\n",
        "\n",
        "        # Load the tags queries\n",
        "        scm_fname = pkg_resources.resource_filename(\n",
        "            __name__, os.path.join(\"queries\", f\"tree-sitter-{lang}-tags.scm\")\n",
        "        )\n",
        "        query_scm = Path(scm_fname)\n",
        "        if not query_scm.exists():\n",
        "            return\n",
        "        query_scm = query_scm.read_text()\n",
        "\n",
        "        code = read_text(fname)\n",
        "        if not code:\n",
        "            return\n",
        "        tree = parser.parse(bytes(code, \"utf-8\"))\n",
        "\n",
        "        # Run the tags queries\n",
        "        query = language.query(query_scm)\n",
        "        captures = query.captures(tree.root_node)\n",
        "\n",
        "        captures = list(captures)\n",
        "\n",
        "        saw = set()\n",
        "        for node, tag in captures:\n",
        "            if tag.startswith(\"name.definition.\"):\n",
        "                kind = \"def\"\n",
        "            elif tag.startswith(\"name.reference.\"):\n",
        "                kind = \"ref\"\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            saw.add(kind)\n",
        "\n",
        "            result = Tag(\n",
        "                rel_fname=rel_fname,\n",
        "                fname=fname,\n",
        "                name=node.text.decode(\"utf-8\"),\n",
        "                kind=kind,\n",
        "                line=node.start_point[0],\n",
        "            )\n",
        "\n",
        "            yield result\n",
        "\n",
        "        if \"ref\" in saw:\n",
        "            return\n",
        "        if \"def\" not in saw:\n",
        "            return\n",
        "\n",
        "        # We saw defs, without any refs\n",
        "        # Some tags files only provide defs (cpp, for example)\n",
        "        # Use pygments to backfill refs\n",
        "\n",
        "        try:\n",
        "            lexer = guess_lexer_for_filename(fname, code)\n",
        "        except ClassNotFound:\n",
        "            return\n",
        "\n",
        "        tokens = list(lexer.get_tokens(code))\n",
        "        tokens = [token[1] for token in tokens if token[0] in Token.Name]\n",
        "\n",
        "        for token in tokens:\n",
        "            yield Tag(\n",
        "                rel_fname=rel_fname,\n",
        "                fname=fname,\n",
        "                name=token,\n",
        "                kind=\"ref\",\n",
        "                line=-1,\n",
        "            )\n",
        "\n",
        "    def get_ranked_tags(self, chat_fnames, other_fnames):\n",
        "        defines = defaultdict(set)\n",
        "        references = defaultdict(list)\n",
        "        definitions = defaultdict(set)\n",
        "\n",
        "        personalization = dict()\n",
        "\n",
        "        fnames = set(chat_fnames).union(set(other_fnames))\n",
        "        chat_rel_fnames = set()\n",
        "\n",
        "        fnames = sorted(fnames)\n",
        "\n",
        "        if self.cache_missing:\n",
        "            fnames = tqdm(fnames)\n",
        "        self.cache_missing = False\n",
        "\n",
        "        for fname in fnames:\n",
        "            if not Path(fname).is_file():\n",
        "                if fname not in self.warned_files:\n",
        "                    if Path(fname).exists():\n",
        "                        print(\n",
        "                            f\"Repo-map can't include {fname}, it is not a normal file\"\n",
        "                        )\n",
        "                    else:\n",
        "                        print(f\"Repo-map can't include {fname}, it no longer exists\")\n",
        "\n",
        "                self.warned_files.add(fname)\n",
        "                continue\n",
        "\n",
        "            # dump(fname)\n",
        "            rel_fname = os.path.relpath(fname, self.root)\n",
        "\n",
        "            if fname in chat_fnames:\n",
        "                personalization[rel_fname] = 1.0\n",
        "                chat_rel_fnames.add(rel_fname)\n",
        "\n",
        "            tags = list(self.get_tags(fname, rel_fname))\n",
        "            if tags is None:\n",
        "                continue\n",
        "\n",
        "            for tag in tags:\n",
        "                if tag.kind == \"def\":\n",
        "                    defines[tag.name].add(rel_fname)\n",
        "                    key = (rel_fname, tag.name)\n",
        "                    definitions[key].add(tag)\n",
        "\n",
        "                if tag.kind == \"ref\":\n",
        "                    references[tag.name].append(rel_fname)\n",
        "\n",
        "        ##\n",
        "        # dump(defines)\n",
        "        # dump(references)\n",
        "\n",
        "        if not references:\n",
        "            references = dict((k, list(v)) for k, v in defines.items())\n",
        "\n",
        "        idents = set(defines.keys()).intersection(set(references.keys()))\n",
        "\n",
        "        G = nx.MultiDiGraph()\n",
        "\n",
        "        for ident in idents:\n",
        "            definers = defines[ident]\n",
        "            for referencer, num_refs in Counter(references[ident]).items():\n",
        "                for definer in definers:\n",
        "                    # if referencer == definer:\n",
        "                    #    continue\n",
        "                    G.add_edge(referencer, definer, weight=num_refs, ident=ident)\n",
        "\n",
        "        if not references:\n",
        "            pass\n",
        "\n",
        "        if personalization:\n",
        "            pers_args = dict(personalization=personalization, dangling=personalization)\n",
        "        else:\n",
        "            pers_args = dict()\n",
        "\n",
        "        try:\n",
        "            ranked = nx.pagerank(G, weight=\"weight\", **pers_args)\n",
        "        except ZeroDivisionError:\n",
        "            return []\n",
        "\n",
        "        # distribute the rank from each source node, across all of its out edges\n",
        "        ranked_definitions = defaultdict(float)\n",
        "        for src in G.nodes:\n",
        "            src_rank = ranked[src]\n",
        "            total_weight = sum(data[\"weight\"] for _src, _dst, data in G.out_edges(src, data=True))\n",
        "            # dump(src, src_rank, total_weight)\n",
        "            for _src, dst, data in G.out_edges(src, data=True):\n",
        "                data[\"rank\"] = src_rank * data[\"weight\"] / total_weight\n",
        "                ident = data[\"ident\"]\n",
        "                ranked_definitions[(dst, ident)] += data[\"rank\"]\n",
        "\n",
        "        ranked_tags = []\n",
        "        ranked_definitions = sorted(ranked_definitions.items(), reverse=True, key=lambda x: x[1])\n",
        "\n",
        "        # dump(ranked_definitions)\n",
        "\n",
        "        for (fname, ident), rank in ranked_definitions:\n",
        "            # print(f\"{rank:.03f} {fname} {ident}\")\n",
        "            if fname in chat_rel_fnames:\n",
        "                continue\n",
        "            ranked_tags += list(definitions.get((fname, ident), []))\n",
        "\n",
        "        rel_other_fnames_without_tags = set(\n",
        "            os.path.relpath(fname, self.root) for fname in other_fnames\n",
        "        )\n",
        "\n",
        "        fnames_already_included = set(rt[0] for rt in ranked_tags)\n",
        "\n",
        "        top_rank = sorted([(rank, node) for (node, rank) in ranked.items()], reverse=True)\n",
        "        for rank, fname in top_rank:\n",
        "            if fname in rel_other_fnames_without_tags:\n",
        "                rel_other_fnames_without_tags.remove(fname)\n",
        "            if fname not in fnames_already_included:\n",
        "                ranked_tags.append((fname,))\n",
        "\n",
        "        for fname in rel_other_fnames_without_tags:\n",
        "            ranked_tags.append((fname,))\n",
        "\n",
        "        return ranked_tags\n",
        "\n",
        "    def get_ranked_tags_map(self, chat_fnames, other_fnames=None):\n",
        "        if not other_fnames:\n",
        "            other_fnames = list()\n",
        "\n",
        "        ranked_tags = self.get_ranked_tags(chat_fnames, other_fnames)\n",
        "        num_tags = len(ranked_tags)\n",
        "\n",
        "        lower_bound = 0\n",
        "        upper_bound = num_tags\n",
        "        best_tree = None\n",
        "\n",
        "        while lower_bound <= upper_bound:\n",
        "            middle = (lower_bound + upper_bound) // 2\n",
        "            tree = self.to_tree(ranked_tags[:middle])\n",
        "            num_tokens = self.token_count(tree)\n",
        "\n",
        "            if num_tokens < self.max_map_tokens:\n",
        "                best_tree = tree\n",
        "                lower_bound = middle + 1\n",
        "            else:\n",
        "                upper_bound = middle - 1\n",
        "\n",
        "        return best_tree\n",
        "\n",
        "    def to_tree(self, tags):\n",
        "        if not tags:\n",
        "            return \"\"\n",
        "\n",
        "        tags = sorted(tags)\n",
        "\n",
        "        cur_fname = None\n",
        "        context = None\n",
        "        output = \"\"\n",
        "\n",
        "        # add a bogus tag at the end so we trip the this_fname != cur_fname...\n",
        "        dummy_tag = (None,)\n",
        "        for tag in tags + [dummy_tag]:\n",
        "            this_fname = tag[0]\n",
        "\n",
        "            # ... here ... to output the final real entry in the list\n",
        "            if this_fname != cur_fname:\n",
        "                if context:\n",
        "                    context.add_context()\n",
        "                    output += \"\\n\"\n",
        "                    output += cur_fname + \":\\n\"\n",
        "                    output += context.format()\n",
        "                    context = None\n",
        "                elif cur_fname:\n",
        "                    output += \"\\n\" + cur_fname + \"\\n\"\n",
        "\n",
        "                if type(tag) is Tag:\n",
        "                    code = read_text(tag.fname) or \"\"\n",
        "\n",
        "                    context = TreeContext(\n",
        "                        tag.rel_fname,\n",
        "                        code,\n",
        "                        color=False,\n",
        "                        line_number=False,\n",
        "                        child_context=False,\n",
        "                        last_line=False,\n",
        "                        margin=0,\n",
        "                        mark_lois=False,\n",
        "                        loi_pad=0,\n",
        "                        # header_max=30,\n",
        "                        show_top_of_file_parent_scope=False,\n",
        "                    )\n",
        "                cur_fname = this_fname\n",
        "\n",
        "            if context:\n",
        "                context.add_lines_of_interest([tag.line])\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def find_src_files(directory):\n",
        "    if not os.path.isdir(directory):\n",
        "        return [directory]\n",
        "\n",
        "    src_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            src_files.append(os.path.join(root, file))\n",
        "    return src_files\n",
        "\n",
        "\n",
        "def get_random_color():\n",
        "    hue = random.random()\n",
        "    r, g, b = [int(x * 255) for x in colorsys.hsv_to_rgb(hue, 1, 0.75)]\n",
        "    res = f\"#{r:02x}{g:02x}{b:02x}\"\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/joshuaalpuerto/node-ddd-boilerplate.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnLCGXPmARXw",
        "outputId": "a02a9c86-cb7d-4e3c-8891-d62f7304dfde"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'node-ddd-boilerplate'...\n",
            "remote: Enumerating objects: 981, done.\u001b[K\n",
            "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 981 (delta 224), reused 210 (delta 196), pack-reused 715\u001b[K\n",
            "Receiving objects: 100% (981/981), 1.73 MiB | 15.27 MiB/s, done.\n",
            "Resolving deltas: 100% (505/505), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_fnames=find_src_files(\"src/\")"
      ],
      "metadata": {
        "id": "STocmBMoBpgB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm = RepoMap(\n",
        "    root=\".\",\n",
        "    # verbose=True,\n",
        ")\n",
        "print(rm.get_ranked_tags_map(chat_fnames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrUK70CSA1Pq",
        "outputId": "ae2d7117-e935-4a20-a4d3-7aa19ae639ba"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repo-map can't include src, it no longer exists\n",
            "\n"
          ]
        }
      ]
    }
  ]
}