{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGo+zF3Tgul5r30gAaPGt1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "526ae97b53d54d87b54a3b9eb9cf5306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_379399a0bf7d4e29b2ac05dc9995026d",
              "IPY_MODEL_5d96e232942048efbc50a9e673f7cb07",
              "IPY_MODEL_6f0a00f4c8b24fb1a0de5bd13519fb7f"
            ],
            "layout": "IPY_MODEL_d34b2cec3948404fba843d0898c8946f"
          }
        },
        "379399a0bf7d4e29b2ac05dc9995026d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13f72c8a1b824887addae19b20031f2c",
            "placeholder": "​",
            "style": "IPY_MODEL_4315618097984b238ac2b6e6bc70c6cc",
            "value": "Epoch 29: 100%"
          }
        },
        "5d96e232942048efbc50a9e673f7cb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bda3046a1b1f4ec9b259edf2996f2b13",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afb7b150ca2444278ffdf8585dcbf49f",
            "value": 2
          }
        },
        "6f0a00f4c8b24fb1a0de5bd13519fb7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d65936cce04980a526901ad7adcaf5",
            "placeholder": "​",
            "style": "IPY_MODEL_ae8406dbb9a346c5af8ed5a503f16d6c",
            "value": " 2/2 [00:00&lt;00:00, 42.09it/s, v_num=2]"
          }
        },
        "d34b2cec3948404fba843d0898c8946f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "13f72c8a1b824887addae19b20031f2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4315618097984b238ac2b6e6bc70c6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bda3046a1b1f4ec9b259edf2996f2b13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afb7b150ca2444278ffdf8585dcbf49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81d65936cce04980a526901ad7adcaf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae8406dbb9a346c5af8ed5a503f16d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuaalpuerto/ML-guide/blob/main/Transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yi2ecr2NnAmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning -q --progress-bar off"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zaBqZhgfom-d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why?\n",
        "\n",
        "- I'm curious of how transformer architecture look like under the hood.\n",
        "- Improve my understanding why and how things work.(help me debug it and other models)\n",
        "   - Interpret it's result not just simply saying because its a \"blackbox\"\n",
        "- Improve myself as Machine learning engineer.\n",
        "\n",
        "Refrences\n",
        "- https://www.youtube.com/watch?v=C9QSpl5nmrY&t=631s"
      ],
      "metadata": {
        "id": "Ox4TV2vxnBff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "\n",
        "Our goal is to have our decoder model output something base on our prompt.\n",
        "\n",
        "If we have prompts: `what is life` and `life is what` it should output `awesome`\n",
        "\n",
        "- Having our vocabulary setup\n",
        "- Creating our training dataset"
      ],
      "metadata": {
        "id": "V524zSxzG163"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create a simple vocabulary\n",
        "token_to_id = {\n",
        "    \"what\": 0,\n",
        "    \"is\": 1,\n",
        "    \"life\": 2,\n",
        "    \"<EOS>\": 3,\n",
        "    \"awesome\": 4,\n",
        "}\n",
        "\n",
        "id_to_token = dict(map(reversed, token_to_id.items()))\n",
        "\n",
        "# to visualize the reasoning behing inputs and outputs/labels check this visualization\n",
        "# https://colab.research.google.com/github/StatQuest/decoder_transformer_from_scratch/blob/main/decoder_transformers_with_pytorch_and_lightning_v2.ipynb#scrollTo=752cda95-7e7e-430a-8c1a-4afc9670379b\n",
        "\n",
        "## NOTE: Because we are using a Decoder-Only Transformer, the inputs contain\n",
        "##       the questions (\"what is life?\" and \"life is what?\") followed\n",
        "##       by an <EOS> token followed by the response, \"awesome\".\n",
        "##       This is because all of those tokens will be used as inputs to the Decoder-Only\n",
        "##       Transformer during Training.\n",
        "## ALSO NOTE: When we train this way, it's called \"teacher forcing\".\n",
        "##       Teacher forcing helps us train the neural network faster.\n",
        "inputs = torch.tensor([[token_to_id[\"what\"], ## input #1: what is life <EOS> awesome\n",
        "                        token_to_id[\"is\"],\n",
        "                        token_to_id[\"life\"],\n",
        "                        token_to_id[\"<EOS>\"],\n",
        "                        token_to_id[\"awesome\"]],\n",
        "\n",
        "                       [token_to_id[\"life\"], # input #2: life is what <EOS> awesome\n",
        "                        token_to_id[\"is\"],\n",
        "                        token_to_id[\"what\"],\n",
        "                        token_to_id[\"<EOS>\"],\n",
        "                        token_to_id[\"awesome\"]]])\n",
        "\n",
        "## NOTE: Because we are using a Decoder-Only Transformer the outputs, or\n",
        "##       the predictions, are the input questions (minus the first word) followed by\n",
        "##       <EOS> awesome <EOS>.  The first <EOS> means we're done processing the input question\n",
        "##       and the second <EOS> means we are done generating the output.\n",
        "##       See the illustration above for more details.\n",
        "labels = torch.tensor([[token_to_id[\"is\"],\n",
        "                        token_to_id[\"life\"],\n",
        "                        token_to_id[\"<EOS>\"],\n",
        "                        token_to_id[\"awesome\"],\n",
        "                        token_to_id[\"<EOS>\"]],\n",
        "\n",
        "                       [token_to_id[\"is\"],\n",
        "                        token_to_id[\"what\"],\n",
        "                        token_to_id[\"<EOS>\"],\n",
        "                        token_to_id[\"awesome\"],\n",
        "                        token_to_id[\"<EOS>\"]]])\n",
        "\n",
        "dataset = TensorDataset(inputs, labels)\n",
        "dataloader = DataLoader(dataset)"
      ],
      "metadata": {
        "id": "bIdlqGkPG3ZR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position encoding\n",
        "\n",
        "In transformer models, **positional embedding** is a technique used to inject information about the order of tokens in a sequence since transformers, unlike recurrent models, process inputs in parallel without inherent sequential ordering. To capture this positional information, sine and cosine functions are often employed.\n",
        "\n",
        "The idea is that `words` could repeat in different parts of the sentence, but we should only adjust them a bit base on their position (otherwise adjusting them with big numbers because of their `position` will overpower   their semantic meaning) - [Youtube](https://https://www.youtube.com/watch?v=1biZfFLPRSY&t=243s)"
      ],
      "metadata": {
        "id": "NHww7y-EVkWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PositionEncodingWithLooping(nn.Module):\n",
        "  def __init__(self, d_model=2, max_len=6):\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      positional_embedding = torch.zeros(max_len, d_model)\n",
        "      ## loop through each tensor to create a sine/cosine pair\n",
        "      ## tensor([[0., 0.],\n",
        "      ##  [0., 0.],\n",
        "      ##  [0., 0.],\n",
        "      ##  [0., 0.],\n",
        "      ##  [0., 0.],\n",
        "      ##  [0., 0.]])\n",
        "      for position, token in enumerate(positional_embedding):\n",
        "        for d_index in range(0, d_model - 1):\n",
        "            index_by_two = 2 * d_index\n",
        "            formula = position / (torch.tensor(10000.0) ** (index_by_two / d_model))\n",
        "\n",
        "            # sine will be for even numbers\n",
        "            positional_embedding[position][index_by_two] = torch.sin(formula)\n",
        "            # cosine will be for odd numbers\n",
        "            positional_embedding[position][index_by_two + 1] = torch.cos(formula)\n",
        "\n",
        "      self.pe = positional_embedding\n",
        "\n",
        "  def forward(self, word_embeddings):\n",
        "    # Get only the same size of word_embeddings from `pe`\n",
        "    return word_embeddings + self.pe[:word_embeddings.size(0), :]"
      ],
      "metadata": {
        "id": "QvN1ZE-XVjsL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Optimized version of the loop above PositionEncoding\n",
        "\n",
        "class PositionEncoding(nn.Module):\n",
        "    def __init__(self, d_model=2, max_len=6):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        ## Now we create a lookup table, pe, of position encoding values and initialize all of them to 0.\n",
        "        ## To do this, we will make a matrix of 0s that has max_len rows and d_model columns.\n",
        "        ## for example...\n",
        "        ## torch.zeros(3, 2)\n",
        "        ## ...returns a matrix of 0s with 3 rows and 2 columns...\n",
        "        ## tensor([[0., 0.],\n",
        "        ##         [0., 0.],\n",
        "        ##         [0., 0.]])\n",
        "        ## We do this to create a placeholder, so computation below will be much simpler.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        ## Now we create a sequence of numbers for each position that a token can have in the input (or output).\n",
        "        ## For example, if the input tokens where \"I'm happy today!\", then \"I'm\" would get the first\n",
        "        ## position, 0, \"happy\" would get the second position, 1, and \"today!\" would get the third position, 2.\n",
        "        ## NOTE: Since we are going to be doing math with these position indices to create the\n",
        "        ## positional encoding for each one, we need them to be floats rather than ints.\n",
        "        ##\n",
        "        ## Lastly, .unsqueeze(1) converts the single list of numbers that torch.arange creates into a matrix with\n",
        "        ## one row for each index, and all of the indices in a single column. So if \"max_len\" = 3, then we\n",
        "        ## would create a matrix with 3 rows and 1 column like this...\n",
        "        ##\n",
        "        ## torch.arange(start=0, end=3, step=1, dtype=torch.float).unsqueeze(1)\n",
        "        ##\n",
        "        ## ...returns...\n",
        "        ##\n",
        "        ## tensor([[0.],\n",
        "        ##         [1.],\n",
        "        ##         [2.]])\n",
        "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
        "\n",
        "\n",
        "        ## Here is where we start doing the math to determine the y-axis coordinates on the\n",
        "        ## sine and cosine curves.\n",
        "        ##\n",
        "        ## The positional encoding equations used in \"Attention is all you need\" are...\n",
        "        ##\n",
        "        ## PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
        "        ## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "        ##\n",
        "        ## ...and we see, within the sin() and cos() functions, we divide \"pos\" by some number that depends\n",
        "        ## on the index (i) and total number of PE values we want per token (d_model).\n",
        "        ##\n",
        "        ## NOTE: When the index, i, is 0 then we are calculating the y-axis coordinates on the **first pair**\n",
        "        ##       of sine and cosine curves. When i=1, then we are calculating the y-axis coordiantes on the\n",
        "        ##       **second pair** of sine and cosine curves. etc. etc.\n",
        "        ##\n",
        "        ## Now, pretty much everyone calculates the term we use to divide \"pos\" by first, and they do it with\n",
        "        ## code that looks like this...\n",
        "        ##\n",
        "        ## div_term = torch.exp(torch.arange(start=0, end=d_model, step=2).float() * -(math.log(10000.0) / d_model))\n",
        "        ##\n",
        "        ## Now, at least to me, it's not obvious that div_term = 1/(10000^(2i/d_model)) for a few reasons:\n",
        "        ##\n",
        "        ##    1) div_term wraps everything in a call to torch.exp()\n",
        "        ##    2) It uses log()\n",
        "        ##    2) The order of the terms is different\n",
        "        ##\n",
        "        ## The reason for these differences is, presumably, trying to prevent underflow (getting too close to 0).\n",
        "        ## So, to show that div_term = 1/(10000^(2i/d_model))...\n",
        "        ##\n",
        "        ## 1) Swap out math.log() for torch.log() (doing this requires converting 10000.0 to a tensor, which is my\n",
        "        ##    guess for why they used math.log() instead of torch.log())...\n",
        "        ##\n",
        "        ## torch.exp(torch.arange(start=0, end=d_model, step=2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        ##\n",
        "        ## 2) Rearrange the terms...\n",
        "        ##\n",
        "        ## torch.exp(-1 * (torch.log(torch.tensor(10000.0)) * torch.arange(start=0, end=d_model, step=2).float() / d_model))\n",
        "        ##\n",
        "        ## 3) Pull out the -1 with exp(-1 * x) = 1/exp(x)\n",
        "        ##\n",
        "        ## 1/torch.exp(torch.log(torch.tensor(10000.0)) * torch.arange(start=0, end=d_model, step=2).float() / d_model)\n",
        "        ##\n",
        "        ## 4) Use exp(a * b) = exp(a)^b to pull out the 2i/d_model term...\n",
        "        ##\n",
        "        ## 1/torch.exp(torch.log(torch.tensor(10000.0)))^(torch.arange(start=0, end=d_model, step=2).float() / d_model)\n",
        "        ##\n",
        "        ## 5) Use exp(log(x)) = x to get the original form of the denominator...\n",
        "        ##\n",
        "        ## 1/(torch.tensor(10000.0)^(torch.arange(start=0, end=d_model, step=2).float() / d_model))\n",
        "        ##\n",
        "        ## 6) Bam.\n",
        "        ##\n",
        "        ## So, that being said, I don't think underflow is actually that big an issue. In fact, some coder at Hugging Face\n",
        "        ## also doesn't think so, and their code for positional encoding in DistilBERT (a streamlined version of BERT, which\n",
        "        ## is a transformer model)\n",
        "        ## calculates the values directly - using the form of the equation found in original Attention is all you need\n",
        "        ## manuscript. See...\n",
        "        ## https://github.com/huggingface/transformers/blob/455c6390938a5c737fa63e78396cedae41e4e87e/src/transformers/modeling_distilbert.py#L53\n",
        "        ## So I think we can simplify the code, but I'm also writing all these comments to show that it is equivalent to what\n",
        "        ## you'll see in the wild...\n",
        "        ##\n",
        "        ## Now let's create an index for the embedding positions to simplify the code a little more...\n",
        "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
        "        ## NOTE: Setting step=2 results in the same sequence numbers that we would get if we multiplied i by 2.\n",
        "        ##       So we can save ourselves a little math by just setting step=2.\n",
        "\n",
        "        ## And now, finally, let's create div_term...\n",
        "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
        "\n",
        "        ## Now we calculate the actual positional encoding values. Remember 'pe' was initialized as a matrix of 0s\n",
        "        ## with max_len (max number of input tokens) rows and d_model (number of embedding values per token) columns.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) ## every other column, starting with the 1st, has sin() values\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) ## every other column, starting with the 2nd, has cos() values\n",
        "        ## NOTE: If the notation for indexing 'pe[]' looks cryptic to you, read on...\n",
        "        ##\n",
        "        ## First, let's look at the general indexing notation:\n",
        "        ##\n",
        "        ## For each row or column in matrix we can select elements in that\n",
        "        ## row or column with the following indexs...\n",
        "        ##\n",
        "        ## i:j:k = select elements between i and j with stepsize = k.\n",
        "        ##\n",
        "        ## ...where...\n",
        "        ##\n",
        "        ## i defaults to 0\n",
        "        ## j defaults to the number of elements in the row, column or whatever.\n",
        "        ## k defaults to 1\n",
        "        ##\n",
        "        ## Now that we have looked at the general notation, let's look at specific\n",
        "        ## examples so that we can understand it.\n",
        "        ##\n",
        "        ## We'll start with: pe[:, 0::2]\n",
        "        ##\n",
        "        ## The stuff that comes before the comma (in this case ':') refers to the rows we want to select.\n",
        "        ## The ':' before the comma means \"select all rows\" because we are not providing specific\n",
        "        ## values for i, j and k and, instead, just using the default values.\n",
        "        ##\n",
        "        ## The stuff after the comma refers to the columns we want to select.\n",
        "        ## In this case, we have '0::2', and that means we start with\n",
        "        ## the first column (column =  0) and go to the end (using the default value for j)\n",
        "        ## and we set the stepsize to 2, which means we skip every other column.\n",
        "        ##\n",
        "        ## Now to understand pe[:, 1::2]\n",
        "        ##\n",
        "        ## Again, the stuff before the comma refers to the rows, and, just like before\n",
        "        ## we use default values for i,j and k, so we select all rows.\n",
        "        ##\n",
        "        ## The stuff that comes after the comma refers to the columns.\n",
        "        ## In this case, we start with the 2nd column (column = 1), and go to the end\n",
        "        ## (using the default value for 'j') and we set the stepsize to 2, which\n",
        "        ## means we skip every other column.\n",
        "        ##\n",
        "        ## NOTE: using this ':' based notation is called \"indexing\" and also called \"slicing\"\n",
        "\n",
        "        ## Now we \"register 'pe'.\n",
        "        self.register_buffer('pe', pe) ## \"register_buffer()\" ensures that\n",
        "                                       ## 'pe' will be moved to wherever the model gets\n",
        "                                       ## moved to. So if the model is moved to a GPU, then,\n",
        "                                       ## even though we don't need to optimize 'pe', it will\n",
        "                                       ## also be moved to that GPU. This, in turn, means\n",
        "                                       ## that accessing 'pe' will be relatively fast copared\n",
        "                                       ## to having a GPU have to get the data from a CPU.\n",
        "\n",
        "    ## Because this class, PositionEncoding, inherits from nn.Module, the forward() method\n",
        "    ## is called by default when we use a PositionEncoding() object.\n",
        "    ## In other words, after we create a PositionEncoding() object, pe = PositionEncoding(),\n",
        "    ## then pe(word_embeddings) will call forward() and so this is where\n",
        "    ## we will add the position encoding values to the word embedding values\n",
        "    def forward(self, word_embeddings):\n",
        "\n",
        "        return word_embeddings + self.pe[:word_embeddings.size(0), :] ## word_embeddings.size(0) = number of embeddings\n",
        "                                                                      ## NOTE: That second ':' is optional and\n",
        "                                                                      ## we could re-write it like this:\n",
        "                                                                      ## self.pe[:word_embeddings.size(0)]"
      ],
      "metadata": {
        "id": "cS_Ck982rdeV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "BjKA7tGnu833"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## NOTE: our code only wors with one input(not batch)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_model=2):\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      # Initialize the weights for the Query, Key and Value\n",
        "      # This is a trainable parameters that our model will learn.\n",
        "      self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "      self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "      self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "      self.d_model=d_model\n",
        "\n",
        "  ## NOTE: the goal here is to adjust meaning of the token based on how the sentence is contruct.\n",
        "  ## So given it's positional embedding, we further adjust the embedding base on the tokens around it.\n",
        "  def forward(self, position_embeddings, mask = None):\n",
        "      # Check if input is a single sequence (2D)(single input) or a batch of sequences (3D)\n",
        "      is_batch = position_embeddings.dim() > 2\n",
        "\n",
        "      if not is_batch:\n",
        "          # Add batch dimension if not present\n",
        "          position_embeddings = position_embeddings.unsqueeze(0)  # (1, seq_len, dim)\n",
        "\n",
        "      q = self.W_q(position_embeddings)\n",
        "      k = self.W_k(position_embeddings)\n",
        "      v = self.W_v(position_embeddings)\n",
        "\n",
        "      ## We cannot use k.T because this will not work when we have batches (batch, token_len, embeddings)\n",
        "      ## k = [batch[token1[-0.2033,  0.4328, -0.5559], ..], ...] (2, 4, 3)\n",
        "      ## k.T = [batch[token1[-0.334,  0.991], ..], ...] (3, 4, 2) (incorrect)\n",
        "      ## We wil use `transpose` to modify particular dimension\n",
        "      ## Note: We do transpose K because we need to compute the relationship of each token against each other.\n",
        "      ## So each token should have a dimension length equals to token length\n",
        "      ##\n",
        "      ## q = (2, 4, 3) (batch, token_len, embeddings)\n",
        "      ## k.T = (2, 3, 4) (batch, embeddings, token_len) we transpose the inner layer\n",
        "      ##\n",
        "      ## Q (token_len, dim_len) = [[-0.2033,  0.4], [-0.4144,  0.0525], [0.1195, -0.2213], [-0.6304, -1.4384]]\n",
        "      ## K.T (dim_len, token_len) = [[-0.5559, -0.8518,  0.3156, -0.7806], [-0.5472, -0.5425,  0.298, 0.2231]]\n",
        "      ##\n",
        "      ## Why?\n",
        "      ## To calculate similarity scores/dot_product of each token features against each other tokens features\n",
        "      ## Similar tokens in a concept space would have bigger attention scores.\n",
        "      ## Q1 embeddings [-0.2033,  0.4] x K1 embeddings [-0.5559, -0.5472]\n",
        "      ## Q2 embeddings [-0.4144,  0.0525] x K2 embeddings [-0.8518, -0.5425]\n",
        "      ## Q3 embeddings [0.1195, -0.2213] x K3 embeddings [0.3156, 0.298]\n",
        "      ## Q4 embeddings [-0.6304, -1.4384] x K4 embeddings [-0.7806, 0.2231]\n",
        "      ## You will notice that this will produce a square (batch, 4, 4)\n",
        "      ## Where each token has score to each token\n",
        "      ## [[[0.8860, 0.5832, 0.3376, 0.8090],\n",
        "      ##  [0.5779, 0.9040, 0.5547, 0.3423],\n",
        "      ##  [0.6343, 0.3644, 0.7104, 0.9464],\n",
        "      ##  [0.7890, 0.2814, 0.7886, 0.5895]], ...]\n",
        "      similarity = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "      # in the formula it should be square_root or power of 0.5 (or half of it.)\n",
        "      scaled_similarity = similarity / torch.tensor(self.d_model**0.5)\n",
        "\n",
        "      ## We mask because this Attention class could also be reuse for encoder/encoder-decoder architecture also.\n",
        "      ## For this intance the mask value for this is decoder model:\n",
        "      ## tensor([[False, True, True, True],\n",
        "      ##   [False, False, True, True],\n",
        "      ##   [False, False, False, True],\n",
        "      ##   [False, False, False, False]])\n",
        "      ## Because we want to set the `True` => -infnity (for masking)\n",
        "      if mask is not None:\n",
        "        # Mask so tokens can only attend to the tokens before it.\n",
        "        # I've also seen -1e20 and -9e15 used in masking, meaning -infinity\n",
        "        scaled_similarity = scaled_similarity.masked_fill(mask=mask, value=-1e9)\n",
        "\n",
        "      ## Apply softmax to determine what percent of each token's value to\n",
        "      ## use in the final attention values.\n",
        "      ## Scaled_sims structure and each token corresponds to  scores to another token:\n",
        "      ## [Token1[<token1_score>, <token2_score>, ..],\n",
        "      ##  Token2[<token1_score>, <token2_score>, ..],\n",
        "      ##  ...]\n",
        "      ## [[-0.0875, -0.0436,  0.0461,  0.1805], <-- apply softmax on each token\n",
        "      ##  [ 0.8043,  0.9314, -0.4446,  0.1211]]\n",
        "      ## and we use dim=-1 because we want to apply softmax on each token.\n",
        "      ## We will use this to visualize how the scores of each tokens change w.r.t to other token.\n",
        "      attention_percents = F.softmax(scaled_similarity, dim=-1)\n",
        "\n",
        "\n",
        "      ## Scale the values by their associated percentages and add them up.\n",
        "      ## NOTE that we don't need transpose here as attention_percents has the same shape as `v`\n",
        "      ## This is because of k.transpose(-2, -1) above\n",
        "      ## Now this will be the new embeddings w.r.t to other token (new semantic meaning based on attention)\n",
        "      ## What we are saying is what is the value we should add to each token to adjust it's meaning.\n",
        "      attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "      return attention_scores if is_batch else attention_scores[0], attention_percents"
      ],
      "metadata": {
        "id": "ZRKUxdmDu8Ym"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "jNoJAo11YolH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning as L\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "class DecoderOnlyTransformer(L.LightningModule):\n",
        "\n",
        "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ## We are set the seed so that you can get the same results as me.\n",
        "        L.seed_everything(seed=42)\n",
        "\n",
        "\n",
        "        # NOTE: that creating token ids/vocab is handled outside of transformer (such as word2vec, glove, BPE etc).\n",
        "        # nn.Embeddings will initiate some random weights(similar to nn.Linear) w.r.t to token_id  and will be adjusted accordingly (training)\n",
        "        # Also nn.Embeddings mapped the token_ids(ids are usually integer (0 - N) or total number of tokens) to weights.\n",
        "        # That's why you will use the same model tokenizer everytime othewise the mapping will be different.\n",
        "        # Then the mapped weights will be adjusted during training (adjust representation).\n",
        "        # num_embeddings x embedding_dim (4x2 = [[1,1], [2,2], .., ..])\n",
        "        # Each token with 2 vector\n",
        "        # Note that we are only processing single input here (not batch)\n",
        "        # [[ 0.3367,  0.1288],\n",
        "        # [ 0.2345,  0.2303],\n",
        "        # [-1.1229, -0.1863],\n",
        "        # [ 2.2082, -0.6380]]\n",
        "        self.we = nn.Embedding(num_embeddings=num_tokens,\n",
        "                               embedding_dim=d_model)\n",
        "\n",
        "        self.pe = PositionEncodingWithLooping(d_model=d_model,\n",
        "                                   max_len=max_len)\n",
        "\n",
        "\n",
        "        self.self_attention = Attention(d_model=d_model)\n",
        "\n",
        "\n",
        "        ## NOTE: In this simple example, we are not doing multi-head attention\n",
        "        ## If we wanted to do multi-head attention, we could\n",
        "        ## initailize more Attention objects like this...\n",
        "        ##\n",
        "        ## self.self_attention_2 = Attention(d_model=d_model)\n",
        "        ## self.self_attention_3 = Attention(d_model=d_model)\n",
        "        ##\n",
        "        ## If d_model=2, then using 3 self_attention objects would\n",
        "        ## result in d_model*3 = 6 self-attention values per token,\n",
        "        ## so we would need to initialize\n",
        "        ## a fully connected layer to reduce the dimension of the\n",
        "        ## self attention values back down to d_model like this:\n",
        "        ##\n",
        "        ## self.reduce_attention_dim = nn.Linear(in_features=(num_attention_heads*d_model), out_features=d_model)\n",
        "\n",
        "        ## Note: that we only have one layer which is also our output layer.\n",
        "        ## Since we need to identify the output for each input/token.\n",
        "        ## We need each token to be a one-hot encoding of all tokens available to our vocabulary\n",
        "        ## [T1 [ 0.1416<T1>,  1.0208<T2>,  0.7418<T3>, -0.4908<T4>,  0.1990<T5>],\n",
        "        ##  T2 [ 0.6305<T1>,  0.8153<T2>,  0.8407<T3>, -0.4216<T4>,  0.3690<T5>]\n",
        "        ##  ..]\n",
        "        ## Then we do argmax for each token to see what should be the output for each of the token.\n",
        "        ## In our case we only care about the latest prediction (the last token),\n",
        "        ## What will be the output of the last token?\n",
        "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        word_embeddings = self.we(token_ids)\n",
        "        position_encoded = self.pe(word_embeddings)\n",
        "\n",
        "        ## For the decoder-only transformer, we need to use \"masked self-attention\" so that\n",
        "        ## when we are training we can't cheat and look ahead at\n",
        "        ## what words come after the current word.\n",
        "        ## To create the mask we are creating a matrix where the lower triangle\n",
        "        ## is filled with 0, and everything above the diagonal is filled with 0s.\n",
        "        ## tensor([[1., 0., 0., 0.],\n",
        "        ##         [1., 1., 0., 0.],\n",
        "        ##         [1., 1., 1., 0.],\n",
        "        ##         [1., 1., 1., 1.]])\n",
        "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device))\n",
        "\n",
        "        ## We then replace the 0s above the digaonal, which represent the words\n",
        "        ## we want to be masked out, with \"True\", and replace the 1s in the lower\n",
        "        ## triangle, which represent the words we want to include when we calcualte\n",
        "        ## self-attention for a specific word in the output, with \"False\".\n",
        "        ## tensor([[False, True, True, True],\n",
        "        ##         [False, False, True, True],\n",
        "        ##         [False, False, False, True],\n",
        "        ##         [False, False, False, False]])\n",
        "        mask = mask == 0\n",
        "\n",
        "        self_attention_values, attention_percents = self.self_attention(position_encoded,\n",
        "                                                    mask=mask)\n",
        "\n",
        "        print('self_attention_percents', attention_percents)\n",
        "\n",
        "        ## Why do we need residual values?\n",
        "        ## Check this youtube - https://www.youtube.com/watch?v=Q1JCrG1bJ-A.\n",
        "        ## where it explained the importance of having residual connections.\n",
        "        ## Also we have notes here - https://www.notion.so/ML-Engineer-paths-14f4f26dcdd24f63850a782c1cf367b9?pvs=4#1265a706faf280bb996fea8bd438514e\n",
        "        ## The idea here is that tensors that goes to the layers just adds/augment to the original features of the input(barely modifying it).\n",
        "        residual_connection_values = position_encoded + self_attention_values\n",
        "\n",
        "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
        "\n",
        "        return fc_layer_output\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        ## configure_optimizers() simply passes the parameters we want to\n",
        "        ## optimize to the optimzes and sets the learning rate\n",
        "        return Adam(self.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        ## training_step() is called by Lightning trainer when\n",
        "        ## we want to train the model.\n",
        "        input_tokens, labels = batch # collect input\n",
        "        output = self.forward(input_tokens[0])\n",
        "        loss = self.loss(output, labels[0])\n",
        "        print('loss', loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "## First, create a model from DecoderOnlyTransformer()\n",
        "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)"
      ],
      "metadata": {
        "id": "tyPiAEcWoPTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58a5263-b19c-4ccc-d16c-e8f4a112467a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 42\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model_input):\n",
        "  input_length = model_input.size(dim=0)\n",
        "\n",
        "  ## Now get get predictions from the model\n",
        "  predictions = model(model_input)\n",
        "\n",
        "  ## NOTE: \"predictions\" is the output from the fully connected layer,\n",
        "  ##      not a softmax() function. We could, if we wanted to,\n",
        "  ##      Run \"predictions\" through a softmax() function, but\n",
        "  ##      since we're going to select the item with the largest value\n",
        "  ##      we can just use argmax instead...\n",
        "  ## ALSO NOTE: \"predictions\" is a matrix, with one row of predicted values\n",
        "  ##      per input token. Since we only want the prediction from the\n",
        "  ##      last row (the most recent prediction) we use reverse index for the\n",
        "  ##      row, -1.\n",
        "  predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "  print('predictions', predictions)\n",
        "  print('predicted_id', predicted_id)\n",
        "\n",
        "  ## We'll store predicted_id in an array, predicted_ids, that\n",
        "  ## we'll add to each time we predict a new output token.\n",
        "  predicted_ids = predicted_id\n",
        "\n",
        "  ## Now use a loop to predict output tokens until we get an\n",
        "  ## <EOS> token.\n",
        "  max_length = 6\n",
        "  for i in range(input_length, max_length):\n",
        "      if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
        "          break\n",
        "\n",
        "      model_input = torch.cat((model_input, predicted_id))\n",
        "\n",
        "      predicted_id = model(model_input)\n",
        "      predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "      predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "  ## Now printout the predicted output phrase.\n",
        "  print(\"Predicted Tokens:\\n\")\n",
        "  for id in predicted_ids:\n",
        "      print(\"\\t\", id_to_token[id.item()])"
      ],
      "metadata": {
        "id": "LXglGXgfDpdg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Without training\n",
        "## Now create the input for the transformer...\n",
        "model_input = torch.tensor([token_to_id[\"what\"],\n",
        "                            token_to_id[\"is\"],\n",
        "                            token_to_id[\"life\"],\n",
        "                            token_to_id[\"<EOS>\"]])\n",
        "predict(model_input)"
      ],
      "metadata": {
        "id": "AtjJmRujDz3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6202a6d6-a5df-4f74-b908-b3b9f5cfb382"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9996e-01, 3.9247e-05, 0.0000e+00, 0.0000e+00],\n",
            "         [7.7210e-05, 9.9970e-01, 2.2281e-04, 0.0000e+00],\n",
            "         [9.9870e-01, 5.3399e-10, 1.3003e-03, 6.8641e-39]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "predictions tensor([[ -1.5503,   8.6861,   0.8554,   1.4063, -10.6343],\n",
            "        [  7.8719,   4.8554,  10.2138, -11.1490,  -8.9896],\n",
            "        [ -4.1358,  -3.6899,  -6.4874,   8.0023,   1.9360],\n",
            "        [  3.6551, -13.2122,  -1.0089,  -0.8882,   9.4588]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "predicted_id tensor([4])\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9996e-01, 3.9247e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.7210e-05, 9.9970e-01, 2.2281e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9870e-01, 5.3399e-10, 1.3003e-03, 6.8641e-39, 0.0000e+00],\n",
            "         [1.9477e-06, 1.0000e+00, 1.4286e-07, 9.0703e-28, 1.4146e-16]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "          0.0000e+00],\n",
            "         [9.9996e-01, 3.9247e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "          0.0000e+00],\n",
            "         [7.7210e-05, 9.9970e-01, 2.2281e-04, 0.0000e+00, 0.0000e+00,\n",
            "          0.0000e+00],\n",
            "         [9.9870e-01, 5.3399e-10, 1.3003e-03, 6.8641e-39, 0.0000e+00,\n",
            "          0.0000e+00],\n",
            "         [1.9477e-06, 1.0000e+00, 1.4286e-07, 9.0703e-28, 1.4146e-16,\n",
            "          0.0000e+00],\n",
            "         [5.0680e-07, 1.0000e+00, 1.7878e-07, 3.0254e-18, 4.5228e-14,\n",
            "          1.7551e-11]]], grad_fn=<SoftmaxBackward0>)\n",
            "Predicted Tokens:\n",
            "\n",
            "\t awesome\n",
            "\t awesome\n",
            "\t awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = L.Trainer(max_epochs=30)\n",
        "trainer.fit(model, train_dataloaders=dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "526ae97b53d54d87b54a3b9eb9cf5306",
            "379399a0bf7d4e29b2ac05dc9995026d",
            "5d96e232942048efbc50a9e673f7cb07",
            "6f0a00f4c8b24fb1a0de5bd13519fb7f",
            "d34b2cec3948404fba843d0898c8946f",
            "13f72c8a1b824887addae19b20031f2c",
            "4315618097984b238ac2b6e6bc70c6cc",
            "bda3046a1b1f4ec9b259edf2996f2b13",
            "afb7b150ca2444278ffdf8585dcbf49f",
            "81d65936cce04980a526901ad7adcaf5",
            "ae8406dbb9a346c5af8ed5a503f16d6c"
          ]
        },
        "id": "7ZIMZDYRT88z",
        "outputId": "11bd0745-70d2-4082-ab39-ddc99d61e41a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO: \n",
            "  | Name           | Type                        | Params | Mode \n",
            "-----------------------------------------------------------------------\n",
            "0 | we             | Embedding                   | 10     | train\n",
            "1 | pe             | PositionEncodingWithLooping | 0      | train\n",
            "2 | self_attention | Attention                   | 12     | train\n",
            "3 | fc_layer       | Linear                      | 15     | train\n",
            "4 | loss           | CrossEntropyLoss            | 0      | train\n",
            "-----------------------------------------------------------------------\n",
            "37        Trainable params\n",
            "0         Non-trainable params\n",
            "37        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "8         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name           | Type                        | Params | Mode \n",
            "-----------------------------------------------------------------------\n",
            "0 | we             | Embedding                   | 10     | train\n",
            "1 | pe             | PositionEncodingWithLooping | 0      | train\n",
            "2 | self_attention | Attention                   | 12     | train\n",
            "3 | fc_layer       | Linear                      | 15     | train\n",
            "4 | loss           | CrossEntropyLoss            | 0      | train\n",
            "-----------------------------------------------------------------------\n",
            "37        Trainable params\n",
            "0         Non-trainable params\n",
            "37        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "8         Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "526ae97b53d54d87b54a3b9eb9cf5306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self_attention_percents tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4783, 0.5217, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3427, 0.3340, 0.3232, 0.0000, 0.0000],\n",
            "         [0.3416, 0.3879, 0.0980, 0.1725, 0.0000],\n",
            "         [0.2021, 0.1967, 0.2064, 0.1879, 0.2070]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(1.5605, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4346, 0.5654, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2982, 0.3977, 0.3040, 0.0000, 0.0000],\n",
            "         [0.2604, 0.4474, 0.2099, 0.0822, 0.0000],\n",
            "         [0.2163, 0.1940, 0.1940, 0.1860, 0.2097]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(1.4294, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4817, 0.5183, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3422, 0.3217, 0.3360, 0.0000, 0.0000],\n",
            "         [0.5072, 0.3747, 0.0906, 0.0275, 0.0000],\n",
            "         [0.2017, 0.1919, 0.2117, 0.1819, 0.2127]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(1.1268, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4278, 0.5722, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3636, 0.3688, 0.2676, 0.0000, 0.0000],\n",
            "         [0.5159, 0.3681, 0.1049, 0.0111, 0.0000],\n",
            "         [0.2428, 0.2010, 0.1901, 0.1563, 0.2098]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(1.1016, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4433, 0.5567, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3644, 0.3204, 0.3152, 0.0000, 0.0000],\n",
            "         [0.6400, 0.2792, 0.0778, 0.0030, 0.0000],\n",
            "         [0.2570, 0.2164, 0.2064, 0.1171, 0.2031]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.8029, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.7581e-01, 6.2419e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.1577e-01, 3.2596e-01, 2.5827e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [8.3123e-01, 1.4022e-01, 2.8099e-02, 4.5513e-04, 0.0000e+00],\n",
            "         [3.4914e-01, 2.3352e-01, 1.6856e-01, 7.0723e-02, 1.7805e-01]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.8191, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.7917e-01, 6.2083e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.9926e-01, 3.3277e-01, 2.6796e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [8.0698e-01, 1.3088e-01, 6.2044e-02, 9.7907e-05, 0.0000e+00],\n",
            "         [3.8525e-01, 2.5477e-01, 1.7744e-01, 3.4752e-02, 1.4779e-01]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.5388, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.8008e-01, 7.1992e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.7264e-01, 2.8332e-01, 2.4404e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [9.7909e-01, 1.7823e-02, 3.0868e-03, 2.9573e-06, 0.0000e+00],\n",
            "         [5.5031e-01, 2.3678e-01, 1.0523e-01, 1.1586e-02, 9.6096e-02]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.6120, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.9708e-01, 7.0292e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.3132e-01, 3.5786e-01, 2.1082e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [9.2250e-01, 3.1589e-02, 4.5908e-02, 8.6446e-07, 0.0000e+00],\n",
            "         [5.5512e-01, 2.5058e-01, 1.2236e-01, 3.5852e-03, 6.8363e-02]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.3584, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.5969e-01, 8.4031e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [5.5254e-01, 2.3896e-01, 2.0850e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9900e-01, 8.0994e-04, 1.8607e-04, 4.1126e-09, 0.0000e+00],\n",
            "         [7.8018e-01, 1.5458e-01, 3.7178e-02, 5.4380e-04, 2.7525e-02]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.4725, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.1093e-01, 7.8907e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.4732e-01, 3.9633e-01, 1.5635e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [9.6130e-01, 3.6751e-03, 3.5028e-02, 2.5858e-09, 0.0000e+00],\n",
            "         [7.1578e-01, 1.9054e-01, 7.2100e-02, 1.3995e-04, 2.1442e-02]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.2465, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.2997e-02, 9.2700e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [6.5488e-01, 1.9025e-01, 1.5486e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9998e-01, 1.6105e-05, 8.5089e-06, 1.6953e-12, 0.0000e+00],\n",
            "         [9.2116e-01, 6.6186e-02, 8.1221e-03, 9.0846e-06, 4.5213e-03]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.3799, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.5604e-01, 8.4396e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.2088e-01, 4.7415e-01, 1.0497e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [9.7432e-01, 2.5171e-04, 2.5426e-02, 3.1007e-12, 0.0000e+00],\n",
            "         [8.2353e-01, 1.3461e-01, 3.7025e-02, 3.0920e-06, 4.8300e-03]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1891, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.4539e-02, 9.5546e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.3531e-01, 1.5887e-01, 1.0581e-01, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 2.7158e-07, 4.4458e-07, 4.0743e-16, 0.0000e+00],\n",
            "         [9.6288e-01, 3.4814e-02, 1.7366e-03, 1.3040e-07, 5.7142e-04]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.3174, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.7857e-01, 8.2143e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.0649e-01, 6.3837e-01, 5.5139e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [9.8428e-01, 1.6485e-05, 1.5705e-02, 2.6232e-15, 0.0000e+00],\n",
            "         [8.4242e-01, 1.4065e-01, 1.6135e-02, 8.4547e-08, 7.9942e-04]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1612, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [6.9529e-02, 9.3047e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.4736e-01, 1.7642e-01, 7.6217e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 7.6512e-09, 3.7759e-08, 1.3494e-19, 0.0000e+00],\n",
            "         [9.5197e-01, 4.7384e-02, 5.7439e-04, 4.7991e-09, 7.6405e-05]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.2618, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.1736e-01, 5.8264e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.2754e-01, 8.5539e-01, 1.7071e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9216e-01, 1.3907e-06, 7.8411e-03, 2.5767e-18, 0.0000e+00],\n",
            "         [6.8135e-01, 3.1321e-01, 5.3457e-03, 6.0474e-09, 9.3406e-05]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1517, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.2721e-01, 6.7279e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [6.4026e-01, 2.9885e-01, 6.0888e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 4.3586e-10, 5.8785e-09, 9.2899e-23, 0.0000e+00],\n",
            "         [7.8944e-01, 2.1023e-01, 3.1333e-04, 8.0732e-10, 1.1604e-05]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.2042, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.5459e-01, 1.4541e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.9512e-02, 9.6734e-01, 3.1450e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9687e-01, 1.3210e-07, 3.1308e-03, 3.3008e-21, 0.0000e+00],\n",
            "         [2.5263e-01, 7.4653e-01, 8.3274e-04, 6.7005e-10, 5.1282e-06]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1630, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.6225e-01, 1.3775e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.8826e-01, 5.6912e-01, 4.2622e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 3.8805e-11, 1.5858e-09, 1.3480e-25, 0.0000e+00],\n",
            "         [2.7149e-01, 7.2841e-01, 9.8233e-05, 1.5240e-10, 9.2355e-07]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1766, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.9949e-01, 1.0051e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.6102e-02, 9.8229e-01, 1.6074e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9889e-01, 1.8184e-08, 1.1067e-03, 1.2462e-23, 0.0000e+00],\n",
            "         [1.8759e-01, 8.1209e-01, 3.2033e-04, 2.9428e-11, 7.7468e-07]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1758, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.8916e-01, 5.1084e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.7660e-01, 5.9219e-01, 3.1207e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 9.6280e-12, 9.5816e-10, 1.5277e-27, 0.0000e+00],\n",
            "         [6.5232e-01, 3.4763e-01, 4.8801e-05, 2.3024e-12, 3.2576e-07]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1348, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [6.1089e-01, 3.8911e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.6118e-02, 9.7129e-01, 2.5898e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9960e-01, 3.6333e-09, 3.9823e-04, 1.4639e-25, 0.0000e+00],\n",
            "         [5.1307e-01, 4.8644e-01, 4.9225e-04, 5.8525e-13, 5.3860e-07]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1822, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [5.7020e-02, 9.4298e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.4388e-01, 5.3017e-01, 2.5943e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 2.9575e-12, 7.3725e-10, 3.4457e-29, 0.0000e+00],\n",
            "         [9.5309e-01, 4.6891e-02, 1.4328e-05, 1.0859e-14, 7.5357e-08]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1272, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.4773e-01, 7.5227e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.8157e-02, 9.5827e-01, 3.5718e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9987e-01, 7.3072e-10, 1.3047e-04, 2.2204e-27, 0.0000e+00],\n",
            "         [8.0009e-01, 1.9952e-01, 3.8922e-04, 1.1615e-14, 2.1661e-07]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1850, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.6915e-02, 9.6308e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.4973e-01, 6.3021e-01, 2.0061e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.0594e-12, 6.8772e-10, 8.6590e-31, 0.0000e+00],\n",
            "         [9.6448e-01, 3.5515e-02, 8.0458e-06, 5.7522e-16, 2.0868e-08]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1224, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.1238e-01, 6.8762e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.4659e-02, 9.7319e-01, 2.1504e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9996e-01, 1.5990e-10, 3.7881e-05, 3.3331e-29, 0.0000e+00],\n",
            "         [7.3261e-01, 2.6721e-01, 1.7698e-04, 1.4180e-15, 5.2781e-08]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1781, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.5905e-01, 8.4095e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.6006e-01, 8.2887e-01, 1.1075e-02, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 4.7038e-13, 6.9028e-10, 2.5573e-32, 0.0000e+00],\n",
            "         [8.4955e-01, 1.5044e-01, 7.6487e-06, 2.8989e-16, 7.4010e-09]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1220, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.2612e-01, 2.7388e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.9510e-03, 9.9029e-01, 7.5694e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9999e-01, 4.3651e-11, 1.1413e-05, 6.5763e-31, 0.0000e+00],\n",
            "         [3.3658e-01, 6.6337e-01, 4.5805e-05, 5.0807e-16, 7.4012e-09]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1589, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.4311e-01, 2.5689e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.5860e-02, 9.5016e-01, 3.9848e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 2.5377e-13, 6.9585e-10, 1.1005e-33, 0.0000e+00],\n",
            "         [3.0003e-01, 6.9996e-01, 3.6952e-06, 2.3053e-16, 1.2872e-09]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1463, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.9837e-01, 1.0163e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.2837e-03, 9.9534e-01, 3.7484e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.5539e-11, 4.4318e-06, 2.5375e-32, 0.0000e+00],\n",
            "         [1.3277e-01, 8.6722e-01, 1.2089e-05, 1.1326e-16, 1.1095e-09]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1366, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.4386e-01, 2.5614e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.0931e-02, 9.6659e-01, 2.4805e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.5043e-13, 6.6524e-10, 9.5063e-35, 0.0000e+00],\n",
            "         [2.6377e-01, 7.3623e-01, 2.2129e-06, 4.0081e-17, 4.7724e-10]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1625, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.1279e-01, 2.8721e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [5.7368e-03, 9.9371e-01, 5.4973e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 7.5382e-12, 2.5752e-06, 2.4591e-33, 0.0000e+00],\n",
            "         [2.8616e-01, 7.1382e-01, 1.9492e-05, 1.5464e-17, 1.1674e-09]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1276, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.4652e-01, 7.5348e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.8575e-02, 9.4871e-01, 2.7179e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 9.3373e-14, 5.2546e-10, 1.4450e-35, 0.0000e+00],\n",
            "         [6.7188e-01, 3.2812e-01, 2.2008e-06, 3.0071e-18, 4.4602e-10]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1493, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.9150e-01, 7.0850e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.9636e-03, 9.9002e-01, 1.0144e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 5.1719e-12, 2.4346e-06, 4.8745e-34, 0.0000e+00],\n",
            "         [6.2879e-01, 3.7117e-01, 3.8808e-05, 1.3104e-18, 1.4115e-09]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1344, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.5190e-02, 9.6481e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.8469e-02, 9.0864e-01, 2.8924e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 5.4080e-14, 2.8404e-10, 2.5608e-36, 0.0000e+00],\n",
            "         [9.2177e-01, 7.8224e-02, 9.3677e-07, 1.1862e-19, 1.9491e-10]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1454, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.2403e-01, 6.7597e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [6.4758e-03, 9.9259e-01, 9.3789e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 5.4386e-12, 3.2000e-06, 1.3942e-34, 0.0000e+00],\n",
            "         [5.6175e-01, 4.3820e-01, 4.2105e-05, 3.0410e-19, 7.6993e-10]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1286, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0362e-01, 8.9638e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [5.9778e-02, 9.3896e-01, 1.2587e-03, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 3.7274e-14, 1.1990e-10, 4.2578e-37, 0.0000e+00],\n",
            "         [7.7532e-01, 2.2468e-01, 4.0586e-07, 7.7862e-20, 6.3409e-11]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1500, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.6806e-01, 1.3194e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.8968e-03, 9.9773e-01, 3.7765e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9999e-01, 8.1892e-12, 5.6893e-06, 5.0703e-35, 0.0000e+00],\n",
            "         [1.0047e-01, 8.9952e-01, 1.2669e-05, 1.4646e-19, 9.3108e-11]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0996, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.8213e-01, 6.1787e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.4979e-02, 9.6461e-01, 4.1109e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 2.2757e-14, 3.8497e-11, 7.1321e-38, 0.0000e+00],\n",
            "         [3.8806e-01, 6.1194e-01, 9.0538e-08, 4.4463e-20, 1.0694e-11]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1716, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.9275e-01, 1.0725e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.3050e-03, 9.9833e-01, 3.6222e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9998e-01, 1.4414e-11, 1.7454e-05, 3.4962e-35, 0.0000e+00],\n",
            "         [7.4142e-02, 9.2584e-01, 1.7931e-05, 2.2532e-20, 4.7582e-11]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0924, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.1930e-02, 9.8807e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.6105e-01, 8.3845e-01, 5.0534e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 9.7264e-15, 7.5354e-12, 1.6583e-38, 0.0000e+00],\n",
            "         [9.4190e-01, 5.8096e-02, 2.0642e-08, 4.4246e-22, 3.9644e-12]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1427, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.5406e-01, 2.4594e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.4308e-03, 9.9803e-01, 5.4118e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9993e-01, 2.9989e-11, 7.0021e-05, 3.6740e-35, 0.0000e+00],\n",
            "         [1.4384e-01, 8.5608e-01, 7.5747e-05, 2.3213e-21, 6.7771e-11]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0941, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [5.7809e-03, 9.9422e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.5068e-01, 7.4911e-01, 2.1235e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 5.7511e-15, 1.4818e-12, 3.9011e-39, 0.0000e+00],\n",
            "         [9.5792e-01, 4.2081e-02, 3.0857e-09, 3.2560e-23, 7.0971e-13]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1406, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.8581e-01, 1.4191e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [4.5178e-04, 9.9932e-01, 2.2836e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9972e-01, 1.0559e-10, 2.7876e-04, 3.8800e-35, 0.0000e+00],\n",
            "         [9.0557e-03, 9.9093e-01, 1.3133e-05, 2.5161e-22, 3.0383e-12]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0675, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.3709e-02, 9.6629e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.7395e-01, 8.2601e-01, 4.4298e-05, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 5.1128e-15, 3.4528e-13, 1.0010e-39, 0.0000e+00],\n",
            "         [7.5566e-01, 2.4434e-01, 5.9219e-10, 1.9976e-23, 1.2549e-13]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1349, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9948e-01, 5.1510e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.5330e-04, 9.9975e-01, 9.7768e-05, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9920e-01, 3.1475e-10, 8.0391e-04, 3.5373e-35, 0.0000e+00],\n",
            "         [4.1712e-04, 9.9958e-01, 1.4689e-06, 2.8409e-23, 1.0216e-13]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0515, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.4858e-01, 8.5142e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.1307e-01, 8.8692e-01, 9.3604e-06, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 4.6111e-15, 1.0271e-13, 2.9517e-40, 0.0000e+00],\n",
            "         [3.3141e-01, 6.6859e-01, 7.2698e-11, 6.7552e-24, 1.4347e-14]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1402, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9949e-01, 5.0932e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.4285e-04, 9.9975e-01, 1.1143e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9698e-01, 6.8370e-10, 3.0162e-03, 3.9955e-35, 0.0000e+00],\n",
            "         [3.5086e-04, 9.9965e-01, 3.2785e-06, 2.5034e-24, 6.6905e-14]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0390, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [7.6319e-04, 9.9924e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [6.1004e-01, 3.8996e-01, 6.6330e-06, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.5459e-15, 1.8846e-14, 7.8735e-41, 0.0000e+00],\n",
            "         [9.7823e-01, 2.1770e-02, 8.7710e-12, 1.4422e-26, 4.0020e-15]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1959, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9898e-01, 1.0239e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.8745e-04, 9.9963e-01, 1.8349e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9365e-01, 1.5914e-09, 6.3494e-03, 2.2449e-35, 0.0000e+00],\n",
            "         [4.2342e-04, 9.9957e-01, 8.5440e-06, 2.0765e-25, 6.4018e-14]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0310, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.8789e-04, 9.9901e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [5.0839e-01, 4.9161e-01, 2.9317e-06, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 3.8370e-15, 1.2171e-14, 1.5889e-41, 0.0000e+00],\n",
            "         [9.3496e-01, 6.5040e-02, 3.4766e-12, 4.4884e-27, 1.5131e-15]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1238, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9983e-01, 1.7479e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.4430e-04, 9.9967e-01, 1.8394e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9533e-01, 3.6514e-09, 4.6658e-03, 4.4211e-36, 0.0000e+00],\n",
            "         [4.4792e-05, 9.9995e-01, 1.2284e-06, 3.2296e-26, 5.4207e-15]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0264, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [5.8078e-03, 9.9419e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [2.5262e-01, 7.4738e-01, 1.1834e-06, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 8.5977e-15, 1.1098e-14, 3.2174e-42, 0.0000e+00],\n",
            "         [5.4185e-01, 4.5815e-01, 1.8042e-12, 7.2553e-27, 6.2429e-16]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1352, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9997e-01, 3.0357e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0614e-04, 9.9972e-01, 1.7353e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9712e-01, 4.8266e-09, 2.8813e-03, 7.8010e-37, 0.0000e+00],\n",
            "         [5.0277e-06, 9.9999e-01, 1.7436e-07, 9.2778e-27, 5.0958e-16]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0237, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.5342e-02, 9.8466e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.4364e-01, 8.5636e-01, 5.2514e-07, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.0192e-14, 1.0234e-14, 7.2868e-43, 0.0000e+00],\n",
            "         [2.0661e-01, 7.9339e-01, 5.4673e-13, 4.3937e-27, 1.5776e-16]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.1276, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9998e-01, 1.6085e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.9948e-05, 9.9973e-01, 1.8297e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9798e-01, 3.3586e-09, 2.0228e-03, 1.5162e-37, 0.0000e+00],\n",
            "         [1.7348e-06, 1.0000e+00, 7.8171e-08, 3.6700e-27, 1.5122e-16]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0226, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0039e-02, 9.8996e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.4160e-01, 8.5840e-01, 3.5344e-07, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 5.3077e-15, 8.3332e-15, 1.4854e-43, 0.0000e+00],\n",
            "         [2.0130e-01, 7.9870e-01, 3.1430e-13, 1.9432e-27, 9.1095e-17]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0872, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9998e-01, 2.2263e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [8.3191e-05, 9.9971e-01, 2.0401e-04, 0.0000e+00, 0.0000e+00],\n",
            "         [9.9839e-01, 1.4153e-09, 1.6078e-03, 3.1236e-38, 0.0000e+00],\n",
            "         [1.5787e-06, 1.0000e+00, 9.2488e-08, 1.7475e-27, 1.2394e-16]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "loss tensor(0.0218, grad_fn=<NllLossBackward0>)\n",
            "self_attention_percents tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [3.6608e-03, 9.9634e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [1.8068e-01, 8.1932e-01, 2.9618e-07, 0.0000e+00, 0.0000e+00],\n",
            "         [1.0000e+00, 1.8631e-15, 6.6009e-15, 3.0829e-44, 0.0000e+00],\n",
            "         [3.2462e-01, 6.7538e-01, 2.7588e-13, 8.3692e-28, 8.7764e-17]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss tensor(0.0508, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title After training\n",
        "## Now create the input for the transformer...\n",
        "model_input = torch.tensor([token_to_id[\"what\"],\n",
        "                            token_to_id[\"is\"],\n",
        "                            token_to_id[\"life\"],\n",
        "                            token_to_id[\"<EOS>\"]])\n",
        "predict(model_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALP_4RsxD_8i",
        "outputId": "93c351e1-f27e-4d8d-dbbf-5552ce694c72"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self_attention_values tensor([[-0.2946,  1.5233],\n",
            "        [-0.2946,  1.5232],\n",
            "        [ 0.4283, -2.5667],\n",
            "        [-0.2945,  1.5229]], grad_fn=<SelectBackward0>)\n",
            "predictions tensor([[ -1.5503,   8.6861,   0.8554,   1.4063, -10.6343],\n",
            "        [  7.8719,   4.8554,  10.2138, -11.1490,  -8.9896],\n",
            "        [ -4.1358,  -3.6899,  -6.4874,   8.0023,   1.9360],\n",
            "        [  3.6551, -13.2122,  -1.0089,  -0.8882,   9.4588]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "predicted_id tensor([4])\n",
            "self_attention_values tensor([[-0.2946,  1.5233],\n",
            "        [-0.2946,  1.5232],\n",
            "        [ 0.4283, -2.5667],\n",
            "        [-0.2945,  1.5229],\n",
            "        [ 0.4285, -2.5679]], grad_fn=<SelectBackward0>)\n",
            "self_attention_values tensor([[-0.2946,  1.5233],\n",
            "        [-0.2946,  1.5232],\n",
            "        [ 0.4283, -2.5667],\n",
            "        [-0.2945,  1.5229],\n",
            "        [ 0.4285, -2.5679],\n",
            "        [ 0.4285, -2.5679]], grad_fn=<SelectBackward0>)\n",
            "Predicted Tokens:\n",
            "\n",
            "\t awesome\n",
            "\t awesome\n",
            "\t awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now create the input for the transformer...\n",
        "model_input = torch.tensor([token_to_id[\"life\"],\n",
        "                            token_to_id[\"is\"],\n",
        "                            token_to_id[\"what\"],\n",
        "                            token_to_id[\"<EOS>\"]])\n",
        "predict(model_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMsxdr3xHP3A",
        "outputId": "db2fa534-ebd3-4b21-a446-39bf5893479e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions tensor([[ -0.8139,  11.5731,   2.7198,  -0.3226, -13.5959],\n",
            "        [  4.5409,  -5.9097,   2.5957,  -3.8665,   2.1823],\n",
            "        [ -4.5077,  -5.2312,  -7.4587,   8.8950,   3.5125],\n",
            "        [  3.3993, -13.8991,  -1.5442,  -0.3618,  10.1812]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "predicted_id tensor([4])\n",
            "Predicted Tokens:\n",
            "\n",
            "\t awesome\n",
            "\t awesome\n",
            "\t awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After training lets visulaize the which `token` attends do w.r.t another `token`\n",
        "\n",
        "NOTE: try to run this before and after trainin so you can see the what token attends what."
      ],
      "metadata": {
        "id": "4YwjIWcDYIyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example self-attention probabilities tensor\n",
        "attention_probs = torch.tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
        "         [9.9996e-01, 3.9247e-05, 0.0000e+00, 0.0000e+00],\n",
        "         [7.7210e-05, 9.9970e-01, 2.2281e-04, 0.0000e+00],\n",
        "         [9.9870e-01, 5.3399e-10, 1.3003e-03, 6.8641e-39]]])\n",
        "\n",
        "# Get the argmax for each token\n",
        "attended_tokens = torch.argmax(attention_probs, dim=-1)\n",
        "\n",
        "print(attended_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pQX1vWiXf_N",
        "outputId": "97efd7c4-a5ba-4017-d490-c74ad40e648d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 1, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kCY1fkfKG0KF"
      }
    }
  ]
}