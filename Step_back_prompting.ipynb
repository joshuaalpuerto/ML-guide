{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2h4jjZbqifflGo3879VhB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuaalpuerto/ML-guide/blob/main/Step_back_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following this blogpost on how to explore step-back prompting.\n",
        "https://levelup.gitconnected.com/step-back-prompting-how-to-make-llm-chatbots-smarter-than-ever-1b938673b8a2"
      ],
      "metadata": {
        "id": "UnrxnhinEPdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkLJrfryLyEp",
        "outputId": "d77dc3b7-4308-4783-f4b1-44e84cf3630c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain\n",
        "!pip install -q fireworks-ai duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88ga5CrhM1Pu",
        "outputId": "8b80abc4-06e7-4dc3-b84a-819c5061a6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/env/env.json') as jsonfile:\n",
        "    env = json.load(jsonfile)"
      ],
      "metadata": {
        "id": "C-n_EAHuM6vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatFireworks\n",
        "\n",
        "\n",
        "os.environ[\"FIREWORKS_API_KEY\"] = env['fireworks.ai']['apiKey']\n",
        "\n",
        "llm = ChatFireworks(model=\"accounts/fireworks/models/mistral-7b-instruct-4k\",\n",
        "                        model_kwargs={\"temperature\": 0.1, \"max_tokens\": 150, \"top_p\": 0.9})"
      ],
      "metadata": {
        "id": "x9NDNwQTNRBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.globals import set_llm_cache, set_debug\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "set_debug(True)"
      ],
      "metadata": {
        "id": "ts7sKESMRl_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing RAG Fusion\n",
        "llm.predict(\"\"\n",
        "\"\"\"<s>[INST] You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "\n",
        "Generate multiple search queries related to: was chatgpt around while trump was president?\n",
        "\n",
        "OUTPUT (4 queries): [/INST]\"\"\")"
      ],
      "metadata": {
        "id": "DKGvXdaWKEAs",
        "outputId": "4b3af7e1-1fe0-40e0-86ae-0a116f45e18d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatFireworks] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: <s>[INST] You are a helpful assistant that generates multiple search queries based on a single input query.\\n\\nGenerate multiple search queries related to: was chatgpt around while trump was president?\\n\\nOUTPUT (4 queries): [/INST]\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatFireworks] [1.02s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"1. \\\"ChatGPT during Trump presidency\\\"\\n2. \\\"ChatGPT and Trump administration\\\"\\n3. \\\"ChatGPT and Trump policies\\\"\\n4. \\\"ChatGPT and Trump controversies\\\"\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"1. \\\"ChatGPT during Trump presidency\\\"\\n2. \\\"ChatGPT and Trump administration\\\"\\n3. \\\"ChatGPT and Trump policies\\\"\\n4. \\\"ChatGPT and Trump controversies\\\"\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"model\": \"accounts/fireworks/models/mistral-7b-instruct-4k\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. \"ChatGPT during Trump presidency\"\\n2. \"ChatGPT and Trump administration\"\\n3. \"ChatGPT and Trump policies\"\\n4. \"ChatGPT and Trump controversies\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Few Shot Examples\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "# We now transform these to example messages\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"Question\", \"{input}\"),\n",
        "        (\"Assistant\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "\n",
        "ABSTRACT_QUESTION_GENERATOR_PROMPT=\"\"\"<s>[INST] You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. You can use the examples below wrap by `>>>` as guidance on what are good paraphrase questions\n",
        "\n",
        ">>>\n",
        "Here are a few examples:\n",
        "Question: Could the members of The Police perform lawful arrests?\n",
        "Assistant: what can the members of The Police do?\n",
        "\n",
        "Question: Jan Sindel’s was born in what country?\n",
        "Assistant:  what is Jan Sindel’s personal history?\n",
        ">>>\n",
        "\n",
        "Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(input_variables=['language', 'topic'], template=ABSTRACT_QUESTION_GENERATOR_PROMPT)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "XUA6zSf2QQhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are not using chat `ChatPromptTemplate` here as we are not chatting to chatbot, we are using LLM not to answer a question but rather to do a task to us."
      ],
      "metadata": {
        "id": "k7oqPShAWFJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts import FewShotPromptTemplate\n",
        "\n",
        "\n",
        "# Few Shot Examples\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "# We now transform these to example messages\n",
        "example_prompt = PromptTemplate(input_variables=[\"input\", \"ouput\"], template=\"Question: {input}\\n Answer:{output}\")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        "    suffix=\"\", # this is required\n",
        "    input_variables=[] # this is required\n",
        ")\n",
        "\n",
        "\n",
        "ABSTRACT_QUESTION_GENERATOR_PROMPT=\"\"\"<s>[INST] You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. You can use the examples below wrap by `>>>` as guidance on what are good paraphrase questions\n",
        "\n",
        ">>>\n",
        "Here are a few examples:\n",
        "{few_shot_prompt}\n",
        ">>>\n",
        "\n",
        "Question: {question} [/INST]\"\"\"\n",
        "\n",
        "\n",
        "prompt = PromptTemplate.from_template(ABSTRACT_QUESTION_GENERATOR_PROMPT)"
      ],
      "metadata": {
        "id": "fkIKe3JlUQ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(question=\"was chatgpt around while trump was president?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "yZHMGkN2OEFX",
        "outputId": "6dedf336-d471-4405-a557-c03b51e7a238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"was chatgpt around while trump was president?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatFireworks] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: <s>[INST] You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. You can use the examples below wrap by `>>>` as guidance on what are good paraphrase questions\\n \\n>>>\\nHere are a few examples:\\nQuestion: Could the members of The Police perform lawful arrests?\\nAssistant: what can the members of The Police do?\\n\\nQuestion: Jan Sindel’s was born in what country?\\nAssistant:  what is Jan Sindel’s personal history?\\n>>>\\n\\nQuestion: was chatgpt around while trump was president? [/INST]\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatFireworks] [2ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [3ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the historical context of the time period when ChatGPT was developed and when Trump was president?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "\n",
        "# function that will allow us to run this to a langchain expression\n",
        "def query_lambda(input):\n",
        "    # Perform some operations on the input\n",
        "    return input\n",
        "\n",
        "# Create a RunnableLambda component with your custom function\n",
        "custom_component = RunnableLambda(query_lambda)\n",
        "\n",
        "query_parser =  custom_component | CommaSeparatedListOutputParser()"
      ],
      "metadata": {
        "id": "B2n-2JdQYPQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
        "\n",
        "search = DuckDuckGoSearchAPIWrapper(max_results=4)\n",
        "\n",
        "def retriever_list(query):\n",
        "    answer = ''\n",
        "    ques = ''\n",
        "    for question in query:\n",
        "        ques += question\n",
        "        ques += '/'\n",
        "        if question[-1] == '?':\n",
        "            ans = search.run(ques)\n",
        "            ques = ''\n",
        "            answer += ans\n",
        "            time.sleep(2)\n",
        "    print(\"Answer: \", answer)\n",
        "    return answer\n",
        "\n",
        "static_query_reteriever = query_parser | retriever_list\n",
        "\n",
        "static_query_reteriever.invoke(\"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "ViyK8ZibXoqH",
        "outputId": "b89b2be2-9f68-427e-caa8-6a32b57468fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:query_lambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:query_lambda] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:parser:CommaSeparatedListOutputParser] Entering Parser run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:parser:CommaSeparatedListOutputParser] [1ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": [\n",
            "    \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:chain:retriever_list] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": [\n",
            "    \"What is the historical context of the time period when ChatGPT was developed and when Trump was president?\"\n",
            "  ]\n",
            "}\n",
            "Answer:  Aleksandra Yosifova 14 Aug 2023 7 min read ChatGPT isn't the first language model; it isn't even the first GPT model. But it made a significant leap in natural language processing—popularizing large language models and accelerating the adoption of AI. What factors contributed to ChatGPT's success? ChatGPT is an artificial intelligence (AI) chatbot built on top of OpenAI's foundational large language models (LLMs) like GPT-4 and its predecessors. This chatbot has redefined the standards of... When OpenAI launched ChatGPT, with zero fanfare, in late November 2022, the San Francisco-based artificial-intelligence company had few expectations. Certainly, nobody inside OpenAI was prepared... ChatGPT, which stands for \"Chat Generative Pre-training Transformer,\" is a large language model developed by OpenAI. It is based on the GPT (Generative Pre-training Transformer) architecture,...\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:chain:retriever_list] [3.18s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Aleksandra Yosifova 14 Aug 2023 7 min read ChatGPT isn't the first language model; it isn't even the first GPT model. But it made a significant leap in natural language processing—popularizing large language models and accelerating the adoption of AI. What factors contributed to ChatGPT's success? ChatGPT is an artificial intelligence (AI) chatbot built on top of OpenAI's foundational large language models (LLMs) like GPT-4 and its predecessors. This chatbot has redefined the standards of... When OpenAI launched ChatGPT, with zero fanfare, in late November 2022, the San Francisco-based artificial-intelligence company had few expectations. Certainly, nobody inside OpenAI was prepared... ChatGPT, which stands for \\\"Chat Generative Pre-training Transformer,\\\" is a large language model developed by OpenAI. It is based on the GPT (Generative Pre-training Transformer) architecture,...\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [3.18s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Aleksandra Yosifova 14 Aug 2023 7 min read ChatGPT isn't the first language model; it isn't even the first GPT model. But it made a significant leap in natural language processing—popularizing large language models and accelerating the adoption of AI. What factors contributed to ChatGPT's success? ChatGPT is an artificial intelligence (AI) chatbot built on top of OpenAI's foundational large language models (LLMs) like GPT-4 and its predecessors. This chatbot has redefined the standards of... When OpenAI launched ChatGPT, with zero fanfare, in late November 2022, the San Francisco-based artificial-intelligence company had few expectations. Certainly, nobody inside OpenAI was prepared... ChatGPT, which stands for \\\"Chat Generative Pre-training Transformer,\\\" is a large language model developed by OpenAI. It is based on the GPT (Generative Pre-training Transformer) architecture,...\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aleksandra Yosifova 14 Aug 2023 7 min read ChatGPT isn\\'t the first language model; it isn\\'t even the first GPT model. But it made a significant leap in natural language processing—popularizing large language models and accelerating the adoption of AI. What factors contributed to ChatGPT\\'s success? ChatGPT is an artificial intelligence (AI) chatbot built on top of OpenAI\\'s foundational large language models (LLMs) like GPT-4 and its predecessors. This chatbot has redefined the standards of... When OpenAI launched ChatGPT, with zero fanfare, in late November 2022, the San Francisco-based artificial-intelligence company had few expectations. Certainly, nobody inside OpenAI was prepared... ChatGPT, which stands for \"Chat Generative Pre-training Transformer,\" is a large language model developed by OpenAI. It is based on the GPT (Generative Pre-training Transformer) architecture,...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ]
}